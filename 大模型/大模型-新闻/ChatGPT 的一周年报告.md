生成式人工智能一直是2023年最大的技术话题。几乎每个人都玩过ChatGPT、Stable Diffusion、GitHub Copilot或Midjourney。也有少数人尝试过Bard或Claude，或者在他们的笔记本电脑上运行LLaMA。每个人都对这些语言模型和艺术生成程序将如何改变工作性质、引领奇点，甚至可能导致人类灭亡持有看法。在企业中，我们看到了从全面采用到严格限制甚至禁止使用生成式AI的各种政策。

现实是什么？为了找出人们实际在做什么，于9月进行了调查。我们的调查重点是企业如何使用生成式人工智能、他们在采用过程中看到的瓶颈，以及需要解决的技能差距。

# 摘要

我们从未见过像生成式人工智能这样迅速被采用的技术 —— 想象一下，ChatGPT才刚刚一岁。截至2023年11月。

- 我们调查的受访者中有三分之二（67%）报告说他们的公司正在使用生成式人工智能。
- AI用户表示，AI编程（66%）和数据分析（59%）是最需要的技能。
- 许多AI采用者仍处于早期阶段。26%的人使用AI不到一年。但已有18%的人在生产中应用了AI应用程序。
- 找到合适的使用案例的困难是用户和非用户采用的最大障碍。
- 16%使用AI的受访者正在使用开源模型。
- 意外结果、安全性、公平与偏见以及隐私是采用者正在测试的最大风险。
- 54%的AI用户预计AI最大的好处将是提高生产力。只有4%的人指出会减少员工数量。

生成式人工智能是否处于炒作曲线的顶点？我们看到了许多增长空间，特别是随着采用者发现新的使用案例和重新构想他们的商业方式。

# 用户与非用户

人工智能的采用正在变得普遍，但尚未普及。我们调查的受访者中有三分之二（67%）报告说他们的公司正在使用生成式人工智能。41%的人说他们的公司使用AI已经一年或更长时间；26%的人说他们的公司使用AI不到一年。仅有33%的人报告说他们的公司根本不使用AI。

使用生成式人工智能的用户是非用户的两倍多，但这意味着什么？如果我们问他们的公司是否使用数据库或网络服务器，毫无疑问，100%的受访者都会回答“是”。在AI达到100%之前，它仍处于采用过程中。ChatGPT于2022年11月30日向公众开放，大约一年前；像Stable Diffusion和DALL-E这样的艺术生成器则更早一些。在第一个网络服务器推出后的一年内，有多少公司拥有网站或正在尝试构建它们？肯定不到三分之二。只看AI用户，超过三分之一（38%）的人报告说他们的公司使用AI不到一年，几乎可以肯定仍处于早期阶段：他们正在进行实验和概念验证项目。（我们稍后会详细讨论这一点。）即使使用像GPT-4这样的云基础模型，它消除了开发自己的模型或提供自己的基础设施的需要，为任何特定用例微调模型仍然是一项重大任务。我们从未见过采用进程如此迅速。

当调查的受访者中有26%的人使用某项技术不到一年时，这是一个重要的动力迹象。是的，可以想象，AI —— 特别是生成式AI —— 可能正处于炒作周期的高峰，正如Gartner所主张的那样。我们不这么认为，尽管这些新项目的失败率无疑很高。但是，尽管迅速采用AI的冲动很强，AI仍然需要向这些新采用者证明其价值，而且很快。它的采用者期望回报，如果没有，那么，AI在过去经历过多次“寒冬”。我们是不是已经到达了采用曲线的顶峰，除了下降之外别无选择？还是还有增长空间？

我们认为还有很大的发展空间。训练模型和在这些模型之上开发复杂应用程序变得更加容易。许多新的开源模型体积更小，资源消耗也不那么大，但仍然能够提供良好的结果（特别是针对特定应用进行训练时）。有些模型甚至可以在笔记本电脑或网络浏览器上轻松运行。围绕生成式人工智能已经发展出一个健康的工具生态系统 —— 正如有关加州淘金热的说法，如果你想知道谁在赚钱，不要看矿工，要看卖铁锹的人。自动化构建复杂提示的过程已经变得普遍，采用了像检索增强生成（RAG）这样的模式和LangChain这样的工具。还有用于存档和索引提示以便重用的工具、用于检索人工智能可用于回答问题的文档的向量数据库等等。我们已经进入了工具的第二代（甚至第三代）。滑入到Gartner所谓的“幻灭谷”中的过山车之旅是不太可能的。

# 什么在阻碍AI？

了解公司不使用AI的原因对我们很重要，所以我们询问了那些公司未使用AI的受访者一个显而易见的问题：“为什么你的公司不使用AI？”我们也问了那些公司正在使用AI的用户一个类似的问题：“什么是阻碍进一步AI采用的主要瓶颈？”两组人都被要求从同一组答案中选择。最常见的原因，且差距显著，是难以找到适当的商业用例（非用户31%，用户22%）。我们可以争辩说这反映了缺乏想象力 —— 但这不仅不礼貌，还假设到处应用AI而不经过仔细思考是个好主意。"快速行动，打破事物"的后果仍在全球范围内发生，而且并不美好。缺乏深思熟虑和执行不力的AI解决方案可能是有害的，所以大多数公司应该仔细考虑如何适当地使用AI。我们不是在鼓励怀疑或恐惧，但公司应该在清楚了解风险的情况下启动AI项目，尤其是那些特定于AI的风险。什么样的用例是适当的，什么样的不是？区分这两者的能力很重要，这对使用AI的公司和不使用AI的公司都是一个问题。我们还必须认识到，许多这些用例将挑战对企业的传统思考方式。认识到AI的用例并理解AI如何让你重新构想业务本身将会并行进行。

![https://cdn.nlark.com/yuque/0/2023/png/406504/1702174368727-d7e58d83-5852-445b-b0fd-f0a15703fc1a.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1702174368727-d7e58d83-5852-445b-b0fd-f0a15703fc1a.png)

第二大常见原因是对法律问题、风险和合规性的担忧（非用户18%，用户20%）。这种担忧当然属于同一故事：在考虑适当的使用案例时必须考虑风险。使用生成式人工智能的法律后果仍然未知。谁拥有AI生成输出的版权？创建模型是否会侵犯版权，或者它是受美国版权法保护的“转型性”使用？我们现在不知道；答案将在未来几年的法庭上得到解决。还有其他风险，包括当模型生成不恰当的输出时的声誉损害、新的安全漏洞等等。

同一难题的另一个部分是缺乏AI使用政策。这样的政策将被设计用来减轻法律问题并要求遵守监管合规性。这并不是一个很重要的问题；它被6.3%的用户和3.9%的非用户提及。企业关于AI使用的政策将在未来一年出现并不断发展。在2023年晚期，我们怀疑相对较少的公司有政策。当然，不使用AI的公司不需要AI使用政策。但重要的是要思考哪个是马车，哪个是马。缺乏政策是否阻止了AI的采用？或者是个人自行采用AI，使公司面临未知的风险和责任？在AI用户中，公司范围内政策的缺失并没有阻碍AI的使用；这是显而易见的。但这可能并不是一件好事。再次强调，AI带来的风险和责任应该得到解决，而不是被忽视。故意的无知只会导致不幸的后果。

阻碍AI使用的另一个因素是公司文化不认识到需求（非用户9.8%，用户6.7%）。在某些方面，不认识到需求类似于找不到适当的商业用例。但也有一个重要的区别：词语“适当”。AI涉及风险，寻找适当的使用案例是一个合理的关切。不认识到需求的文化是轻蔑的，可能表明缺乏想象力或预见性：“AI只是一种时尚，所以我们将继续做一直对我们有效的事情。”这是问题所在吗？很难想象一个AI无法发挥作用的商业领域，而忽略这种可能性对公司的长期成功来说也不可能是有益的。

我们对担心缺乏熟练人员的公司表示同情，这是9.4%的非用户和13%的用户所报告的问题。拥有AI技能的人员一直难以找到且通常代价昂贵。我们预计在不久的将来这种情况不会有太大变化。虽然经验丰富的AI开发人员开始离开像谷歌、OpenAI、Meta和微软这样的大公司，但离开的人数还不足以满足需求 —— 而且他们中的大多数可能会转向初创公司，而不是增加成熟公司中的AI人才。然而，我们也对这个问题没有更突出地出现感到惊讶。采用AI的公司显然在某处找到了员工，无论是通过招聘还是培训现有员工。

一小部分人（非用户3.7%，用户5.4%）报告说“基础设施问题”是一个问题。是的，构建AI基础设施是困难且昂贵的，AI用户更加强烈地感受到这个问题也不足为奇。我们都读过关于推动像ChatGPT这样的模型的高端GPU短缺的报道。这是一个云服务提供商已经承担了大部分负担，并将在未来继续承担的领域。目前，很少有AI采用者维护自己的基础设施，并且他们的提供商使他们免受基础设施问题的影响。从长远来看，这些问题可能会减缓AI的采用。我们怀疑许多API服务是作为亏本卖的领头羊 —— 主要提供商故意将价格定得低以获取市场份额。这种定价不会持续，特别是当硬件短缺推高建设基础设施的成本时。当从AWS、微软或谷歌租用基础设施的成本上升时，AI采用者会如何反应？考虑到配备高端GPU的数据中心的成本，他们可能不会尝试构建自己的基础设施。但他们可能会减少AI开发。

很少有非用户（2%）报告缺乏数据或数据质量是一个问题，只有1.3%的人报告培训模型的难度是一个问题。事后看来，这是可以预见的：这些问题只有在你开始走向生成式AI的道路后才会出现。AI用户确实面临着这些问题：7%的人报告数据质量阻碍了进一步采用，4%的人提到了在他们的数据上训练模型的难度。但尽管数据质量和训练模型的难度显然是重要问题，它们似乎并不是使用AI构建的最大障碍。开发人员正在学习如何找到高质量的数据并构建有效的模型。

# 企业如何使用AI

我们询问了一些关于受访者如何与AI合作的具体问题，以及他们是“使用”它还是仅仅“实验”它。

我们并不惊讶于生成式AI最常见的应用是在编程中，使用像GitHub Copilot或ChatGPT这样的工具。然而，我们对采用程度感到惊讶：77%的受访者报告使用AI作为编程的辅助工具；34%正在实验它，44%已经在他们的工作中使用它。数据分析显示了类似的模式：总共70%；32%使用AI，38%正在实验它。更多用户正在实验的比例可能反映了OpenAI将高级数据分析（以前叫做代码解释器）加入到ChatGPT的Beta功能系列中。高级数据分析在探索和分析数据集方面做得相当不错 —— 尽管我们预计数据分析师会谨慎检查AI的输出，并不信任标记为“beta”的软件。

将生成式AI工具用于与编程（包括数据分析）相关的任务几乎是普遍的。对于不明确禁止其使用的组织，这当然会变得普遍。我们预期，即使在禁止使用AI的组织中，程序员也会使用AI。程序员总是开发出有助于他们完成工作的工具，从测试框架到源代码控制到集成开发环境。他们总是采用这些工具，无论是否得到管理层的许可。从程序员的角度来看，代码生成只是另一个节省劳动的工具，使他们在不断变得更复杂的工作中保持生产力。在2000年代初，一些关于开源采用的研究发现，大多数员工表示他们正在使用开源，尽管大多数首席信息官（CIO）表示他们的公司没有。显然，这些CIO要么不知道他们的员工在做什么，要么愿意睁一只眼闭一只眼。我们将看到这种模式重复自己：程序员会做必要的事情来完成工作，而管理者会在他们的团队更有生产力且目标正在被达成时愉快地无知。

在编程和数据分析之后，生成式人工智能的下一个最常见用途是与客户互动的应用程序，包括客户支持：65%的受访者报告说他们的公司正在使用AI（22%）或实验AI（43%）进行客户服务。尽管公司长期以来一直在谈论AI改善客户支持的潜力，我们并没有预料到客户服务会排名这么高。面向客户的互动风险很大：错误的回答、偏见或性别歧视行为，以及许多其他生成式人工智能的众所周知的问题很容易导致难以挽回的损害。也许这就是为什么这么大比例的受访者正在实验这项技术而不是使用它的原因（比任何其他类型的应用程序都多）。任何试图自动化客户服务的尝试都需要非常谨慎地测试和调试。我们将调查结果解释为“谨慎但兴奋地采用”。显然，自动化客户服务可以大大降低成本，甚至如果做得好，可以使客户更满意。没有人想落后，但同时，没有人想要一个高度可见的公关灾难或诉讼。

适度数量的受访者报告他们的公司正在使用生成式AI生成文案（书面文字）。47%的人专门用它来生成营销文案，56%的人用它来生成其他类型的文案（例如内部备忘录和报告）。尽管流言四起，但我们很少看到有人因AI而失去工作的报道 —— 但这些报道几乎完全来自文案撰写人员。AI目前还没有达到能像经验丰富的人类一样写得很好的水平，但如果你的公司需要为数百件商品编写目录描述，速度可能比辉煌的散文更重要。并且还有许多其他机器生成文本的应用：AI擅长总结文档。当与语音识别服务结合时，它可以完成创建会议记录甚至播客文字稿的工作。它也很适合写快速电子邮件。

使用生成式AI的应用中用户最少的是网页设计（总计42%；28%正在实验，14%正在使用）和艺术（总计36%；25%正在实验，11%正在使用）。然而，还有几个其他因素在起作用。首先，已经有很多低代码和无代码的网页设计工具，其中许多功能包含AI，但尚未使用生成式AI。在这个拥挤的市场中，生成式AI将面临重大的根深蒂固的竞争。其次，尽管OpenAI在去年3月的GPT-4发布会上展示了从手绘草图生成网站代码的能力，但该功能在调查结束后才可用。第三，虽然为一个简单网站勾勒出HTML和JavaScript很适合演示，但这并不是网页设计师需要解决的真正问题。他们想要一个可以在屏幕上编辑的拖放界面，而生成式AI模型尚未拥有这一功能。这些应用程序很快就会被构建出来；tldraw是一个非常早期的例子，展示了它们可能是什么样的。适合专业使用的设计工具还不存在，但它们很快就会出现。

受访者中只有一小部分表示他们的公司正在使用生成式人工智能创造艺术作品。虽然我们已经读到一些创业公司创始人使用Stable Diffusion和Midjourney等工具以低成本创建公司或产品标志的报道，但这仍然是一种专业应用，而且不是经常进行的活动。但这并不是公司所需的全部艺术作品：博客帖子的“英雄形象”、报告和白皮书的设计、宣传照片的编辑等都是必需的。生成式人工智能是答案吗？也许还没有到那一步。以Midjourney为例：尽管它的功能令人印象深刻，但这个工具也会犯愚蠢的错误，比如在主体上搞错手指（或手臂）的数量。虽然Midjourney的最新版本已经大为改进，但它推出的时间不长，许多艺术家和设计师更愿意避免处理这些错误。他们也更愿意避免法律责任。在生成艺术供应商中，Shutterstock、Adobe和Getty Images对其工具用户提供版权索赔的赔偿。微软、谷歌、IBM和OpenAI提供了更通用的赔偿。

![https://cdn.nlark.com/yuque/0/2023/png/406504/1702174471120-998ddc0e-9876-4640-b4b5-17965a130842.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1702174471120-998ddc0e-9876-4640-b4b5-17965a130842.png)

我们还询问了受访者的公司是否在使用AI创建某种其他类型的应用程序，如果是，具体是什么。虽然许多这样的应用程序重复了微软、OpenAI和谷歌等大型AI提供商已经提供的功能，但其他应用程序涵盖了非常广泛的范围。许多应用程序涉及到总结：新闻、法律文件和合同、兽医医学和财务信息等领域尤为突出。一些受访者还提到了与视频工作：分析视频数据流、视频分析以及生成或编辑视频。

受访者列出的其他应用包括欺诈检测、教学、客户关系管理、人力资源和合规性，以及更可预测的应用，如聊天、代码生成和写作。我们无法统计和汇总所有回应，但很明显，创意和创新并不缺乏。同样明显的是，几乎没有哪个行业不会受到影响 —— AI将成为几乎每个职业不可或缺的一部分。

生成式AI将作为终极办公室生产力工具占据一席之地。当这种情况发生时，它可能不再被认为是AI；它将只是微软Office、谷歌文档或Adobe Photoshop的一个功能，所有这些都正在集成生成式AI模型。GitHub Copilot和谷歌的Codey都已被整合到微软和谷歌各自的编程环境中。它们将成为软件开发者工作环境的一部分。大约20到25年前，网络也发生了同样的事情：为办公室或住宅布线以使用以太网曾经是一件大事。现在，我们到处都期望有无线网络，甚至这样说也不准确。我们不是“期望”它 —— 我们认为它理所当然，如果没有，就是一个问题。我们期望移动网络无处不在，包括地图服务，如果在信号不好的地方迷路了，就是一个问题。我们期望搜索无处不在。AI也会是这样。它不会被期待；它将被视为理所当然，而AI无处不在的重要转变部分将是了解在没有AI的情况下如何工作。

# 建造者及其工具

为了了解我们的客户如何使用AI，我们询问了他们正在使用哪些模型来构建定制应用程序。36%的受访者表示他们没有构建定制应用程序。相反，他们正在使用像ChatGPT、GitHub Copilot、集成到Microsoft Office和Google Docs中的AI功能或类似的预打包应用程序。剩余的64%已经从使用AI转向开发AI应用程序。这一转变代表了一次重大飞跃：它需要对人员、基础设施和教育的投资。

### 哪个模型？

尽管GPT模型在大多数在线讨论中占主导地位，但用于构建应用程序的模型数量正在迅速增加。我们几乎每天都会读到一个新模型的消息 —— 至少每周一次 —— 而快速浏览Hugging Face网站将会展示出你数不清的模型。（截至11月，其存储库中的模型数量接近40万。）开发人员显然有选择。但他们在做什么选择？他们正在使用哪些模型？

23%的受访者报告他们的公司正在使用GPT系列模型（2、3.5、4和4V）中的一个，这比其他任何模型都多，这并不令人惊讶。更令人惊讶的是，有21%的受访者正在开发他们自己的模型；这项任务需要大量的人员和基础设施资源。值得关注的是这种情况将如何演变：公司是否会继续开发自己的模型，或者他们会使用允许定制基础模型（如GPT-4）的AI服务？

16%的受访者报告他们的公司正在开源模型之上构建。开源模型是一个大型且多样化的群体。一个重要的子类别包括来自Meta的LLaMA衍生的模型：llama.cpp、Alpaca、Vicuna等等。这些模型通常更小（70亿至140亿参数）且更易于微调，可以在非常有限的硬件上运行；许多可以在笔记本电脑、手机或像树莓派这样的纳米计算机上运行。训练需要更多的硬件，但在有限环境中运行的能力意味着完成的模型可以嵌入到硬件或软件产品中。另一个子类别的模型与LLaMA无关：RedPajama、Falcon、MPT、Bloom等等，大多数都可以在Hugging Face上找到。使用任何特定模型的开发人员数量相对较小，但总体数量令人印象深刻，展示了GPT之外一个充满活力和活跃的世界。这些“其他”模型吸引了相当多的关注。不过要小心：尽管这组模型通常被称为“开源”，但其中许多限制了开发人员可以从中构建的内容。在使用任何所谓的开源模型之前，仔细查看许可证。有些限制模型仅用于研究工作并禁止商业应用；有些禁止与模型的开发者竞争；等等。就目前而言，我们不得不使用“开源”这个术语，但在AI领域，开源往往并不像看起来那样。

只有2.4%的受访者正在使用LLaMA和Llama 2构建。尽管LLaMA模型的源代码和权重在网上可用，但LLaMA模

型尚未拥有Meta支持的公开API —— 尽管似乎有几个由第三方开发的API，Google Cloud和Microsoft Azure都提供Llama 2服务。LLaMA家族模型也属于限制你可以构建内容的“所谓开源”类别。

![https://cdn.nlark.com/yuque/0/2023/png/406504/1702174540781-9c3ff18d-4564-4463-800c-238415ff6a1d.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1702174540781-9c3ff18d-4564-4463-800c-238415ff6a1d.png)

只有1%的人正在使用谷歌的Bard构建，这可能比其他模型曝光度更低。一些作家声称Bard的结果不如LLaMA和GPT模型，这可能在聊天方面是正确的，但我发现在GPT-4失败的情况下，Bard往往是正确的。对于应用程序开发者来说，Bard的最大问题可能不是准确性或正确性；而是可用性。2023年3月，谷歌宣布了Bard API的公共测试计划。然而，截至11月，关于API可用性的问题仍然通过链接到测试公告来回答。Bard API的使用无疑受到了只有少数开发者能够访问它的限制。使用Anthropic开发的非常有能力的Claude的人更少。Claude没有像Meta、OpenAI和谷歌的模型那样得到大量新闻报道，这是不幸的：Anthropic的宪法式人工智能安全方法是解决AI行业最大问题的独特而有前景的尝试。

### 处于哪个阶段？

当被问及公司在工作中处于哪个阶段时，大多数受访者分享说他们仍处于早期阶段。考虑到生成式人工智能相对较新，这并不是新闻。如果有什么需要惊讶的话，我们应该对生成式人工智能如此深入、如此迅速地渗透感到惊讶。34%的受访者正在进行初步的概念验证。14%在产品开发中，大概是在开发了一个PoC之后；10%正在构建模型，这也是早期阶段的活动；还有8%正在测试，这意味着他们已经构建了一个概念验证并正在向部署迈进 —— 他们至少有一个看似有效的模型。

![https://cdn.nlark.com/yuque/0/2023/png/406504/1702174550759-af433056-cd10-49db-86da-c289c11b5adb.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1702174550759-af433056-cd10-49db-86da-c289c11b5adb.png)

令人瞩目的是，有18%的受访者所在的公司已经在生产中使用AI应用程序。考虑到这项技术是新的，并且许多AI项目失败，2有18%的受访者报告他们的公司已经在生产中使用生成式人工智能应用程序，这是令人惊讶的。我们并不是怀疑者；这是一个证据，表明虽然大多数受访者报告的公司正在进行概念验证或其他早期阶段的工作，但生成式人工智能正在被采用并正在进行实际工作。我们已经看到一些重要的AI集成到现有产品中，包括我们自己的产品。我们预期其他人也会跟进。

# 风险与测试

我们询问了正在使用AI的公司的受访者，他们正在测试哪些风险。前五个答案集中在45%到50%之间：意外结果（49%）、安全漏洞（48%）、安全性和可靠性（46%）、公平性、偏见和伦理（46%）、隐私（46%）。

几乎一半的受访者选择了“意外结果”，这超过了其他任何答案，这一点很重要：任何使用生成式人工智能的人都需要知道，不正确的结果（通常称为幻觉）是常见的。如果这里有令人惊讶的地方，那就是这个答案没有被100%的参与者选择。意外、不正确或不恰当的结果几乎可以肯定是与生成式人工智能相关的最大单一风险。

我们希望看到更多公司测试公平性。在许多应用中（例如医疗应用），偏见是需要测试的最重要问题之一，消除训练数据中的历史偏见非常困难且至关重要。重要的是要意识到，不公平或有偏见的输出可能非常微妙，尤其是如果应用开发者不属于经历偏见的群体 —— 对于开发者来说可能是“微妙”的东西，对于用户来说往往是非常明显的。一个不理解用户口音的聊天应用是一个明显的问题（搜索“亚马逊Alexa不理解苏格兰口音”）。同样重要的是寻找偏见不是问题的应用。ChatGPT引起了对个人使用案例的关注，但有许多应用中偏见和公平性并不是主要问题：例如，检查图像以判断农作物是否患病，或优化建筑的供暖和空调系统以实现最大效率同时保持舒适。

看到安全性和安全问题位列前茅是件好事。公司逐渐意识到安全是一个严肃的问题，而不仅仅是一个成本中心。在许多应用中（例如客户服务），生成式人工智能可能会造成重大的声誉损害，除了造成法律责任。此外，生成式人工智能还有自己的漏洞，如提示注入，目前尚无已知解决方案。模型窃取，即攻击者使用特别设计的提示来重构模型训练所用的数据，是AI独有的另一种攻击。虽然48%的比例不算差，但我们希望看到对测试AI应用程序安全性的需求有更大的认识。

模型可解释性（35%）和模型退化（31%）的关注度不如上述问题高。不幸的是，可解释性仍然是生成式人工智能的研究问题。至少对于当前的语言模型来说，很难解释为什么生成模型给出了对任何问题的特定答案。对于大多数当前应用来说，可解释性可能不是一个要求。如果ChatGPT为您编写了一个Python脚本，您可能不在乎它为什么写了这个特定的脚本而不是别的。（也值得记住，如果您询问ChatGPT为什么产生任何回应，它的答案不会是之前回应的原因，而是一如既往地针对您的问题的最可能的回应。）但对于

诊断偏见问题，可解释性至关重要，并且当涉及生成式人工智能的案件出现在法庭上时，它将非常重要。

模型退化是一个不同的问题。任何AI模型的性能都会随着时间而退化，据我们所知，大型语言模型也不例外。一个备受争议的热门研究认为，GPT-4的回应质量随时间降低。语言以微妙的方式变化；用户提出的问题会发生变化，可能无法用旧的训练数据回答。即使是AI回答问题的存在也可能导致问问题的方式发生变化。另一个引人入胜的问题是，当生成模型在其他生成模型生成的数据上进行训练时会发生什么。模型崩溃是否真实存在，以及在模型重新训练时它会有什么影响？

如果您仅在现有模型之上构建应用程序，您可能无法对模型退化做任何事情。对于正在构建自己的模型或进行额外训练以微调现有模型的开发人员来说，模型退化是一个更大的问题。训练模型是昂贵的，可能是一个持续的过程。

![https://cdn.nlark.com/yuque/0/2023/png/406504/1702174789773-be737c10-6af1-479a-abdc-bf239601975d.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1702174789773-be737c10-6af1-479a-abdc-bf239601975d.png)

# 缺失技能

面临使用AI开发的公司的最大挑战之一是专业知识。他们是否拥有构建、部署和管理这些应用程序所需的技能的员工？为了了解技能短缺在哪里，我们询问了受访者他们的组织需要为AI项目获取哪些技能。我们并不惊讶AI编程（66%）和数据分析（59%）是两个最需要的技能。AI是我们几年前称为“数据科学”的下一代，而数据科学代表了统计建模和软件开发之间的融合。这个领域可能已从传统的统计分析演变为人工智能，但其整体形态并没有太大变化。

接下来最需要的技能是AI和ML的运营（54%）。我们很高兴看到人们认识到这一点；我们长期以来一直认为运营是AI和ML的“房间里的大象”。部署和管理AI产品并不简单。这些产品在许多方面与更传统的应用程序不同，虽然像持续集成和部署这样的实践对于传统软件应用非常有效，但AI需要对这些以代码为中心的方法进行重新思考。在任何AI应用中，模型而不是源代码是最重要的部分，而模型是大型二进制文件，不适用于像Git这样的源代码控制工具。与源代码不同，模型会随着时间变得陈旧，并需要持续监控和测试。大多数模型的统计行为意味着简单的确定性测试不起作用；你不能保证在给定相同输入的情况下，模型会生成相同的输出。结果是，AI运营是一个自成一体的专业，除了更传统的运营外，它还需要对AI及其需求有深刻理解。我们需要什么样的部署管道、存储库和测试框架来将AI应用投入生产？我们不知道；我们仍在开发部署和成功管理AI所需的工具和实践。

基础设施工程，被45%的受访者选择，排名并不高。这有点令人困惑：在生产中运行AI应用可能需要巨大的资源，就像微软这样的大公司正在发现的那样。然而，大多数组织尚未在自己的基础设施上运行AI。他们要么使用像OpenAI、微软、亚马逊或谷歌这样的AI提供商的API，要么使用云提供商来运行自制应用程序。但在这两种情况下，都有其他提供商构建和管理基础设施。尤其是OpenAI提供企业服务，其中包括用于训练定制模型的API以及关于保护公司数据私密性的更强保证。然而，随着云提供商运行接近满负荷，对于投资AI的公司来说，开始考虑自己的基础设施并获取建造它的能力是有意义的。

![https://cdn.nlark.com/yuque/0/2023/png/406504/1702174803433-f0a0511c-1483-480d-89f2-beb0f68f850c.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1702174803433-f0a0511c-1483-480d-89f2-beb0f68f850c.png)

超过半数的受访者（52%）将普遍的AI素养列为所需技能。虽然这个数字可能更高，但我们很高兴我们的用户认识到熟悉AI及其系统行为（或行为不端）是必不可少的。生成式AI具有很大的吸引力：只需一个简单的提示，你就可以让ChatGPT告诉你关于麦克斯韦方程或伯罗奔尼撒战争的信息。但是，在商业中，简单的提示并不能带你走得很远。AI用户很快就会学到，好的提示通常非常复杂，详细描述他们想要的结果以及如何获得它。提示可以非常长，并且可以包括回答用户问题所需的所有资源。研究人员正在讨论将来是否还需要这种程度的提示工程，但显然在未来几年内它将会伴随我们。AI用户还需要预期错误的答案，并准备检查AI产生的几乎所有输出。这通常被称为批判性思维，但更像是法律中的发现过程：对所有可能证据的详尽搜索。用户还需要知道如何为AI系统创建一个提示，以生成有用的答案。

# 最后，商业

那么底线是什么？企业如何从AI中受益？超过一半（54%）的受访者预期他们的企业将因提高生产力而受益。21%的人预期收入增加，这的确可能是提高生产力的结果。加起来，这是四分之三的受访者。另有9%的人说，他们的公司将从更好的计划和预测中受益。

只有4%的人认为主要好处将是人员数量的减少。我们长期以来一直认为失去工作给AI的恐惧被夸大了。尽管一些工作变得过时，从而短期内会有一些岗位流失，但AI也会创造新的工作岗位 —— 几乎每项重要的新技术，包括计算本身，都是如此。大多数工作依赖于多种个人技能，而生成式AI只能替代其中的一小部分。大多数员工也愿意使用能让他们的工作更轻松的工具，从而提高生产力。我们不认为AI会取代人类，我们的受访者也是这样认为的。另一方面，员工需要培训才能有效地使用AI驱动的工具，而提供这种培训是雇主的责任。

![https://cdn.nlark.com/yuque/0/2023/png/406504/1702174815128-4c92250b-e2c1-4b39-852c-0088f90da28f.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1702174815128-4c92250b-e2c1-4b39-852c-0088f90da28f.png)

对生成式人工智能的未来，我们持乐观态度。意识到ChatGPT仅存在一年，而在这短短的时间里，技术世界已经发生了巨大变化，这很难让人相信。我们从未见过新技术如此迅速地吸引如此多的关注：不是个人电脑，不是互联网，也不是网络。当然有可能，如果对生成式人工智能的投资没有产生预期效果，我们将进入另一个AI冬天。确实存在需要解决的问题 —— 正确性、公平性、偏见和安全性是其中最大的几个 —— 一些早期采用者将忽视这些风险并承受后果。另一方面，我们相信担心通用AI决定人类是不必要的，要么是读了太多科幻小说的人的困扰，要么是鼓励立法给现有企业相对于初创公司的优势的策略。

是时候开始了解生成式人工智能，思考它如何能改善您公司的业务，并规划策略了。我们不能告诉您该做什么；开发者正在将AI推向商业的几乎每个方面。但公司将需要在培训上投资，包括软件开发者和AI用户；他们将需要投资于开发和运行应用程序所需的资源，无论是在云中还是在自己的数据中心；他们还需要创造性地思考如何利用AI，意识到答案可能不是他们预期的。
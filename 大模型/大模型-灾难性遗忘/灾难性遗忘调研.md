### 综述文章
1. 《An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks》，蒙特利尔，2015，综述，深度网络
    1. 方法：Dropout；激活函数；超参自动搜索
2. 《Catastrophic Forgetting in Deep Learning: A Comprehensive Taxonomy》，亚马逊，2023，综述
    1. 分类：Rehearsal（数据回放）；Distance-Based（距离）；Sub-Networks（参数约束）；Dynamic Networks（增删结构）；Hybrid Approaches
3. 《A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning》，京东&东北大学，2023
    1. 有益&有害遗忘
    2. 遗忘的种类
4. 《CONTINUAL PRE-TRAINING OF LANGUAGE MODELS》，芝加哥&北大，2023
    1. 提出 DAS 的benchmark
    2. 分类 CF：Regularization methods；Replay methods retain； parameter-isolation methods
5. 《Continual Learning and Catastrophic Forgetting》，德克萨斯，2024，综述
    1. 定义持续学习：任务增量学习；领域增量；类增量
    2. 分类 CF： Replay；Parameter Regularization； Functional Regularization（蒸馏，关注的是输入-输出映射）； Optimization-based Approaches（自适应学习率）； Context-dependent Processing（动态网络）； Template-based Classification（专家网络）
    3. 完整实验代码：[CODE](https://colab.research.google.com/drive/1CaRudn_CUEkGizePYDe_Xb11Gg4jJ60v?usp=sharing)
6. 《Continual Learning of Large Language Models: A Comprehensive Survey》，罗格斯&谷歌，2024，综述分类
    1. 类型：任务增量；领域增量；类增量
    2. 技术：Replay-Based Methods；Regularization-Based Methods；Architecture-Based Methods

### 遗忘的原因
+ 过度拟合新的任务数据
+ 有限的模型容量
+ 不适合任务的训练技术
+ 缺乏正则化，参数改变太大

### 应对方法
1. **<font style="color:rgb(20, 38, 64);">正则化：</font>**<font style="color:rgb(20, 38, 64);">正则化技术会保留有意义的权重参数，这些参数在为新任务训练模型时对旧任务很重要</font>
    1. [Elastic Weight Consolidation (EWC)](https://www.pnas.org/doi/10.1073/pnas.1611835114)：基于 Fisher 信息矩阵为损失函数添加一个罚项，从而限制学习过程以保留先前任务的知识

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1728529288498-bcdfe683-b8c8-4b53-afa6-657215a14b4e.png)

    2. [Synaptic Intelligence (SI)](https://arxiv.org/abs/1703.04200)：在学习过程中随着时间的推移积累每个权重的重要性，并使用此信息来限制权重更新，从而保留先前任务的知识
    3. [Learning without Forgetting (LwF)](https://arxiv.org/abs/1606.09282)：一种结合知识蒸馏来训练网络完成新任务的方法，同时保留先前任务的输出概率，从而保持旧知识
    4. [Sparse Coding](http://ufldl.stanford.edu/tutorial/unsupervised/SparseCoding/#:~:text=Sparse%20coding%20is%20a%20class,1ai%CF%95i)：一种表示学习方法，在网络激活中强制执行稀疏性，从而为不同的任务提供更独特和不重叠的表示，从而减少干扰，参考历史的 Dropout 方法。
    5. [Knowledge Distillation](https://arxiv.org/abs/2012.04584)：通过 teacher-student 的方式， 利用正则约束 teacher 模型和 student 的 logits，将原知识传递给新的模型
2. **<font style="color:rgb(20, 38, 64);">基于重放的技术：</font>**<font style="color:rgb(20, 38, 64);">将有关旧任务的信息存储到某种内存存储中，然后模型可以使用该内存在当前任务学习期间“重放”信息，数据的操作；简单性、稳定性和高性能；</font>
    1. Memory Replay：模型保留先前训练数据的子集，用于将来定期重新训练模型，这有助于“提醒”它们过去的信息
        1. [Continual Learning with Memory Replay (CLMR)](https://arxiv.org/abs/2108.12641)：一种维护以前学习的示例的内存缓冲区的技术，这些示例会定期与新数据一起重放以防止遗忘。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1728529590612-bcf63979-32b8-4e29-812e-ba29905b5eda.png)

        2. 分为生成样本和使用原始样本两种类型
            1. <font style="color:rgb(20, 38, 64);">Generative Replay：</font>合成样本由其他模型生成，它模仿以前的数据集，用于加强模型的先前学习。一个缺点是生成的数据通常比原始数据的质量低。
            2. Mini-Rehearsal：在训练新任务时保留原始数据的一个子集（称为核心集coreset）来防止CF。
    2. Memory-Augmented Networks：模型配备了外部内存模块，增强了它们存储和检索先前学习的能力，从而防止遗忘
    3. **代表模型：**
        * SaulLM（法国-法律）-来自维基百科、StackExchange和GitHub的通用数据，构成最终数据集的约2%；
        * PMC-LLama（谷歌-生物医疗）-以5%的比率在训练批次内重放；
        * Me-Llama（耶鲁-医药）-混合了约25%的通用领域数据；
        * GeoGalactica（上交大-地理）-520亿token的地球科学语料库外，还以8:1:1的混合比例纳入了Arxiv论文和代码数据；
        * Llemma（普林斯顿-数学）-550亿数学预训练数据集和通用领域数据的混合上进行DAP，比例为19:1；
        * PLlama（农科院-农业）-9:1的比例混合领域特定和通用领域数据；
        * DeepSeek-Coder-v1.5（DeepSeek-代码）-87%的源代码，10%的英文代码相关自然语言，和3%的中文自然语言语料库
3. **<font style="color:rgb(20, 38, 64);">基于架构的方法：</font>**<font style="color:rgb(20, 38, 64);">对模型架构的修改，可以帮助 “冻结” 旧任务的关键参数，以适应新的任务学习，或者在需要更多模型容量时增加模型大小</font>
    1. [Progressive Neural Networks (PNN)](https://arxiv.org/abs/1606.04671)：为每个任务训练单独神经网络列的架构，通过横向连接将以前学到的知识转移到新任务中，而不会改变以前的网络。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1728528739784-5f129b90-62b8-4cb2-9f7c-3cf7b1ec9565.png)

    2. Expert Gate Modules：利用了一个基础网络，该网络通过其他子网针对每项任务进行了增强，每个子网都配备了一个自动编码器，使其成为其任务的“专家”。训练后，每个模型的参数都被 “冻结”，只有相关的 “专家 ”解决它所设计的任务，并保留一个共享的 “主干 ”或知识基础。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1728528675792-da442f0b-f403-4157-b3c5-fa55481f42f5.png)

    3. Dynamic Expandable Networks (DEN)：允许模型决定它需要的网络容量，为每个新任务添加新的人工神经元和连接，并“修剪”任何冗余链接

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1728529037649-a72afca6-e54e-4b62-9679-feb1fedc05fb.png)

    4. Adaptive learning：通过对网络的参数进行冻结或者逐层学习的方式，尽可能的保留原有的知识情况下，适配新的知识
4. **基于混合的方法：**在训练过程中，通过使用多种技术，达到减缓知识遗忘的目的
    1. [https://arxiv.org/pdf/2312.10549](https://arxiv.org/pdf/2312.10549)

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1728521743183-99fc3bb8-3d15-43a0-a57f-4bdd36dc7c3f.png)

### 探索方向
#### Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning：浙大&腾讯，2024
![](https://cdn.nlark.com/yuque/0/2024/png/406504/1728553856509-bf71484f-70f7-46dc-b45d-78367cd436ec.png)

利用提示词和种子大模型（要训练的模型），对任务数据进行重写，生成种子模型下的新标签，然后基于原始 x 和新的标签 y～去训练数据：![](https://cdn.nlark.com/yuque/0/2024/png/406504/1728553999719-e2921f51-5e9e-49bc-af73-758e3cfc68fd.png)，对比一般的微调损失：![](https://cdn.nlark.com/yuque/0/2024/png/406504/1728554028781-7f055eb8-a330-435e-9b9e-279664241bf8.png)

+ **生效阶段：**SFT
+ **参考借鉴意义：**通过自蒸馏的方式，可以生成数据

实验设定：

1. LoRA 训练，r=8；lr=1e-4，batch-size=8；
2. Llama2-7B-chat 模型
3. 数据设定

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1728554206515-331b2ac0-03de-4801-9a68-8bb3342c54e0.png)![](https://cdn.nlark.com/yuque/0/2024/png/406504/1728554429945-e09c27e8-1f31-44bb-982e-623cd2ae6068.png)

#### Continual Learning of Large Language Models: A Comprehensive Survey，罗格斯&谷歌，2024，综述
+ **生效阶段：**PT&SFT
+ **参考借鉴意义：**通过大模型生成或者使用公开的语料，混合到训练过程中，混合的比例，各不相同
    - SaulLM（法国-法律）-来自维基百科、StackExchange和GitHub的通用数据，构成最终数据集的约2%；
    - PMC-LLama（谷歌-生物医疗）-以5%的比率在训练批次内重放；
    - Me-Llama（耶鲁-医药）-混合了约25%的通用领域数据；
    - GeoGalactica（上交大-地理）-520亿token的地球科学语料库外，还以8:1:1的混合比例纳入了Arxiv论文和代码数据；
    - Llemma（普林斯顿-数学）-550亿数学预训练数据集和通用领域数据的混合上进行训练，比例为19:1；
    - PLlama（农科院-农业）-9:1的比例混合领域特定和通用领域数据；
    - DeepSeek-Coder-v1.5（DeepSeek-代码）-87%的源代码，10%的英文代码相关自然语言，和3%的中文自然语言语料库

#### Role Prompting Guided Domain Adaptation with General Capability Preserve for Large Language Models，哈工大&港大，2024
+ **生效阶段：**SFT
+ **参考借鉴意义：**通过给出了融合通用语料和任务语料的新的方法，通过自蒸馏+提示的方式，实现通用语料到任务语料到过渡
    - 模型：BELLE-7B-2M和BELLE-13B-2M（基于 LLaMA 的监督微调版本）和Vicuna-1.5-7B
    - LoRA微调：r=16；batch-size=16；epoch=2

**整个方法过程**

自蒸馏的目的是缓解通用领域的灾难性遗忘。具体步骤如下：

        1. 收集一组高质量的通用领域指令 $ I = \{(x_g, )\} $。
        2. 让LLM为每个 $ x_g $ 生成响应 $ y_g $。
        3. 生成的数据集 $ I = \{(x_g, y_g)\} $ 被称为 $ D_g $，作为通用领域的样本保留，并在后续训练过程中重放，以恢复模型的通用知识分布。

现在，训练语料库可以表示为：

$ D^+ = \{D_g, D_1, D_2, ..., D_n\} $

自蒸馏的关键优势在于它不需要访问原始的预训练数据，而是利用模型自身的知识来创建通用领域的样本。这种方法特别适用于那些原始训练数据不可用或受限的情况。



角色提示旨在缓解领域间混淆，通过为每个领域的数据分配特定的角色提示来帮助LLMs区分不同领域。具体做法如下：

        1. 为通用领域分配中心提示 $ p_c $。
        2. 为 $ n $ 个领域中的每一个分配唯一的角色提示，形成角色提示集 $ P = \{p_c, p_1, p_2, ..., p_n\} $。
        3. 每个指令-响应对 $ (x, y) $ 都以其对应的领域特定角色提示为前缀。

因此，当前的训练数据集为：

$ D^+_r = \{(p_c \oplus x_g, y_g)|(x_g, y_g) \in D_g\} \cup \{(p_i \oplus x_i, y_i)|(x_i, y_i) \in \bigcup_{i=1}^n D_i\} $

其中 $ \oplus $ 表示字符串连接操作。这种方法的优势在于它为模型提供了明确的上下文信息，使模型能够更好地区分和处理来自不同领域的任务。



角色整合的目的是使中心提示 $ p_c $ 获得与每个领域的角色提示 $ p_i $ 相关的专门能力，从而在推理时避免选择不同角色提示的需要。具体步骤如下：

        1. 从每个领域的数据集 $ D_i $ 中随机选择一小部分数据，记为 $ D'_i $。
        2. 将 $ D'_i $ 与通用领域数据 $ D_g $ 组合，并以中心提示 $ p_c $ 作为前缀。

最终的复合数据集结构为：

$ T^s_r = \{(p_c \oplus x_g, y_g)|(x_g, y_g) \in D_g \cup (\bigcup_{i=1}^n D'_i), D'_i \subset D_i\} \cup \{(p_i \oplus x_i, y_i)|(x_i, y_i) \in \bigcup_{i=1}^n D_i\} $

这个过程引入了混合比率 $ r $，定义为每个选定子集 $ D'_i $ 与其完整领域数据集 $ D_i $ 的比率：

$ r = \frac{|D'_i|}{|D_i|} $

混合比率 $ r $ 允许在训练过程中调整领域暴露程度，从而在通用能力和领域特定能力之间取得平衡。

**数据方面：**

中文实验使用了11个数据集，涵盖三个领域：

1. 医学领域：
    - cMedQQ：用于复述识别任务
    - cMedTC：用于句子分类任务
    - cMedQA：用于问答任务
2. 法律领域：
    - LawQA：用于法律问答任务
    - LawSum：用于法律文档摘要任务
3. 金融领域：
    - FNA、FQA、FNL、FRE、FFE、FSP：这些数据集涵盖了从情感分析到实体关系分类的多种任务

在自蒸馏过程中更好地保持模型的通用能力，研究者采用了以下策略

1. 中文模型：
    - 从Chinese-Alpaca项目随机提取50K指令样本
    - 从MOSS项目随机提取50K指令样本
    - 总计100K样本
2. 英文模型：
    - 从WizardLM项目随机选择50K指令样本
    - 从Alpaca项目随机选择50K指令样本
    - 总计100K样本

这些指令被输入到BELLE和Vicuna模型中，以获得蒸馏的示例集$ D_g $。在生成响应时，使用温度参数0.7和top-p参数0.95来控制生成的多样性和质量。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1728543760281-356c7e09-40e8-4a6b-95bd-12733c6fb100.png?x-oss-process=image%2Fformat%2Cwebp%2Fresize%2Cw_667%2Climit_0)

#### Continual Learning and Catastrophic Forgetting，德克萨斯，2024，综述
+ **生效阶段：**PT&SFT
+ **参考借鉴意义：**调整训练的超参数
    - 优化器调整：Adam、SGD、Adagrad、L2

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1728611034318-6bbae4c2-d59a-47d9-b68e-6a8726d03ed8.png)

    - 学习率调整：![](https://cdn.nlark.com/yuque/0/2024/png/406504/1728611465531-1e414940-7a8d-4708-a38c-ad4783e18980.png)
        * This approach is related to parameter regularization, but it is different because the use of adaptive learning rates does not change the loss function, while parameter regularization does

#### <font style="color:rgb(11, 11, 11);">Unlocking Continual Learning Abilities in Language Models，港大&清华，2024</font>
+ **生效阶段：**CPT&SFT
+ **参考借鉴意义：**调整参数更新的逻辑，只更新超过阈值的权重
    - 模型：T5、RoBERTa、LLaMA2-7B
    - LoRA微调：r=16；batch-size=16；epoch=2

**整体方法过程：**

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1728470261958-e4c90402-22ab-4e66-8772-4d5874c9fa25.png?x-oss-process=image%2Fformat%2Cwebp%2Fresize%2Cw_543%2Climit_0)

给定权重矩阵 $ W \in \mathbb{R}^{d_{in} \times d_{out}} $，我们将W的列解释为一组 $ d_{out} $ 向量，每个向量维度为 $ d_{in} $：$ W = [w_1, ..., w_i, ..., w_{d_{out}}] $， 其中 $ w_i \in \mathbb{R}^{d_{in}} $

给定层的输入向量 $ x \in \mathbb{R}^{d_{in}} $，该层的操作可以视为x与每个权重向量 $ w_i $ 的点积：$ h_i = x \cdot w_i $

然后我们使用L1范数计算归一化乘积幅度 $ n_i $：$ n_i = \|h_i\|_1 $，其中 $ \|\cdot\|_1 $ 表示L1范数。因此，我们得到W的L1归一化幅度乘积分布向量n。

### 参考资料
1. 相关论文
    1. <font style="color:rgb(11, 11, 11);">Overcoming catastrophic forgetting in neural networks（EWC算法），2017，斯坦福</font>
    2. <font style="color:rgb(11, 11, 11);">Unlocking Continual Learning Abilities in Language Models（</font>MIGU算法<font style="color:rgb(11, 11, 11);">），2024，港大&清华</font>
    3. Fine-Grained Gradient Restriction: A Simple Approach for Mitigating Catastrophic Forgetting（GEM算法），德克萨斯&索尼，2024
    4. Continual Learning of Large Language Models: A Comprehensive Survey
    5. Continual Learning and Catastrophic Forgetting
    6. A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning
    7. An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks
    8. Catastrophic Forgetting in Deep Learning: A Comprehensive Taxonomy
    9. CONTINUAL PRE-TRAINING OF LANGUAGE MODELS
    10. <font style="color:rgb(0, 0, 0);">Progressive Neural Networks</font>  

2. 相关链接
    1. [https://paperswithcode.com/task/continual-learning](https://paperswithcode.com/task/continual-learning)
    2. [https://github.com/xialeiliu/Awesome-Incremental-Learning](https://github.com/xialeiliu/Awesome-Incremental-Learning)
    3. [https://github.com/EnnengYang/Awesome-Forgetting-in-Deep-Learning](https://github.com/EnnengYang/Awesome-Forgetting-in-Deep-Learning)




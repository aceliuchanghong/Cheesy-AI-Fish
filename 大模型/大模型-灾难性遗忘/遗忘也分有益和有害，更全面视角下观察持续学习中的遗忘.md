> 原文：《A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning》
>

本论文对深度学习中的遗忘问题进行了全面的调查。虽然现有的遗忘调查主要集中在持续学习领域，但遗忘是一种在深度学习的其他研究领域中也普遍存在的现象。本调查旨在提供一个更全面的视角来理解遗忘现象，并探讨其潜在的优势。

## 1. 引言
遗忘指的是机器学习系统中先前获得的信息或知识随时间的流逝而丢失或退化的现象。早期的神经网络研究主要集中在静态数据集上训练模型，在这些设置中遗忘并不是一个显著的问题。灾难性遗忘的概念最初由McCloskey和Cohen正式提出。他们证明，神经网络在顺序学习不同任务时，会忘记先前学习的任务。这一观察突出了在顺序学习场景中解决遗忘问题的必要性。

与现有主要关注持续学习中遗忘的调查不同，本调查的目标是提供对遗忘的更全面理解。作者强调了遗忘的双重性质，既有益处也有害处。此外，本调查超出了持续学习的范围，涵盖了其他领域中的遗忘问题，包括基础模型、领域自适应、元学习、测试时自适应、生成模型、强化学习和联邦学习。

本调查将机器学习中的遗忘分为两类：有害遗忘和有益遗忘。

### 1.1 有害遗忘
有害遗忘不仅出现在持续学习中，还存在于其他研究领域，如基础模型、领域自适应、元学习、测试时自适应、生成模型、强化学习和联邦学习。这些领域中的遗忘可归因于各种因素：

+ 持续学习：不同任务间的数据分布变化
+ 元学习：任务分布的变化  
+ 联邦学习：不同客户端间的异构数据分布
+ 领域自适应：领域变化
+ 测试时自适应：适应测试数据分布
+ 生成模型：生成器随时间变化或学习非平稳数据分布
+ 强化学习：状态、动作、奖励和状态转移动态随时间变化
+ 基础模型：微调遗忘、增量流数据预训练、用于下游持续学习任务

### 1.2 有益遗忘
虽然大多数现有工作认为遗忘是有害的，但作者认识到遗忘是一把双刃剑。在某些情况下，有意遗忘某些知识是有益的：

1. 选择性遗忘可以帮助缓解过拟合
2. 为了提高模型泛化或促进新任务/知识的学习，必须消除先前学习知识中的偏见或无关信息
3. 机器遗忘可以防止数据隐私泄露

### 1.3 解决遗忘的挑战
解决遗忘面临许多挑战，在不同研究领域有所不同：

1. 数据可用性：在各种场景中，数据可用性是一个重大挑战，极大地增加了解决遗忘的难度。
2. 资源限制：资源受限的环境，如内存和计算受限，给有效解决遗忘带来了挑战。
3. 适应新环境/分布：在各个领域中，目标环境或数据分布可能随时间变化。学习代理需要适应这些新环境或场景。
4. 任务干扰/不一致：不同任务之间的目标冲突和不兼容可能导致任务干扰，给防止遗忘带来挑战。
5. 隐私泄露防止：在某些情况下，保留旧知识可能会引发隐私问题，因为它可能无意中暴露私人信息。

### 1.4 调查范围、贡献和组织
本调查的主要目标是提供上述领域中主要研究方向的遗忘问题的全面概述。作者的贡献可以总结为三个方面：

1. 与现有调查相比，本调查对持续学习进行了更系统的调查。
2. 除了持续学习，本调查还扩展到其他研究领域中的遗忘问题。
3. 与现有的持续学习和遗忘调查不同，本调查揭示了遗忘可以被视为一把双刃剑。

论文的组织结构如下：

+ 第2-9节：全面调查各种机器学习领域中有害遗忘现象
+ 第10节：探讨有益遗忘的概念及其在提高模型泛化性能和促进机器遗忘中的作用
+ 第11节：介绍当前的研究趋势，并对该领域的潜在未来发展提供见解

## 2. 持续学习中的遗忘
持续学习(CL)的目标是在不忘记先前任务知识的情况下，学习一系列任务 $T_1, T_2, \cdots, T_N$。它可以用以下优化目标来表述：

假设在学习任务 $t$ 时，目标是最小化到目前为止所有已见任务的风险，即：

$L(\theta_t) = \sum_{t=1}^N \mathbb{E}_{(x,y)\sim D_{T_t}} L_{\theta_t}(x, y)$


其中 $ \theta_t $ 是学习任务 $ t $ 时的参数， $ D_{T_t} $ 表示与任务 $ t $ 相关的训练数据。

持续学习问题可以从几个不同的角度进行分类：

1. 根据训练过程中是否有明确的任务划分/信息，CL可以分为任务感知和任务自由场景。
2. 根据模型是单次还是多次处理数据，CL可以分为在线和离线CL。
3. 根据CL中使用的标记数据量，可以分为监督、半监督、少样本和无监督CL。

### 2.1 任务感知和任务自由CL
#### 2.1.1 任务感知CL
任务感知CL侧重于在CL过程中有明确任务定义(如任务ID或标签)的情况。任务感知设置中三种最常见的CL场景是任务增量学习、领域增量学习和类增量学习。

问题设置：考虑标准的CL问题，学习N个任务序列，表示为 $ D^{tr} = \{D^{tr}_1, D^{tr}_2, \cdots, D^{tr}_N\} $。第k个任务 $ D^{tr}_k $ 的训练数据由一组三元组 $ \{(x^k_i, y^k_i, T_k)^{n_k}_{i=1}\} $ 组成，其中 $ x^k_i $ 是第i个任务数据样本， $ y^k_i $ 是与 $ x^k_i $ 相关的数据标签， $ T_k $ 是任务标识符。目标是在训练任务序列 $ D^{tr} $ 上学习具有参数 $ \theta $ 的神经网络，即 $ f_\theta $，使其在所有学习任务的测试集 $ D^{te} = \{D^{te}_1, D^{te}_2, \cdots, D^{te}_N\} $ 上表现良好，而不忘记先前任务的知识。

任务感知CL的现有方法主要探索了五个主要分支：基于内存、基于架构、基于正则化、基于子空间和基于贝叶斯的方法。

1. 基于内存的方法：
    - 原始内存重放
    - 内存样本选择
    - 生成重放
    - 压缩内存重放
2. 基于架构的方法：
    - 固定容量
    - 容量增加
3. 基于正则化的方法：
    - 惩罚重要参数更新
    - 基于知识蒸馏
4. 基于子空间的方法：
    - 正交梯度子空间
    - 正交特征子空间
5. 基于贝叶斯的方法：
    - 权重空间正则化
    - 函数空间正则化
    - 贝叶斯架构扩展

#### 2.1.2 任务自由CL
任务自由CL指的是学习系统无法访问任何明确的任务信息的特定场景。与任务感知CL设置不同，任务自由CL旨在在没有明确的任务边界或标签的情况下执行适应。系统需要随时间不断更新其模型或表示，以适应新信息，同时保留先前学习的知识。

任务自由CL中缺乏明确的任务信息带来了更大的挑战，因为学习系统必须自主识别和适应数据分布的变化或转移，而没有任何特定于任务的线索或标签。任务自由CL的现有方法可以分为两类：基于内存的方法和基于网络扩展的方法。

1. 基于内存的方法：  
这些方法涉及存储一小部分先前数据，并与新的小批量数据一起重放它们。
2. 基于扩展的方法：  
这些方法引入了基于Dirichlet过程混合模型等的网络结构扩展方法，以适应新的数据分布或概念，同时保留先前学习的知识。

### 2.2 在线CL
#### 2.2.1 在线CL方法
在线CL设置与离线CL相比带来了额外的挑战。在在线CL中，学习者只允许对每个任务的数据进行一次处理。解决在线CL中遗忘问题的现有工作主要基于排练重放。

#### 2.2.2 在线CL中的类别不平衡问题
在线CL中不平衡数据流的存在引起了重大关注，主要是由于它在现实世界应用场景中的普遍性。解决类别不平衡问题可以通过两种主要策略：

1. 在训练阶段平衡旧类和新类的学习
2. 采用后处理技术来校准模型中固有的偏差

### 2.3 半监督、少样本和无监督CL
#### 2.3.1 半监督CL
半监督CL是传统CL的扩展，允许每个任务同时包含未标记数据。现有的半监督CL工作主要包括生成重放和蒸馏来避免遗忘。

#### 2.3.2 少样本CL
少样本CL指的是模型需要仅使用每个任务的有限数量的标记样本来学习新任务，同时保留先前遇到的任务知识的场景。挑战在于有效利用有限的标记数据和先前学习的知识来适应新任务，同时避免遗忘。

与传统CL相比，少样本CL面临由于每个任务可用的样本数量有限而导致的过拟合挑战。为了解决少样本CL中的遗忘问题，现有方法采用各种技术，包括度量学习、元学习和参数正则化。

#### 2.3.3 无监督CL
无监督CL是一个快速增长的研究领域，强调仅从未标记数据中学习。与依赖标记数据的传统监督CL不同，无监督CL探索仅使用未标记数据实现学习和适应的技术。

现有的无监督CL方法主要依赖于基于表示的对比学习技术。

### 2.4 理论分析
CL的理论分析相对较少。一些研究提供了CL的泛化界限，分析了可解情况下的泛化性能，研究了类增量学习，并提出了统一多个现有CL解决方案的理想持续学习器。

## 3. 基础模型中的遗忘
基础模型中的遗忘表现在几个不同的方向：

1. 在微调基础模型时，倾向于忘记预训练知识，导致下游任务性能次优。
2. 基础模型通常在数据集上进行单次训练，导致两种类型的遗忘：
    - 在流数据的情况下，挑战在于保留先前的预训练知识。
    - 在训练期间较早遇到的示例可能比后来的示例更快被覆盖或遗忘。
3. 基础模型具有强大的特征提取能力，使其越来越受欢迎用于CL方法，旨在在多个任务中实现出色的性能而不忘记先前学习的知识。

### 3.1 微调基础模型中的遗忘
微调基础模型可以在下游任务上取得令人印象深刻的性能。然而，微调基础模型可能导致预训练知识的遗忘，这可能导致下游任务的次优性能。当目标模型在微调过程中显著偏离预训练模型时，就会发生遗忘。这种偏离增加了过拟合到小型微调集的可能性。

有几种简单有效的策略可以缓解微。

### 3.2 一轮预训练中的遗忘
#### 3.2.1 预训练中先前知识的遗忘
基础模型通常使用自监督学习进行训练，而流数据的自监督学习已成为一个重要的研究领域，这是由于存储和训练大量未标记数据所涉及的存储和时间成本。在这种情况下，Hu等人提出了一种自监督学习的顺序训练方法，证明自监督学习比其监督学习对应物表现出更少的遗忘。同样，Purushwalkam等人在连续非独立同分布(non-iid)数据流上执行自监督学习，并引入了最小冗余(MinRed)缓冲区方法来缓解灾难性遗忘。此外，Lin等人开发了一个基于排练的框架，结合了他们提出的采样策略和自监督知识蒸馏，以解决流式自监督学习中的遗忘问题。

#### 3.2.2 预训练中有益的遗忘
大型基础模型的记忆和遗忘引起了广泛关注，一些新兴的研究探讨了训练数据的记忆与不同类型遗忘之间的联系。基础模型表现出两种看似矛盾的现象：训练数据的记忆和不同类型的遗忘。记忆指的是模型过度拟合特定训练样本，使其容易受到隐私泄露的影响，例如推断数据点是否属于与预训练模型相关的训练数据。一些研究表明，这种现象在语言模型中可能泄露个人身份信息，这在实践中是不可取的。另一方面，遗忘涉及逐渐失去关于早期训练示例的信息。

与持续学习不同，持续学习将遗忘度量为保留先前学习知识的对立面。相比之下，Jagielski等人的研究考察了一个特定任务的模型，并研究了对特定训练示例表现出的遗忘程度。

### 3.3 基础模型中的持续学习
最近，研究人员探索了使用基础模型来解决持续学习(CL)问题。基于基础或预训练模型的CL显示出前景，特别是在自然语言处理(NLP)领域。诸如Ke等人和Wu等人的研究深入探讨了预训练模型在NLP任务中的有效应用。此外，Scialom等人证明了微调的预训练语言模型可以作为持续学习器。

鉴于Transformer模型在计算机视觉任务中的巨大成功，该领域的许多研究开始探索使用基础模型进行CL的应用。Ostapenko等人研究了预训练视觉模型作为下游CL任务基础的有效性。Mehta等人从损失景观的角度解释了为什么预训练模型在缓解遗忘方面更有帮助。他们发现预训练模型驱使权重收敛到更宽的最小值，这通常表示更好的泛化能力。Ramasesh等人观察到，预训练模型比随机初始化的模型更能抵抗遗忘，并且这种能力随着预训练模型和数据规模的增加而增加。

除了上述研究之外，许多研究人员还探索了预训练模型的微调，以增强其对下游任务的适应能力。这些努力主要围绕参数高效的微调技术，如Adapters和Prompts。

1. 基于Adapter的方法：
    - ADAM展示了使用冻结的基础模型生成可泛化的嵌入，并将分类器权重设置为原型特征可以优于最先进的CL方法。
    - ADA采用了不同的方法，为每个新任务学习单个adapter，而不是调整整个CL模型。然后，新的adapter与现有的adapter融合，以保持固定的容量。
2. 基于Prompt的方法：
    - L2P引入了一种动态实例级学习方法，为每个新任务确定匹配的专家提示，同时冻结预训练模型。
    - DualPrompt在L2P的基础上，通过在预训练主干上附加互补提示(共享通用提示和匹配的专家提示)来改进。
    - S-Prompt专注于无样本的领域增量学习。它独立地学习每个领域的提示，并将它们存储在一个池中以防止遗忘。
    - Progressive Prompts也为每个新任务学习一个提示，但它冻结旧任务的提示，并将它们纳入新任务，以鼓励知识的前向转移。

## 4. 领域自适应中的遗忘
领域自适应的目标是将知识从源领域转移到目标领域。领域代表输入空间X和输出空间Y的联合分布。具体而言，源领域定义为 $ P_S(x,y) $，其中x属于输入空间 $ X_S $，y属于输出空间 $ Y_S $。同样，目标领域定义为 $ P_T(x,y) $，其中x属于输入空间 $ X_T $，y属于输出空间 $ Y_T $。

在持续领域自适应(CDA)的背景下，主要关注协变量偏移设置。协变量偏移指的是输入数据X的分布在源域和目标域之间不同，而输出Y的条件分布保持不变。这种设置假设输入和输出之间的关系在各个领域保持一致，但输入数据的分布发生变化。这正式定义如下：

$ P_S(X=x) \neq P_T(X=x), P_S(y|X=x) = P_T(y|X=x) $

CDA和传统CL有不同的特征和目标。一方面，CDA在跨目标领域序列转移知识的可用性方面与传统CL不同。在CDA中，源领域数据是可访问的，目标是将模型从源领域适应到目标领域，利用可用的源领域数据。然而，目标领域可能只提供未标记的数据，要求模型在没有明确监督的情况下适应新领域。另一方面，传统CL旨在学习和适应一系列任务，而不访问先前任务特定的标记数据。在传统CL中，通常为每个任务提供标记数据。

问题设置：假设我们有一个预训练模型 $ f_\theta $，它已经在一组源领域数据 $ P_S(x,y) $ 上进行了训练，其中x属于源领域输入空间 $ X_S $，y属于源领域标签空间 $ Y_S $。此外，我们有一系列演变的目标分布 $ P^T_t(x,y) $，其中x属于目标领域t的输入空间 $ X^T_t $。y属于目标领域t的标签空间 $ Y^T_t $。t表示从1到N的领域索引。CDA的目标是训练 $ f_\theta $，使其在演变的目标领域序列中的所有领域 $ P^T_t(x,y) $ 上表现良好，定义如下：

$ \min_\theta \mathbb{E}_{t\in[1,...,N]} \mathbb{E}_{x_T \sim P_T^t, x_S \sim P_S} L(f_\theta(x_T), f_\theta(x_S)) $

在处理新的目标领域时，需要注意的是，它们的数据分布与源领域不同。因此，将模型适应这些新领域可能会无意中导致忘记从先前领域获得的知识。为了缓解这个问题，已经开发了各种方法来解决领域自适应中的灾难性遗忘问题。

当源领域数据可用时，大多数工作通过重放源领域数据来避免遗忘，少数工作基于正则化或元学习。最近，一些工作集中在无源方法上，旨在保护源领域数据的隐私，这在许多情况下是无法访问的。

## 5. 测试时自适应中的遗忘
测试时自适应(TTA)指的是在推理或测试期间即时将预训练模型适应到未标记的测试数据的过程。与领域自适应不同，测试时自适应发生在部署阶段而不是训练阶段。这种方法允许模型适应测试时遇到的特定实例或条件，以提高模型的泛化性能。TTA可能涉及根据推理过程中获得的额外信息或反馈更新模型的某些参数或特征。

在传统的机器学习场景中，在测试期间，通常假设测试数据 $ D_{test} $ 遵循与训练数据相同的分布。然而，在现实世界的应用中，测试数据分布通常会偏离训练数据分布。为了解决训练和测试阶段之间的分布偏移，采用了TTA。TTA涉及使用无监督适应损失函数在未标记的测试数据x上适应预训练模型。这种适应旨在最小化损失函数 $ L(x,\theta) $，其中 $ \theta $ 是参数。重要的是要注意，x是从测试数据集 $ D_{test} $ 中采样的。随后，适应后的模型使用更新后的参数对测试输入x进行预测。这允许模型考虑训练和测试阶段之间的分布偏移，并有望提高其在测试数据上的性能。

现有工作：Tent方法提出了一种测试时熵最小化方法来增强模型泛化。该方法专注于最小化模型在测试数据上的预测熵，从而提高模型对未见示例的泛化能力。另一种方法MECTA旨在提高测试时自适应的内存效率。MEMO提出了一种方法，对测试数据点应用各种数据增强。随后，通过最小化模型在增强样本中的输出分布熵来适应所有模型参数。

当预训练模型适应新的未标记测试数据时，模型会转向新数据，可能导致它忘记先前从源领域数据中学到的关键信息。这种现象可能导致知识的大量损失，并对模型的整体性能产生不利影响。为了解决这个问题，现有方法主要采用两种策略：

1. 一种直观的方法是采用两步过程。首先，冻结在源数据上训练的模型。随后，引入新的可学习参数来适应模型到测试时数据。
2. 另一种常见方法是通过限制重要参数的更新来防止遗忘，从而避免引入新参数。

## 6. 元学习中的遗忘
元学习，也称为学习如何学习，专注于开发能够从以前的学习经验中学习的算法和模型，以提高它们学习新任务或适应新领域的能力。在元学习中，目标是使学习系统(通常称为元学习器或元模型)从一组相关的学习任务或领域中获取通用知识或"元知识"。然后，这种元知识被用来促进更快的学习、更好的泛化和对新的、未见过的任务或领域的改进适应。

形式上，考虑任务的分布，表示为 $ P(T) $。对于特定任务 $ T_t $，它由训练数据集 $ D_{train} $ 和验证数据集 $ D_{val} $ 组成，从任务分布 $ P(T) $ 中采样。任务 $ T_t $ 的损失函数，其中元参数为 $ \theta $，定义为：

$ L(T_t) = \log P(D_{val}|D_{train}; \theta) $

元学习的目标是优化元损失函数，给定：

$ \min_\theta \mathbb{E}_{T_t \sim P(T)} L(T_t) $

换句话说，目标是找到最优的元参数 $ \theta $，以最小化跨任务的预期损失，其中任务从任务分布 $ P(T) $ 中采样。

然而，在元学习的背景下，遗忘仍然可能发生，并可以分为两个不同的研究方向：

1. 增量少样本学习(IFSL)：目标是在预训练的基础类之外元学习新类。在这种情况下，遗忘源于丢失与预训练基础类相关的信息。
2. 持续元学习：代理在学习新任务时遇到非平稳任务分布。与IFSL不同，这里的目标不是记住特定的基础类，而是保留从先前任务分布中获得的元知识。

### 6.1 增量少样本学习
增量少样本学习(IFSL)专注于在保留先前学习类别知识的同时，用有限的标记数据学习新类别的挑战。在这种情况下，标准分类网络之前已经接受训练以识别预定义的基础类集。之后，重点是纳入额外的新类，每个新类只伴随少量标记示例。随后，模型在考虑基础类和新类的情况下测试其分类性能。

现有工作： Gidaris等人提出了IFSL问题并提出了一种基于注意力的解决方案来缓解IFSL中的遗忘。Ren等人提出的注意力吸引网络是另一种方法，其中在增量元学习阶段的每个情节训练目标使用注意力机制来关注基础类集进行调节。与之前提取每个任务固定表示的方法不同，XtarNet强调通过结合新特征和基础特征来提取任务自适应表示，以增强表示的适应性。Shi等人建议在基础分类器预训练阶段投入更多努力，而不是后来的少样本学习阶段。因此，他们提出寻求基础分类器训练目标函数的平坦局部最小值，并在面对新任务时在该平坦区域内微调模型参数。此外，C-FSCIL引入了一个可训练的固定大小全连接层和一个可重写的动态增长内存缓冲区以缓解遗忘。这个内存缓冲区可以存储到该时间点为止遇到的每个类的向量。

### 6.2 持续元学习
持续元学习(CML)的目标是解决非平稳任务分布中的遗忘挑战。传统的元学习方法通常专注于单一任务分布。然而，CML将这个概念扩展到处理一系列任务分布，表示为 $ P_1(T), P_2(T), \cdots, P_N(T) $。在CML中，目标是开发能够有效适应和泛化到随时间出现的新任务分布的元学习算法。这些任务分布可以代表不同的环境、领域或上下文。它旨在减轻先前学习的任务分布的遗忘，同时有效地适应新任务。

现有工作： 在线元学习(OML)是一个假设任务顺序到达并旨在提高未来任务性能的框架。Jerfel等人扩展了模型无关元学习(MAML)方法，并利用Dirichlet过程混合将相似的训练任务分组在一起。然而，由于需要为每个组件独立的参数，这种方法无法扩展到大规模非平稳分布。Yap等人提出了一种使用拉普拉斯近似来建模元参数后验分布的方法。Zhang等人进一步扩展了这个框架，使用动态混合模型来学习元参数的分布，而不是单一分布。此外，他们使用结构变分推断技术来推断模型中的潜在变量。Wang等人引入了一个大规模基准用于顺序域元学习。他们提出了不同的设置，包括监督学习、不平衡域和半监督设置，以评估各种方法在顺序域元学习中的性能。

## 7. 生成模型中的遗忘
生成模型的目标是学习一个能够从目标分布生成样本的生成器。在生成模型的背景下，与遗忘相关的研究可以分为两个主要类别：(1) GAN训练本身可以被视为CL; (2) 非平稳分布上的终身学习生成模型。

### 7.1 GAN训练是一个持续学习问题
Thanh-Tung等人将GAN训练视为一个CL问题。他们将判别器视为学习一系列任务，每个任务代表由特定生成器生成的数据分布。为了解决灾难性遗忘问题，他们提出在判别器上使用动量或梯度惩罚。通过结合这些技术，他们旨在防止先前学习任务的遗忘，从而改善GAN训练的收敛性并减少模式崩溃。同样，Liang等人采用弹性权重巩固(EWC)或突触智能(SI)来缓解GAN训练中判别器的遗忘。

将GAN训练视为CL的一个应用是在无数据知识蒸馏(DFKD)的背景下。DFKD旨在从预训练的教师模型中提取一个紧凑的学生模型，而教师模型的原始训练数据不可用。DFKD中的基本方法涉及使用生成模型重建或生成难例作为预训练模型的输入，从而从教师模型中蒸馏知识。然而，遗忘在DFKD中构成了一个重大挑战。DFKD中的遗忘是由于伪样本生成器生成的伪样本分布随时间变化而产生的非平稳分布造成的。为了缓解这个遗忘问题，Binici等人提出使用一个动态收集生成样本的内存缓冲区。通过保留过去的样本，内存缓冲区有助于缓解DFKD中的遗忘。Binici等人利用生成重放，涉及记住先前的数据分布，以进一步缓解DFKD中的遗忘。

另一种解决DFKD中遗忘的方法是Do等人提出的MAD，它维护生成器的指数移动平均值，防止生成的数据分布发生剧烈变化并保留先前的知识。Patel等人提出了一个受元学习启发的框架来解决DFKD中的遗忘问题。

### 7.2 生成模型的终身学习
生成模型的终身学习旨在使生成模型能够为新任务生成数据，同时保留为先前学习的任务生成数据的能力，而不发生遗忘。目标是开发能够持续为新的和先前遇到的任务生成高质量样本的生成模型。生成模型终身学习中提出的方法包括基于生成对抗网络(GAN)和基于变分自编码器(VAE)的方法。

在基于GAN的方法领域，Zhai等人提出了终身GAN，它允许条件图像生成，同时避免遗忘先前获得的知识。在基于VAE的方法方面，Ramapuram等人引入了一种教师-学生架构。此外，Ye等人提出网络扩展和动态最优传输公式来解决持续VAE问题。

扩散模型中的有意遗忘：文本到图像扩散模型的进步引发了关于数据隐私、版权侵犯和生成模型安全性的重大担忧，主要是因为在大型生成模型中观察到的记忆效应。这种效应导致学习和生成未经授权和潜在有害的内容，从而促进了未经管制材料的创建和传播。为了解决这些担忧，Forget-Me-Not方法和选择性失忆(SA)应运而生，旨在减轻潜在有害或未经授权内容的存在。这些方法通过利用遗忘机制来消除不需要的信息，从而促进生成输出的多样性并保护数据隐私。

## 8. 强化学习中的遗忘
虽然大多数现有的CL方法主要解决图像分类中的遗忘问题，但值得注意的是，遗忘也广泛存在于强化学习(RL)中，称为持续RL。解决RL中的灾难性遗忘对于推进能够持续学习和适应新任务和环境的智能代理至关重要。

标准RL公式可以定义如下。我们用S表示状态空间，A表示动作空间，奖励函数为r： S × A → R。在每个时间步t，代理根据策略函数从动作空间中采样动作，该函数输出最优动作或动作空间上的分布。确定性策略以当前状态s作为输入，并根据 $ a_t = \mu(s_t) $ 输出动作。随机策略以状态 $ s_t $ 为输入，根据 $ a_t \sim \pi(\cdot|s_t) $ 输出最优动作分布。然后，状态转移函数以状态 $ s_t $ 和 $ a_t $ 为输入，确定性地输出下一个动作 $ s_{t+1} = f(s_t, a_t) $ 或随机地输出 $ s_{t+1} \sim p(\cdot|s_t, a_t) $。RL代理的目标是累积尽可能多的奖励。

根据Khetarpal等人的定义，一般持续RL可以表述如下：

定义8.1(一般持续RL)： 给定状态空间S，动作空间A和观察空间O。奖励函数为r： S × A → R;转移函数为p： S × A → S;观察函数为x： S → O。一般持续RL可以表述为

$ M \stackrel{\text{def}}{=} \langle S^{(t)}, A^{(t)}, r^{(t)}, p^{(t)}, x^{(t)}, O^{(t)} \rangle $

定义8.1强调了在持续RL中，各种组件如状态、动作、奖励、观察等随时间变化。这强调了RL过程在持续设置中的动态性质。

持续RL方法：现有的持续RL方法可以分为四个主要类别：

1. 基于正则化的方法：这些方法采用知识蒸馏等技术来缓解遗忘。
2. 基于排练的方法：这些方法利用排练或经验重放来缓解遗忘。
3. 基于架构的方法：这些方法专注于学习共享结构，如网络模块化或组合，以促进持续学习。
4. 基于元学习的方法。

## 9. 联邦学习中的遗忘
联邦学习(FL)是一种分散的机器学习方法，其中训练过程发生在本地设备或边缘服务器上，而不是集中式服务器。在联邦学习中，模型被分发到多个客户端设备或服务器，而不是将原始数据发送到中央服务器。每个客户端设备在其本地数据上执行训练，只有模型更新被发送回中央服务器。中央服务器汇总来自多个客户端的这些更新以更新全局模型。这种协作学习过程实现了隐私保护，因为原始数据保留在本地设备上，减少了与数据共享相关的风险。

我们可以将联邦学习(FL)中的遗忘问题分为两个分支：

1. 第一个分支涉及由参与FL的不同客户端之间固有的非独立同分布(non-IID)数据造成的遗忘问题。在这种情况下，每个客户端的数据分布可能显著不同，导致在从多个客户端聚合模型更新时保留先前学习知识的挑战。
2. 第二个分支解决了联邦学习过程中每个单独客户端内的持续学习问题，这导致了整体FL级别的遗忘。这个分支被称为联邦持续学习(FCL)。FCL涉及每个客户端在参与联邦学习的同时进行持续学习和模型适应，可能导致在全局FL级别遗忘先前学习的知识。

### 9.1 由FL中非IID数据导致的遗忘
客户端漂移问题涉及个别客户端之间数据分布存在显著变化的情况。在联邦学习中，每个客户端使用自己的数据进行本地训练，随后模型更新被聚合以构建全局模型。然而，当客户端之间的数据分布存在显著变化时，会导致每个客户端以不同方向更新其模型，最终导致客户端模型之间的漂移。这种漂移可能导致全局模型的性能下降或不稳定。换句话说，联邦学习中的遗忘发生在服务器端的模型平均过程中，主要由客户端之间模型的不一致性驱动。这种不一致性源于客户端之间存在异构数据分布。

Shoham等人通过将客户端漂移问题与持续学习中的遗忘概念联系起来，提供了对FL中客户端漂移问题的解释。他们提出了一种专门为解决非IID数据设置中联邦学习的客户端漂移而设计的基于正则化的方法。他们的方法旨在缓解遗忘的影响，并在联邦学习的训练过程中保留先前学习的知识。随后，多种方法被提出来缓解遗忘问题，通过整合考虑客户端模型偏移的惩罚或正则化项。

FCCL利用两个教师，即在客户端私有数据上预训练的最优模型和服务器协作更新后的模型，并应用知识蒸馏来约束客户端模型更新，从而缓解遗忘。FedReg通过使用生成的伪数据来正则化客户端训练期间的参数更新来解决遗忘问题，注意到解决联邦学习中的遗忘可以提高算法的收敛速度。FedNTD采用知识蒸馏来缓解知识遗忘，但只关注蒸馏错误预测的类，而忽略了预测良好的类。此外，受GEM和OGD的启发，GradMA采用来自先前本地模型和集中模型的梯度信息来限制本地模型更新的梯度方向。通过重新思考联邦学习中使用的模型设计，Qu等人提出了架构策略来缓解遗忘并改善联邦学习系统的整体性能和稳定性。

### 9.2 联邦持续学习
在传统的联邦学习中，主要关注的是从不同客户端聚合模型更新，而不考虑跨多个训练轮次的长期知识保留。然而，在客户端遇到非平稳分布的情况下，有必要纳入持续学习技术，以避免灾难性遗忘并保留先前训练轮次的知识。在标准CL中，代理学习一系列任务，表示为 $ T_1, \cdots, T_N $。然而，在联邦持续学习(FCL)中，每个客户端学习自己的私有任务序列。

FCL比传统的FL或CL更复杂，因为需要同时解决非独立同分布和灾难性遗忘问题。FedWeIT首次从CL的角度正式引入了FCL设置。解决FCL中的遗忘非常具有挑战性，原因有二：(1)每个客户端的非平稳数据分布；(2)服务器中的模型平均。解决遗忘的现有方法可以分为三类：

1. 参数隔离方法：FedWeIT将每个客户端的参数分解为全局参数和稀疏本地任务适应参数，以减少客户端间的干扰，从而缓解遗忘。
2. 基于重放的方法：受传统基于内存的CL启发，一些方法通过重放客户端的旧数据来解决遗忘问题。
3. 知识蒸馏方法：CFeD假设客户端和服务器中存在未标记的野生数据集。它提出了一种基于蒸馏的方法，利用未标记的代理数据集来聚合客户端，并通过排练旧数据来避免遗忘。

## 10. 有益遗忘
遗忘并不总是有害的；事实上，有意遗忘在许多情况下可能是有益的。在本节中，我们探讨了各种学习场景中有益遗忘的概念。我们首先讨论选择性遗忘及其对缓解过拟合的积极影响。接下来，我们强调在获取新知识时丢弃旧的、无关知识的重要性。此外，我们深入探讨了机器遗忘领域，该领域专注于从预训练模型中擦除私人用户数据的任务。这一研究领域旨在通过有效删除已在用户数据上训练的模型中的敏感信息来解决隐私问题。

### 10.1 通过遗忘对抗过拟合
神经网络中的过拟合发生在模型过度记忆训练数据，导致泛化能力差的情况。为了解决过拟合，有必要选择性地遗忘无关或噪声信息。Shibata等人提出的一种缓解过拟合的策略是选择性遗忘。这种方法涉及识别并丢弃训练数据中较不相关或噪声信息，使模型能够专注于最重要的模式并提高其对未见数据的泛化能力。

当前的方法，包括l1正则化、特征选择和早停等技术，可以被视为选择性遗忘的形式。这些方法在从模型记忆中移除信息量较少的组件方面发挥着重要作用。通过应用l1正则化，鼓励模型参数变得稀疏，有效地遗忘了不太相关的特征。特征选择技术专注于识别和保留最具信息量的特征，同时丢弃其余部分。此外，早停在模型在验证集上的性能开始恶化时停止训练过程，防止进一步记忆噪声或无关模式。

在学习噪声标记数据的背景下，记忆和过拟合噪声标记数据可能会对模型的泛化性能产生不利影响。SIGUA提出了一种方法来选择性地忘记噪声标记数据的不期望记忆，加强期望的记忆并提高模型的整体性能。

Jaiswal等人描述的对抗性遗忘提供了一种机制来缓解对训练数据中存在的不需要和干扰因素的过拟合。这种方法涉及选择性地遗忘无关信息以促进学习不变表示，从而减少无关因素对模型性能的影响。

### 10.2 通过遗忘先前知识学习新知识
心理学和神经科学的众多研究揭示了遗忘和学习的相互关联性质。在机器学习领域，Baik等人提出的学会遗忘方法强调，并非所有通过元学习获得的先前知识都有利于学习新任务。他们提出选择性地遗忘先前知识的某些方面，以促进新任务的更快学习。

在不同的研究方向上，Zhou等人引入了忘记-重学范式。他们表明，添加遗忘步骤可以提高模型重新学习的泛化性和有效性。另一种方法，学会不学习(LNL)，专注于遗忘有偏见的信息。LNL旨在最小化特征嵌入和偏见之间的互信息，从而通过减少有偏见或无关信息的影响来增强测试时的性能。

Bevan等人利用遗忘有偏见信息的概念进行黑色素瘤分类。他们探讨了选择性遗忘数据集中可能存在的某些有偏见信息的想法，以提高黑色素瘤分类模型的准确性和有效性。Chen等人深入研究了元学习最优任务选择背景下的学习-遗忘范式。他们的研究调查了任务的选择性学习和遗忘如何有助于在元学习场景中实现最优任务选择。

此外，在持续学习的背景下，Wang等人发现，当旧知识干扰新任务的学习时，准确保留旧知识可能会加剧干扰。为了解决这个问题，他们提出了一种主动遗忘机制，选择性地丢弃阻碍新任务学习的旧知识。通过这样做，他们展示了主动遗忘在CL中的潜在好处。

### 10.3 机器遗忘
#### 10.3.1 问题概述
机器遗忘是最近的一个研究领域，解决了遗忘先前学习的训练数据以保护用户数据隐私的需求，并与欧盟通用数据保护条例和被遗忘权等隐私法规保持一致。这些法规要求公司和组织在特定情况下为用户提供删除其数据的能力。

有趣的是，机器遗忘展示了与遗忘相关的两种不同现象。一方面，它涉及遗忘预训练模型记忆的特定训练数据。机器遗忘问题可以分为两种主要类型：精确遗忘和近似遗忘。另一方面，机器遗忘可能导致对目标遗忘训练集以外的数据的灾难性遗忘。预训练模型的渐进更新以实现遗忘是导致这第二种遗忘的原因。

#### 10.3.2 精确遗忘
精确遗忘指的是通过在剩余数据集上训练获得的模型分布与遗忘模型的分布相同的情况。设 $ P(A(D)) $ 表示在数据集D上用算法A训练的模型分布。设F表示要从预训练模型中删除的数据集。$ P(A(D\setminus F)) $ 表示通过在剩余数据集 $ D\setminus F $ 上训练获得的模型分布。$ P(U(D, F, A(D))) $ 表示遗忘模型分布。这个概念可以定义如下：

定义10.1(精确遗忘)。对于学习算法A，数据集D和要遗忘的数据集F，精确遗忘U可以定义为：

$ P(A(D\setminus F)) = P(U(D, F, A(D))) $

从机器学习模型中实现目标训练数据的精确遗忘的一种直接方法是使用剩余数据集重新训练模型。这种方法有效地删除了与删除集相关的所有信息。然而，这种方法在大规模预训练模型和数据集的情况下可能在计算上很昂贵。

现有工作：为了解决上述挑战，DeltaGrad在学习过程中缓存模型参数和梯度，以加速在剩余数据集上的重新训练。SISA涉及将数据集分区为不同的分片或子集，并通过在每个子集上训练模型来维护多个独立模型。在推理过程中，这些单独模型的预测被组合或聚合。在收到删除请求时，SISA方法的重新训练过程仅集中于最初在包含该数据点的子集上训练的组成模型。这种方法确保了与已删除数据点相关的所有信息的消除。Sekhari等人研究了机器遗忘，重点关注总体风险最小化，而不是先前工作关注的经验风险最小化。Ullah等人进一步提出了一种基于总变差(TV)稳定性概念的精确遗忘方法。ARCANE是一种利用集成学习来解决与遗忘相关的重新训练成本的方法。ARCANE不执行完整的重新训练过程，而是将其转化为多个一类分类任务。通过这样做，它减少了计算和资源需求，同时仍然保持所需的模型性能。

#### 10.3.3 近似遗忘
如上所述，精确遗忘在计算和内存方面可能很密集，特别是在大规模预训练模型和数据集的情况下。此外，验证遗忘后模型的分布是否与使用剩余数据集完全重新训练的模型匹配也是一个挑战。因此，最近的研究已经接受了近似遗忘的概念。这个概念允许一种更有效和可行的遗忘方法，在计算资源和所需遗忘水平之间提供平衡。"近似遗忘"的概念定义如下：

定义10.2(近似遗忘)。给定一个 $ \epsilon $，对于学习算法A，数据集D和要遗忘的数据集F，遗忘算法U执行 $ \epsilon $ 认证移除以删除z的影响，定义如下：

$ \log||P(A(D\setminus z)) - P(U(D, z, A(D)))|| \leq \epsilon $

直观地说，近似遗忘的定义努力最小化遗忘模型的参数分布与通过在剩余数据集上完全重新训练获得的模型之间的差异。近似遗忘放松了要求，不是强制精确匹配，而是专注于使它们的分布更接近。这种放宽允许采用更实际的遗忘方法，承认精确复制在剩余数据集上完全训练的模型可能并不总是可行或必要的。

现有工作：认证移除引入了近似遗忘的理论框架。它提供了一个令人信服的保证，即从中删除了特定数据的模型与从未遇到过该数据的模型无法区分。这种理论保证为遗忘过程的有效性提供了坚实的基础。通过确保模型的行为与缺乏被删除数据知识的模型保持一致，认证移除为遗忘程序的隐私和安全性提供了高度的信心。擦除程序利用广义和较弱形式的差分隐私，提供了机器遗忘的替代框架。它修改神经网络权重以消除或"擦除"与特定训练数据相关的信息。变分贝叶斯遗忘(VBU)是近似遗忘的概率框架。它旨在最小化通过直接遗忘获得的模型参数近似后验与通过完整数据重新训练模型获得的精确后验之间的KL散度。VBU在完全遗忘被擦除数据和保留模型在完整数据上训练时捕获的重要知识之间取得平衡。在后续研究中，L-CODEC是一种近似遗忘方法，它在个体样本级别识别具有最高语义重叠的特定模型参数子集。然后在机器遗忘的背景下利用这个参数子集来实现更有效和有针对性的遗忘。

#### 10.3.4 其他正常示例的灾难性遗忘
机器遗忘中的灾难性遗忘指的是由于遗忘而导致剩余数据集的对数后验概率 $ P(\theta | D\setminus F) $ 意外减少。这种现象导致遗忘模型在目标样本以外的数据上性能下降，这是不可取的。随着遗忘数据量的增加，灾难性遗忘的显著性增加。重要的是要注意，这个概念不同于机器遗忘的目标，后者仅专注于遗忘特定样本。在UNLEARN方法中，新参数值对旧值的更新幅度受到限制，并维护一个内存缓冲区来存储先前的示例。Forsaken方法引入了一个动态梯度惩罚项，以限制正常数据上的参数变化，有效地缓解了灾难性遗忘问题。由Nguyen等人提出的贝叶斯遗忘框架提供了在完全遗忘擦除数据和保留通过在整个数据集上学习获得的后验信念之间的自然权衡解释。此外，擦除程序通过向模型权重引入随机噪声，在最小化剩余数据的损失的同时最大化目标遗忘数据集上的遗忘，允许在指定数据集上遗忘，同时保留剩余数据上的知识。此外，PUMA旨在缓解由于删除目标训练数据可能造成的潜在负面影响。PUMA通过最佳地重新加权剩余数据来实现这一点，确保在遗忘特定样本时模型的性能不会受到过度影响。

最后，显然我们不能简单地擦除过多的数据点，因为这最终会导致测试集上的性能下降。在Sekhari等人的研究中，作者进行了理论分析，以确定可以从预训练模型中删除的最大样本数量，而不会损害测试集上的性能。这种分析提供了对机器遗忘在特定训练集子集上和保持测试集上的泛化性能之间权衡的洞察。

#### 10.3.5 机器遗忘的应用
机器遗忘技术在各种领域找到应用，包括后门攻击防御和防御成员推理攻击。在后门攻击防御的背景下，机器遗忘可用于擦除已注入训练模型的后门触发器。通过选择性地遗忘与后门触发器相关的特定信息，可以清除模型的恶意行为。同样，机器遗忘也可以作为防御成员推理攻击的机制。通过从预训练模型中删除特定训练数据，这些数据的痕迹或特征被消除，使攻击者更难推断成员身份或提取敏感信息。

## 11. 讨论和未来展望
### 11.1 总结和研究趋势
关于遗忘的跨学科研究：跨学科研究对推进机器学习学科至关重要。各个领域通过跨学科研究提供了宝贵的见解和创新。这种探索既包括将CL技术应用于解决其他研究领域的挑战，也包括应用各个领域的技术来解决CL问题。这些跨学科研究努力为不同研究领域的进步铺平了道路。在未来的工作中，我们预计积极促进合作并促进不同研究领域之间的想法交流将是克服学科障碍的关键。

机器学习中的有意遗忘：有意遗忘已成为近期研究中提高模型性能和解决数据隐私问题的一种有前途的方法。一方面，有意遗忘在消除模型训练过程中的偏见和无关信息方面证明是有价值的，从而缓解过拟合并改善泛化。另一方面，在机器遗忘的背景下，遗忘在删除私人数据以保护数据隐私方面发挥着关键作用。

理论分析的增长趋势：最近，出现了一种趋势，即进行理论分析以深入理解和分析各种研究领域中的遗忘现象。这些理论分析旨在提供更有价值的见解，揭示支配遗忘的基本原则。

用基础模型解决遗忘问题：基础模型在解决各种研究领域的遗忘问题方面展示了巨大的潜力，使其成为一个活跃和有前途的研究领域。

### 11.2 开放研究问题和未来展望
在以下内容中，我们展望了未来研究的几个潜在方向：

对遗忘的深入理论分析：大多数现有方法都是经验性的，缺乏理论保证和全面分析。因此，迫切需要在不同学习领域进行更彻底的理论分析。未来的研究努力应旨在通过纳入对遗忘的严格理论分析来弥合这一差距。

遗忘的可信度：大多数现有关于遗忘的研究主要集中在其对模型性能的影响上，而忽视了其他关键方面，如鲁棒性、公平性、透明度等。为了弥合这一研究差距，未来的调查应该更多地致力于分析和理解遗忘对这些可信度属性的影响。

记忆和遗忘之间的适当权衡：在记住先前知识和保护隐私之间取得平衡构成了一个关键的研究问题。记住更多的先前知识可能会增加记忆私人信息的风险，从而可能危及隐私。相反，优先考虑隐私保护可能会导致牺牲先前任务的性能。在记忆和遗忘之间达成适当的平衡对于有效解决这一挑战至关重要。未来的研究应该专注于开发能够有效管理知识保留的同时确保隐私保护的方法和技术，从而在两者之间取得最佳平衡。

总的来说，这篇综述全面探讨了深度学习中的遗忘问题，涵盖了从持续学习到基础模型、联邦学习等多个领域。它不仅讨论了遗忘的有害影响，还强调了有意遗忘在某些场景下的潜在好处。通过系统梳理现有研究和未来方向，这份调查为深入理解和解决机器学习中的遗忘问题提供了宝贵的见解和框架。


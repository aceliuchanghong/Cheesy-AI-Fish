> 原文：《<font style="color:rgb(41, 38, 27);background-color:rgb(245, 244, 239);">Continual Learning of Large Language Models： A Comprehensive Survey</font>》
>

## 1. 引言
大语言模型(LLMs)在自然语言处理领域取得了显著进展，展示了令人印象深刻的能力，如多步推理、少样本上下文学习和指令遵循。然而，这些模型通常是在静态预收集的数据集上进行训练的，这导致它们在面对不断变化的数据分布时性能会逐渐下降。为了有效地适应LLMs以应对不断发展的数据分布，同时最小化对先前知识领域的性能下降，研究人员采用了持续学习的方法。

持续学习，也称为终身学习或增量学习，涉及对一系列任务进行顺序训练机器学习模型，期望在所有任务上保持性能。在训练过程中，模型对先前数据的访问有限或没有，这给保留过去知识带来了挑战，因为在当前任务学习期间缺少来自未见先前数据的优化约束。这一挑战被称为灾难性遗忘，自持续学习研究开始以来一直是其中心焦点。

本文对LLMs背景下的持续学习研究进行了全面综述，重点关注：

1. 持续预训练(CPT)：供应商在新收集的数据和现有数据上持续预训练LLMs。
2. 领域自适应预训练(DAP)：通过在特定领域的未标记数据上进行额外预训练，为特定领域应用准备LLMs。
3. 持续微调(CFT)：在消费者端针对最终下游任务对模型进行更新。

论文首次将持续学习LLMs分为两个维度的连续性：

+ 垂直连续性：LLMs从大规模通用领域向小规模特定领域过渡，涉及学习目标和执行实体的转变。
+ 水平连续性：跨时间和领域的持续适应，通常涉及多个训练阶段，更容易出现遗忘。

这种明确区分垂直和水平持续学习不仅仅是对现有持续学习类型的简单修改，而是为分析和描述持续LLMs中复杂学习范式提供了一个强大的概念框架。

## 2. 预备知识
### 2.1 大语言模型
在过去二十年里，神经语言建模已成为深度学习的主导领域，取得了重大而快速的进展。主要基于Transformer架构构建的预训练语言模型(PLMs)通过在大规模未标记文本语料库上进行广泛预训练，建立了一个通用的隐藏嵌入空间。遵循预训练和微调范式，PLMs在少量特定任务数据上进行微调后，在各种自然语言处理任务上表现出令人promising的性能。

关于扩展规律（scaling lawing）的研究表明，增加模型规模可以提高语言模型的能力。通过将参数扩展到数十亿甚至数百亿，并在海量文本数据集上进行训练，PLMs不仅展示出卓越的语言理解和生成能力，还表现出诸如上下文学习、指令遵循和多步推理等新兴能力，这些能力在像BERT这样的小规模语言模型中是不存在的。这些更大的模型通常被称为大语言模型(LLMs)。

#### 2.1.1 LLMs的预训练
预训练对于语言模型获取广泛的语言表示至关重要。仅解码器模型在预训练期间通常采用概率语言建模(LM)任务。在这种情况下，LM特指自回归LM。给定一个标记序列 $ \mathbf{x} = [x_1, x_2, \cdots, x_N] $，LM基于所有前序标记 $ \mathbf{x}_{<t} = [x_1, x_2, \cdots, x_{t-1}] $ 自回归预测下一个标记 $ x_t $，并通过最小化负对数似然来训练整个网络：

$ L_{LM}(\mathbf{x}) \triangleq -\sum_{t=1}^N \log P(x_t|\mathbf{x}_{<t}) $

其中 $ P(x_1|\mathbf{x}_{<1}) \triangleq P(x_1) $ 是第一个标记的无条件概率估计。

三个最流行的仅解码器模型家族是GPT、PaLM和LLaMA。GPT家族由OpenAI开发，包括GPT-2、GPT-3、ChatGPT和GPT-4等模型。值得注意的是，GPT-3是第一个展示出小型PLMs中不存在的新兴能力的LLM。另一个值得注意的家族是Gemini，由Google开发，与GPT家族相当。虽然GPT和Gemini家族都是闭源的，但由Meta发布的LLaMA目前是最流行的开源LLM家族。这些模型的权重以非商业许可的形式向研究社区开放。

掩码语言建模(MLM)任务作为像BERT这样的仅编码器模型的常见预训练目标。在MLM中，对于输入序列 $ \mathbf{x} $，一部分输入标记 $ m(\mathbf{x}) $ 被掩码并替换为特殊的[MASK]标记。预训练的目标是利用未掩码部分 $ \mathbf{x}_{\setminus m(\mathbf{x})} $ 来预测被掩码的部分 $ m(\mathbf{x}) $。总之，MLM的目标是最小化负对数似然：

$ L_{MLM}(\mathbf{x}) \triangleq -\sum_{\hat{x} \in m(\mathbf{x})} \log P(\hat{x}|\mathbf{x}_{\setminus m(\mathbf{x})}) $

一些编码器-解码器架构模型，如T5，也将序列到序列MLM任务用作预训练目标。它们将掩码句子作为编码器输入，并利用解码器顺序预测被掩码的标记。

#### 2.1.2 LLMs的适应
预训练后，LLMs需要有效适应以更好地服务于下游任务。一系列适应方法已被提出用于特定目标。由于LLMs在预训练期间主要专注于生成语言连贯的文本，它们的性能可能不一定符合人类用户的实际需求或符合人类的价值观、偏好和原则。此外，由于预训练数据的时效性等问题，LLMs也可能遇到知识截止或谬误问题。因此，指令调整、模型细化和模型对齐被提出来解决这些问题。以下是三种LLMs适应任务的正式定义。

**定义 2.1 (指令调整， Instruction Tuning，IT)** 

设 $ h(\mathbf{x}) $ 为一个语言模型，其输入数据 $ \mathbf{x} $ 通常由自然语言指令或查询组成。指令调整(IT)是一种专门设计的训练方法，旨在提高模型准确有效响应特定指令的能力。IT的目标是通过使用从IT数据分布 $ D_I $ 中抽取的指定训练样本集 $ I = \{(\mathbf{x}_i, \hat{\mathbf{y}}_i)\}_{i=1}^N $ 来调整 $ h $ 的参数，其中 $ \hat{\mathbf{y}}_i $ 表示 $ \mathbf{x} $ 的期望输出。这个集合经过策划，针对需要改进性能的特定任务或功能。形式上，IT寻求找到满足以下条件的最优细化假设 $ h^* $：

$ h^* \triangleq \arg\min_{h'} \mathbb{E}_{(\mathbf{x},\mathbf{y})\sim D_I}[-\log P(\hat{\mathbf{y}}|\mathbf{x}, h')] \approx \arg\min_{h'} \sum_{i=1}^N -\log P(\hat{\mathbf{y}}_i|\mathbf{x}_i, h') $

**备注** 

模型对齐(MA)任务通常以与IT相同的问题定义形式化，对齐数据集大小为 $ M $ 的 $ A = \{(\mathbf{x}_a, \mathbf{y}_a, \hat{\mathbf{y}}_a)\}_{a=1}^M $，其中 $ \mathbf{y}_a $ 表示模型对输入 $ \mathbf{x}_a $ 的原始决策，而 $ \hat{\mathbf{y}}_a $ 表示符合指定道德准则或期望结果的对齐决策。

**定义 2.2 (模型细化，(Model Refinement，MR)** 

假设我们有一个模型 $ h(\mathbf{x}) $ 以数据 $ \mathbf{x} $ (例如，自然语言查询)作为输入。考虑一个大小为 $ N $ 的编辑集 $ E = \{(\mathbf{x}_e, \mathbf{y}_e, \hat{\mathbf{y}}_e)\}_{e=1}^N $，其中 $ \hat{\mathbf{y}}_e $ 表示 $ \mathbf{x}_e $ 的真实标签，但模型对 $ \mathbf{x}_e $ 错误输出 $ \mathbf{y}_e $。模型细化(MR)旨在高效地将模型从 $ h $ 更新到 $ h' $，使其正确预测编辑集 $ E $，同时保留 $ E $ 之外的原始输出。形式上，我们旨在找到满足以下条件的 $ h' $：

$ h'(\mathbf{x}_0) = \begin{cases} \hat{\mathbf{y}}_0 & \text{if } (\mathbf{x}_0, \hat{\mathbf{y}}_0) \in E, \\ h(\mathbf{x}_0) & \text{otherwise} \end{cases} $

### 2.2 持续学习
人类能够逐步积累知识和技能，跨越任务而不会显著降低先前任务的性能。相比之下，机器学习模型通常以数据为中心，最小化后续任务的训练损失将导致模型在旧任务上失败，这种现象被称为"灾难性遗忘"。解决这一挑战是持续学习研究的焦点。持续学习社区广泛研究了在不遗忘的情况下高效适应模型到任务序列的问题。这些研究通常在以下持续学习的内存约束下进行。

**定义 2.3 (持续学习的内存约束，Memory Constraint of Continual Learning)** 

假设 $ T $ 组观察 $ \{S_t \sim T_t\}_{t=1}^T $ 作为序列到来，其中 $ \{T_t\}_{t=1}^T $ 表示 $ T $ 个任务分布。在学习阶段 $ t > 1 $，观察集 $ \{S_i\}_{i=1}^{t-1} $ 不可访问(严格)或仅部分可访问(放松)。

**备注** 

在CL的早期阶段，大多数工作专注于严格的内存约束：随着研究领域的发展，更多关注放宽内存约束到小缓冲区以进行重放：一些现代CL工作完全放弃内存约束，但关注计算预算。之前的数据是不可观察和使用的。

#### 2.2.1 三种持续学习类型
持续学习有三种突出的场景类型：

1. 任务增量学习(TIL)
2. 领域增量学习(DIL)
3. 类增量学习(CIL)。

为了为后续讨论奠定基础，我们遵循提出的概念框架，为这三种持续学习场景提供正式定义。

##### TIL：定义 2.4 (任务增量学习，(Task-Incremental Learning) 
假设 $ T $ 个任务分布 $ \{T_t\}_{t=1}^T $ 作为序列集合，其中 $ T_t $ 表示第 $ t $ 个任务的输入空间和标签空间 $ (X_t, Y_t) $ 上的联合分布。将 $ X \triangleq \bigcup_{t=1}^T X_t $ 和 $ Y \triangleq \bigcup_{t=1}^T Y_t $ 分别定义为输入和标签空间的并集。在定义 2.3 中定义的内存约束下，任务增量学习(TIL)旨在找到满足以下条件的最优假设 $ h^* : X \times [T] \to Y $：

$ h^* = \arg\min_h \sum_{t=1}^T \mathbb{E}_{(\mathbf{x},y)\sim T_t}[\mathbb{1}_{h(\mathbf{x},t)\neq y}] $

##### DIL：定义 2.5 (领域增量学习，Domain-Incremental Learning) 
假设 $ T $ 个领域分布 $ \{D_t\}_{t=1}^T $ 作为序列到来，其中 $ D_t $ 表示第 $ t $ 个共享输入空间和标签空间 $ (X, Y) $ 上的联合分布。在定义 2.3 中定义的内存约束下，领域增量学习(DIL)旨在找到满足以下条件的最优假设 $ h^* : X \to Y $：

$ h^* = \arg\min_h \sum_{t=1}^T \mathbb{E}_{(\mathbf{x},y)\sim D_t}[\mathbb{1}_{h(\mathbf{x})\neq y}] $

##### CIL：定义 2.6 (类增量学习，Class-Incremental Learning) 
假设 $ T $ 个任务分布 $ \{T_t\}_{t=1}^T $ 作为序列到来，其中 $ T_t $ 表示第 $ t $ 个任务的输入空间和标签空间 $ (X_t, Y_t) $ 上的联合分布。将 $ X \triangleq \bigcup_{t=1}^T X_t $ 和 $ Y \triangleq \bigcup_{t=1}^T Y_t $ 分别定义为输入和标签空间的并集。在定义 2.3 中定义的内存约束下，类增量学习(CIL)旨在找到满足以下条件的最优假设 $ h^* : X \to [T] \times Y $：

$ h^* = \arg\min_h \sum_{t=1}^T \mathbb{E}_{(\mathbf{x},y)\sim T_t}[\mathbb{1}_{h(\mathbf{x})\neq(t,y)}] $

**备注** 

在TIL中，通常有一个共享的输入空间 $ X = X_t, \forall t \in [T] $，但标签分布空间 $ Y_t $ 可以是不同的 $ (Y_i \cap Y_j = \emptyset, \forall i \neq j) $，部分共享的 $ (Y_i \cap Y_j \neq \emptyset, \exists i \neq j) $，或在不同任务间共享的 $ (Y = Y_t, \forall t \in [T]) $。在DIL中，任务以相同的格式定义，即相同的输入空间 $ X $ 和相同的输出空间 $ Y $。在推理过程中，不为假设提供任务ID，这意味着持续学习模型需要捕捉领域不变特征和标签之间的模式。DIL通常被认为比TIL更难。CIL通常被视为最具挑战性的持续学习场景，因为模型需要同时推断标签和任务ID。

#### 2.2.2 持续学习技术
持续学习的目标是找到一个在所有任务/领域上最小化风险的假设。以DIL为例，在第 $ t $ 个学习阶段，理想的训练目标 $ L(h) $ 定义为：

$ L(h) \triangleq \sum_{i=1}^{t-1} L_{D_i}(h) + L_{D_t}(h) $

其中，第一项表示过去领域的目标，第二项表示当前领域的目标。由于内存约束(定义 2.3)，过去领域的目标通常难以测量或优化。因此，设计持续学习算法的核心在于在不违反内存约束的情况下为第一项找到一个代理学习目标。

现有的持续学习技术大致可以分为5类：

+ (i)基于重放
+ (ii)基于正则化
+ (iii)基于架构
+ (iv)基于优化
+ (v)基于表示

这里，我们将简洁而全面地介绍前三类持续学习技术，因为它们在持续学习大语言模型中得到了广泛应用。

##### 基于重放的方法 
基于重放的方法采用放松的内存约束，为每个任务 $ T_i $ 保留一个小的观察数据缓冲区 $ \{M_i\}_{i=1}^{t-1} $。形式上，它们寻求优化以下经验训练目标：

$ \hat{L}_{replay}(h) \triangleq \sum_{i=1}^{t-1} \hat{L}_{M_i}(h) + \hat{L}_{S_t}(h) $

其中 $ \hat{L}_S $ 表示在样本集 $ S $ 上评估的经验损失项。虽然理论上可能导致松散的泛化界，但基于重放的方法因其简单性、稳定性和高性能而受到重视，即使使用小型情节记忆也是如此。例如，DER++通过重放一小组过去的例子及其logits(称为dark experience replay)来展示一致的性能增强。

ESM-ER引入了错误敏感度调制(ESM)来缓解由高错误新样本引起的突然表示漂移。基于重放的CL的一个重要焦点是提高缓冲区维护的样本效率。例如，某些方法基于herding优先选择示例以准确模拟整个类增量学习过程中的类均值。

其他方法提出存储低保真度样本以实现内存高效的示例集维护。RM(Rainbow Memory)基于每个样本的不确定性估计和数据增强引入了面向多样性的内存更新，用于类增量学习。

##### 基于正则化的方法 
假设 $ h_{\boldsymbol{\theta}_{t-1}} $ 是第 $ t-1 $ 阶段训练后得到的假设，由参数 $ \boldsymbol{\theta}_{t-1} $ 参数化。基于正则化的方法使用正则化项作为过去领域损失的代理，该项由参数空间中的距离确定。

$ \hat{L}_{reg}(h_{\boldsymbol{\theta}}) \triangleq \lambda \cdot \|\boldsymbol{\theta} - \boldsymbol{\theta}_{t-1}\|_{\boldsymbol{\Sigma}} + \hat{L}_{S_t}(h_{\boldsymbol{\theta}}) $

其中 $ \|\mathbf{v}\|_{\boldsymbol{\Sigma}} = \mathbf{v}^\top\boldsymbol{\Sigma}\mathbf{v} $ 是在正半定矩阵 $ \boldsymbol{\Sigma} $ 上评估的向量范数，而 $ \lambda $ 是正则化系数，这是一个引入的超参数，用于平衡过去知识保留和当前知识学习。引入的矩阵 $ \boldsymbol{\Sigma} $ 用于衡量每个参数在保留过去知识中的不同重要性水平及其相关性。

在实践中，为了减少计算开销，通常设计对角矩阵仅编码每个参数的重要性。例如，弹性权重巩固(EWC)采用贝叶斯视角，使用Fisher信息矩阵(FIM)的对角值作为参数Hessian矩阵的近似。这形成了持续学习的序列最大后验(MAP)优化。

记忆感知突触(MAS)以在线和无监督的方式计算参数重要性，通过训练期间累积的绝对梯度定义重要性。值得注意的是，当 $ \boldsymbol{\Sigma} = \mathbf{I} $ 退化为单位矩阵时，正则化项简化为基本的 $ l_2 $ 惩罚项，对每个参数均等惩罚，这在一些持续LLMs的情况下可能出人意料地有效。

##### 基于架构的方法 
动态扩展网络架构以同化新知识被认为是最有效的持续学习形式。这种方法主要解决适应挑战，当任务ID在推理期间可用或可以正确推断时，可以实现零遗忘。然而，由于任务ID推断的困难，架构扩展主要用于TIL，但在DIL或CIL中很少探索。

渐进神经网络(PNN)提出在新任务出现时学习横向连接的神经元，确保不遗忘并使先前学习的神经元能够转移到未来任务。结合预训练的主干大模型(如ViT)，CoLoR为不同任务训练各种低秩适应(LoRA)模块。它估计并存储每个任务的原型，并在测试期间利用预训练模型的自然聚类能力来推断任务ID，选择相应的LoRA组件以生成预测。

在持续LLMs领域，随着参数高效微调(PEFT)的兴起，架构扩展再次受到欢迎，这个话题我们将很快深入讨论。

#### 2.2.3 持续学习的评估指标
在传统的持续学习中，任务流采用分类的形式，许多指标依赖于准确度矩阵的概念。将这个概念扩展到持续学习LLMs的背景中，我们引入性能矩阵 $ \mathbf{P} \in \mathbb{R}^{T \times T} $，其中 $ T $ 表示训练阶段的总数。$ \mathbf{P} $ 的每个条目对应于对模型评估的性能指标，例如在预训练数据上的困惑度，在下游数据上不经微调的零样本/少样本评估指标，在下游任务上的微调准确度，以及从微调附加组件评估的探测准确度。在 $ \mathbf{P} $ 中，$ P_{i,j} $ 表示模型在任务 $ i $ 上训练后在任务 $ j $ 上评估的性能。有了这个性能矩阵的定义，我们介绍广泛采用的主要评估协议。

**整体性能(OP)** 

整体性能(OP)是平均准确度概念的自然扩展。直到训练阶段 $ t $ 测量的OP是模型在阶段 $ t $ 之后训练的平均性能。将其表示为 $ OP_t $，我们有：

$ OP_t \triangleq \frac{1}{t}\sum_{i=1}^t P_{t,i} $

如上所述，OP对应于定义2.4、2.5和2.6中定义的主要优化目标。在许多持续学习文献中，一旦完成所有 $ T $ 个任务，就报告最终OP($ OP_T $)，通常为简洁起见省略下标 $ T $。在一些工作中，OP按任务重要性加权 $ \widetilde{OP} \triangleq \frac{1}{T}\sum_{i=1}^T w_i P_{t,i} $，其中 $ w_i = N_i / \sum_{j=1}^T N_j $ 表示数据的比率。

**遗忘(F)** 

将 $ F_t $ 定义为直到任务 $ t $ 的遗忘，它表示整个训练过程中观察到的最大性能下降，平均超过 $ t $ 个训练阶段：

$ F_t \triangleq \frac{1}{t-1}\sum_{j=1}^{t-1}\left[\max_{l \in [t-1]}\{P_{l,j} - P_{t,j}\}\right] $

通常，研究人员报告整个训练过程结束时的平均遗忘 $ F = F_T $。遗忘量化了学习新任务对先前获得知识的影响。理想情况下，一个强大的持续学习框架应该实现反向转移(BWT)，其中学习新任务会增强先前任务的性能。这种增强通常通过取遗忘的负值来衡量，从而表示早期任务性能的改善。

**前向转移(FWT)** 

前向转移衡量持续学习算法的泛化能力。形式上，直到训练阶段 $ t $ 的前向转移 FWT_t 定义为：

$ FWT_t \triangleq \frac{1}{t-1}\sum_{i=2}^t P_{i-1,i} - b_i $

其中 $ b_i $ 是模型在进行持续学习之前在任务 $ i $ 上评估的基线性能。严格来说，$ b_i $ 的定义与之前工作中的定义不同，在那里它用于表示模型随机初始化的性能。

## 3. 持续学习遇上大语言模型
大语言模型(LLMs)在各个维度上都很庞大，包括模型参数的大小、预训练数据集、计算资源、项目团队和开发周期。LLMs的巨大规模为开发团队带来了显著挑战，特别是在环境快速变化中保持它们的更新。为了说明这一点，在2023年，每天新增的推文平均超过5亿条，即使只训练其中的一小部分大量数据也是负担不起的。

Recyclable Tuning是第一个明确概述现代LLM生产管道中供应商-消费者结构的工作。这种结构允许我们从各种角色的视角分解持续LLMs的挑战。在供应商方面，模型在一系列大规模未标记数据集上持续预训练。每次发布预训练模型后，消费者需要利用更强大和更新的上游模型进行下游任务。与上游供应商相比，下游用户通常缺乏收集和存储大规模数据、维护大规模硬件系统和自行训练LLMs的能力。

因此，Recyclable Tuning主要关注如何高效地持续适应更新的预训练LLM到下游任务。在本调查中，作者进一步提出了一个综合框架，涵盖了各种关于持续LLM预训练、适应和部署的研究。本调查框架与现有研究的不同之处在于纳入了两个方向的连续性：垂直连续性和水平连续性。

![图 1](https://cdn.nlark.com/yuque/0/2024/png/406504/1728525081824-a3cff033-f26a-4cd5-88f1-07e83bc40303.png)

### 3.1 垂直连续性(垂直持续学习)
**定义**：垂直连续性(或垂直持续学习)长期以来在现有文献中被隐式或显式地研究。垂直连续性的特征是一个包含数据包容性、任务范围和计算资源的层次结构。具体来说，训练任务从通用预训练逐渐过渡到下游任务，通常由生产管道中不同的实体承担。图1展示了LLMs垂直连续性的典型管道，即"预训练" → "领域自适应训练" → "下游微调"：

1. **预训练**：在预训练阶段，需要大量来自不同领域的数据来开发通用LLM。这个阶段需要一个专门的大型研究和开发团队来训练和基准测试模型，以及大量的计算资源。
2. **领域自适应预训练**：随后，下游机构可能选择进行领域自适应预训练，使用上游供应商无法获得的特定领域数据来定制模型以适应特定任务。
3. **微调**：最后，LLM在部署之前会在下游任务的标注数据上进行微调。

在整个过程中，未标记的特定领域数据集规模小于上游预训练阶段，但大于最终下游任务微调阶段。这种模式也适用于计算资源、团队规模和其他因素。需要注意的是，垂直连续性可以涉及三个以上的阶段。在实际应用中，在领域自适应预训练期间，可以添加额外的层来适应具有不同目标但在同一领域内运作的多个实体，例如各种部门。

**垂直遗忘**：我们将模型在垂直持续学习过程中通用知识的性能下降称为"垂直遗忘"。如图2所示，对于垂直持续学习，上游任务的数据分布部分覆盖了下游任务，这意味着模型可能从一个不错的初始化开始进行后续阶段的训练。有两个重要的挑战需要解决以防止垂直遗忘：

1. **任务异质性**：源于上游任务和下游任务固有的定义差异，任务异质性可能导致模型结构和训练方案的差异，这长期以来被认为是一个主要障碍。为缓解这个问题，从业者经常采用在下游阶段冻结共享参数或将下游任务重新定义为与预训练任务结构匹配的方法。
2. **无法访问上游数据**：这个挑战主要来自于进行垂直持续学习的实体之间不同级别的保密性。在不同协议下收集和策划的数据可能无法被某些下游实体访问。这种情况比传统CL中提出的严格内存约束更具挑战性，因为后者的算法依赖于在特定点访问先前数据以进行参数重要性测量或重放。为解决无法访问上游数据的挑战，现有方法要么使用公共数据集，要么生成伪样本来创建代理预训练数据集。

![图 2](https://cdn.nlark.com/yuque/0/2024/png/406504/1728525165272-2d833e06-7fea-4b13-9013-d618386eb4be.png)

### 3.2 水平连续性(水平持续学习)
**定义**：水平连续性（或水平持续学习）指的是跨时间和领域的持续适应，这是持续学习社区广泛探索的主题。保持水平连续性的主要原理在于数据分布随时间的动态性质。为了跟上这些内容变化，LLM必须增量学习新出现的数据。否则，重新训练的成本将变得极其昂贵且不切实际。

尽管LLMs展示了令人印象深刻的能力，但实证证据一致表明，它们在面对未来未见数据时，特别是在面临时间或领域转移时，难以有效泛化。此外，在适应新的时间领域时，它们也难以保留过去经验的完整知识，尽管它们确实展示了更高水平的鲁棒性来对抗灾难性遗忘。

在放宽内存约束的方向上，能够访问训练数据的机构可能选择保留完全访问权而不限制内存大小，因为内存存储的成本是可以承受的。在这种情况下，如某些研究所强调的，挑战从存储效率转移到计算效率。为实现持续学习目标，模型必须高效地适应新数据（高效适应）并选择关键经验进行重放（高效重放）。

在另一端，具有更严格内存约束的研究在现代LLMs的持续学习中仍然至关重要。如图1所示，LLMs的上游供应商通常不会提供带有发布模型权重的训练数据。因此，消费者必须在无法访问实际重放数据的情况下将这些模型适应到下游数据。在这种情况下应用了各种无重放的持续策略，例如从替代来源收集数据示例，利用LLMs的生成能力产生伪样本进行重放，以及在参数空间中实施正则化技术。

**水平遗忘**：我们非正式地将模型在进行水平持续学习时先前任务的性能下降定义为"水平遗忘"。如图2所示，水平持续学习通常涉及类似规模的训练阶段，它们的数据分布可能有潜在的重叠。总的来说，水平持续学习需要解决两个主要挑战：

1. **长任务序列**：水平持续学习理想情况下涉及大量增量阶段，特别是为了适应数据分布的时间变化。更长的任务序列意味着模型更多的更新步骤，不可避免地导致先前学习任务的遗忘。
2. **突然分布转移**：与垂直连续性中分布转移通常可预测不同，水平持续学习对任务属性不施加约束。证据表明，任务分布的突然变化可能导致模型显著的水平遗忘。

为了更直观地理解垂直连续性和水平连续性的区别，我们可以用下面的表格来总结它们的主要特征：

| 特征 | 垂直连续性 | 水平连续性 |
| --- | --- | --- |
| 主要目标 | 从通用到特定领域的适应 | 跨时间和领域的持续适应 |
| 数据规模变化 | 逐渐减小 | 相对稳定 |
| 计算资源需求 | 逐渐减小 | 相对稳定 |
| 主要挑战 | 任务异质性、无法访问上游数据 | 长任务序列、突然分布转移 |
| 典型应用场景 | 预训练→领域自适应→微调 | 持续更新已部署的模型 |


通过这种方式，我们可以清楚地看到这两种连续性在LLMs持续学习中的不同角色和挑战。这种框架为理解和开发更强大、更适应性强的LLMs提供了重要的指导。

## 4. 持续大语言模型的学习阶段
图1提供了持续学习LLMs的概览。沿着垂直连续性的轴线，现代持续学习主要呈现出三个层次：

1. **持续预训练(CPT)**：涉及供应商在新收集的数据和现有数据上持续预训练LLMs（第4.1节）。
2. **领域自适应预训练(DAP)**：通过在领域特定的未标记数据上进行额外预训练，为领域特定应用准备LLMs（第4.2节）。
3. **持续微调(CFT)**：针对消费者端的最终下游任务对模型进行目标定向（第4.3节），其中模型需要在部署后为指定任务进行更新。

### 4.1 持续预训练 (CPT)
#### 4.1.1 CPT： 有效性和效率
在深入讨论持续预训练(CPT)的细节之前，有必要解答两个基本问题：

1. **有效性**：CPT能否在下游任务上提升性能，超越初始在广泛数据领域上的训练？

广泛的研究不仅证明了CPT对改善下游性能的必要性，还表明当分布变化是渐进的或某种程度上相关时，CPT可以有效帮助模型泛化到未见数据。

2. **效率**：考虑到LLM参数和数据（包括旧数据和新数据）的庞大规模，我们能否以计算效率高的方式实现适应和知识保留？

关于效率，大多数研究集中在高效知识保留的技术上，这与解决灾难性遗忘的CL文献有很大重叠。与之前充分利用新兴数据的方法不同，一些研究认识到这种方法在实际生产环境中的不切实际性。相反，他们专注于进一步提高适应效率。例如：

+ ELLE 采用函数保持的模型扩展来促进高效知识增长。
+ 一些研究基于新颖性和多样性对训练数据进行子采样以提高训练效率，实现了优于全数据训练的性能。

虽然目前探索不足，但考虑到近期研究强调数据质量而非数量对LLM泛化的重要性，持续预训练中的高效适应有望成为重要的研究方向。

#### 4.1.2 CPT的一般观察
![表 1](https://cdn.nlark.com/yuque/0/2024/png/406504/1728525524506-49eeb6c8-674d-4f18-862f-a30976918f49.png)

表1总结了现有的持续预训练(CPT)研究，以下是一些关键观察：

1. **OBS-1： 专门为CPT定制的高级技术开发处于起步阶段，需要进一步探索。**
    - 只有约一半的论文提出了CPT的新技术。
    - 其余一半要么仅关注纯适应效果而不考虑CL技术，要么对现有CL技术进行经验研究。
2. **OBS-2： CPT中采用的CL技术多样性仍然有限。**
    - 大多数CPT的实际实现主要集中在LLMs的架构扩展上。
    - 只有少数明确利用重放和参数正则化。
3. **OBS-3： 现有研究与CPT的实际生产环境之间存在明显差距。**
    - 除了最近的一项研究在159个领域上进行CPT外，其他探索的最大为8。
    - 这远远不及现实世界场景，其中持续预训练发生得更频繁，持续数月或数年。
    - CPT方法在这种长期场景中的效果仍不确定。
    - 未来研究探索无任务边界数据流设置下的CPT也很重要。

#### 4.1.3 CPT中的分布变化
本调查将CPT的分布变化分为三种主要类型：

1. **语言转移**：LLMs顺序学习不同语言语料库，例如英语 → 中文。
2. **内容转移**：LLMs顺序学习来自不同领域的语料库，例如化学 → 生物学。
3. **时间转移**：分布变化随时间发生，例如2021年的新闻 → 2022年的新闻，主要关注时间敏感知识的保留和更新。

下面我们将详细讨论每种分布变化类型的研究发现和方法：

+ **语言转移**：
    - 研究关注评估LLMs自然学习新语言的能力，观察到知识的一致正向转移，但遗忘仍然是一个重大挑战。
    - 一些工作调查了在适应LLMs到新语言时先前学习语言的遗忘程度，评估了各种CL技术。
    - 初步实验结果突出了解决CPT下语言转移水平遗忘的非平凡性质。
+ **内容转移**：
    - 研究探索了大规模CPT在多个内容领域上的效果，发现与单领域DAP相比，CPT可以有效提高模型的适应能力。
    - 一些工作建立了"领域增量数据流"的全面训练和评估协议，评估了多种CL方法。
    - 一些方法提出为新内容领域训练额外的领域特定专家，如DEMix和Lifelong-MoE。
+ **时间转移**：
    - 一些研究强调了在时间转移背景下进行CPT的重要性，因为时间转移可能引入冲突信息。
    - 提出了同时实现三个目标的方法：(i) 保留旧知识，(ii) 获取新知识，(iii) 更新过时知识。
    - 时间语言模型(TLMs)采用不同的方法，通过将时间信息集成到模型中来解决知识保留、获取和更新。
+ **其他**：
    - 一些研究使用CPT作为一种技术来渐进获取新知识，用于改进LLMs的行为。
    - 一些工作提出选择性语言建模(SLM)，使用参考模型评估训练语料库中每个标记的困惑度，并在高困惑度标记上持续预训练模型。

这些研究为我们理解和改进CPT提供了宝贵的见解，但也突显了该领域仍然存在的挑战和机会。

### 4.2 领域自适应预训练 ( Domain-Adaptive Pre-training，DAP)
#### DAP背景
无论规模大小，机构通常拥有大量未标记的领域特定数据。这些数据在通用LLMs（在多样语料库上训练）和为特定下游任务设计的微调LLMs之间架起了桥梁。利用这些数据作为预备阶段可以促进LLMs向下游任务的有效适应。这种"持续/连续预训练"、"进一步预训练"、"领域调优"、"知识增强预训练"和"知识注入训练"等过程在本调查中统一称为"**领域自适应预训练(DAP)**"，以保持清晰和一致性。

在领域自适应预训练(DAPT)的开创性工作中，作者在对下游任务进行微调之前，在更大的领域特定数据集上持续预训练语言模型，结果显示在各种任务上普遍改善了性能。由于这一观察已在多个并行领域得到验证，包括生物医学、计算机科学、新闻和评论，从业者普遍接受在额外的未标记领域特定数据上进行DAP有利于下游任务。因此，这种技术已在许多现代LLMs中广泛部署。

#### DAP的LLMs概述
![表 2](https://cdn.nlark.com/yuque/0/2024/png/406504/1728525861991-8b4dc960-f0b7-40b5-8c99-2deb3f163bb6.png)

表2提供了现有利用DAP的LLMs研究摘要。每个条目的特征包括三个主要方面：

1. 训练过程规范，包括LLMs训练的垂直领域、发布前的训练管道和采用的LLM架构。
2. 采用的持续学习技术，包括重放、参数正则化和架构扩展。
3. CL的评估指标，如向后转移（遗忘）和向前转移（适应下游数据）。

#### 4.2.1 DAP的一般观察
关于DAP研究格局的几个关键观察如下：

1. **OBS-1： DAP主要发生在单个阶段。**
    - 涉及多于一个阶段的持续DAP很少被探索：在表2列出的所有论文中，只有一篇采用了两个DAP阶段（Code Llama中的"PT → DAP → DAP → FT"）。
    - 可以说，只进行一个DAP阶段且不再进行其他操作的研究可以归类为CPT而不是DAP。然而，考虑到所有这些论文都旨在将通用LLM适应到特定领域，我们在本节中包括它们进行讨论。
2. **OBS-2： 有意或无意地通过CL视角解释DAP的概念被广泛接受。**
    - 如表所示，除了第一部分（白色，13/41）忽视DAP可能导致垂直遗忘的任何潜在副作用外，其余部分（全灰色，28/41）要么评估DAP的潜在负面影响，要么主动采用CL技术来缓解垂直遗忘的风险。
3. **OBS-3： 不仅针对DAP，还针对一般垂直持续学习的更复杂CL技术的进一步研究非常必要。**
    - 这得到了CL技术在训练领域特定LLMs中广泛采用的支持（22/41）。
    - 然而，这些技术的多样性有限，仅使用重放和参数扩展（LoRA）或层/块扩展。
    - 事实上，似乎个人可能没有明确认识到DAP应该从垂直连续性的角度来看，因为他们经常在不知不觉中采用CL技术。这一推断源于两个观察：  
(i) 参数扩展方法体现了隐式CL技术，因为它们限制了可调参数的大小；  
(ii) 部署重放的研究通常将该技术称为"数据组合"或"数据混合/混合"，而没有认识到它是垂直持续学习的典型CL解决方案。

#### 4.2.2 DAP的不同领域
##### 法律领域
鉴于法律行业管理不断增长的法律文件量的需求，利用LLMs来帮助法律专业人士导航、解释和生成高质量法律材料的需求日益增长。一些notable的研究包括：

1. Layer Llama：研究者从中国法院网站收集了公开可用的法律文本，总计约100亿个token。
2. SaulLM：研究者从不同国家的各种司法管辖区收集了DAP语料库，结果得到了300亿toekn的语料库，涵盖了法律文本的多个方面。当与先前可用的数据集结合时，用于法律领域DAP的token总数达到940亿。

这些大规模DAP数据虽然为特定领域提供了宝贵的见解，但由于涉及大量更新步骤，增加了一般知识垂直遗忘的风险。为缓解这个问题，一些方法采取了以下策略：

+ SaulLM在DAP数据中加入了来自维基百科、StackExchange和GitHub的通用数据，构成最终数据集的约2%。
+ Lawyer Llama在DAP期间纳入了通用领域数据的重放，但未披露重放率。
+ 一些研究在构建日语商业特定LLM时，在DAP期间重放了非最新的商业文档。

##### 医疗领域
LLMs的发展有望在医疗行业带来革命性变化，在医疗沟通、疾病诊断和医生决策方面提供潜在的效率和质量改进。一些努力通过从头训练LLM或微调公开可用的LLMs来满足特定医疗需求来开发医疗专家。DAP技术被广泛用于保留通用LLM的沟通和指令遵循能力，为后续医疗应用做准备。一些notable的研究包括：

1. BioMedGPT：这是一个多模态生物医学语言模型，整合了人类语言和生命语言（分子、蛋白质、细胞、基因等）的表示。在最终多模态监督微调之前，作者从Llama2-Chat初始化模型，并使用来自S2ORC的大量生物医学文档进行DAP，而不考虑任何CL技术或评估。
2. PMC-LLama：从S2ORC收集生物医学论文和医学教科书进行"知识注入训练"。在此阶段，来自RedPajama-Data的通用语言语料库以5%的比率在训练批次内重放。然而，该论文没有分析混合通用领域数据对DAP的有效性。
3. AF Adapter：提出了一个适配器结构，扩展注意力层和FFN的宽度，以获取领域知识，并且在DAP期间只调整适配器。
4. Hippocrates：在DAP期间部署LoRA，同时注入医学特定知识和保留一般能力。
5. Me-Llama：在临床笔记和生物医学文章的DAP中混合了约25%的通用领域数据，在MMLU上甚至实现了积极的向后转移。
6. HuatuoGPT-II：提出将DAP融入最终SFT，使两阶段开发成为一个统一过程。这种方法的挑战主要来自DAP的非标记语料库的数据异质性。作者通过使用现有大语言模型将数据段重新格式化为（指令，输出）格式来解决这个挑战。他们进一步采用优先采样策略来避免损害下游能力，这是在固定比率数据混合策略中观察到的一个缺陷。

##### 金融领域
与医疗领域类似，LLMs在增强金融沟通、决策过程和风险评估方面具有巨大潜力。尽管取得了进展，通用LLMs和现有领域特定小规模LLMs之间仍存在差距，突显了通过整合LLMs开发更强大的金融领域专家的迫切需求。一些notable的研究包括：

1. BBT-Fin：收集了一个中文金融DAP数据集，包括800亿个token，来自公司报告、分析师报告、社交媒体和金融新闻。除了常规的掩码语言建模（MLM）训练目标外，BBT-Fin还在DAP期间纳入了三元组掩码和跨度掩码技术。
2. CFGPT：创建了CFData，一个用于DAP和SFT的金融数据集，包含1410亿个token。在DAP期间，CFGPT没有采用CL技术，但在SFT期间使用QLoRA来防止对下游数据的过拟合，并平衡通用响应能力和领域特定能力。
3. WeaverBird：引入了一个智能金融对话系统，其中编码器在中英文金融文档以及专家注释的金融查询-响应对上使用LoRA进行训练。
4. Xuanyuan 2.0：类似于HuatuoGPT-II，提出了混合调整技术，将DAP和SFT阶段融合为一个，将通用领域数据和金融领域数据融合为一个。值得注意的是，混合调整中数据的分布是非常规的：金融DAP数据仅占13%的小部分。
5. 一项研究旨在提高DAP的数据效率。当下游任务的数据分布T已知时，作者提出基于泛化界采样DAP数据子集，其分布D与下游任务的数据相似。当下游数据分布未知时，作者建议确保采样语料库的新颖性和多样性。这种方法显著提高了DAP效率：它仅使用10%的原始收集数据，却优于在整个DAP数据集上训练的模型。

##### 科学领域
垂直科学LLMs涵盖许多学科，包括天文学、数学、地质学、化学和物理学、生物学等。然而，在所有研究中，只有一小部分采用了DAP技术。一些notable的研究包括：

1. OceanGPT：是第一个专门针对海洋领域定制的LLM。它在海洋科学文献的原始语料库上执行DAP，优先考虑最新研究和具有历史意义的作品。
2. K2：开创了专门针对地球科学的基础语言模型的开发。它聚合了地球科学开放获取文献和与地球科学相关的维基百科页面进行DAP。
3. AstroLlama：仅从arXiv上的天文学论文摘要收集数据并进行预训练。它观察到在学术天文学领域的困惑度有所改善，但没有提供更多定量评估。
4. MarineGPT：是专门为海洋领域设计的多模态LLM。在DAP期间，MarineGPT纳入了500万个海洋图像-文本对来注入领域知识。

一些方法积极整合通用领域数据的重放，以缓解垂直遗忘：

+ GeoGalactica：在DAP阶段，除了520亿token的地球科学语料库外，还以8:1:1的混合比例纳入了Arxiv论文和代码数据。
+ Llemma：专注于数学，从Code Llama初始化，并在550亿数学预训练数据集和通用领域数据的混合上进行DAP，比例为19:1。
+ PLlama：为植物科学设计，以9:1的比例混合领域特定和通用领域数据。

##### 代码领域
开发用于自动代码填充、调试和生成的LLMs具有重要的实际意义。在LLMs时代，趋势转向仅解码器架构，利用在通用自然语言上预训练的模型。从CL的角度来看，代码领域为DAP提供了独特的优势和挑战。一些notable的研究包括：

1. CodeGen：包括一套为自然语言（CodeGen-NL）、多语言编程语言（CodeGen-Multi）和单语言编程语言（CodeGen-Mono）设计的LLMs。这些模型按顺序训练，每个后续模型从先前在更通用领域数据上训练的模型初始化。
2. Comment-Aug：通过在用生成的额外注释增强的代码上执行DAP来解决编程语言与自然语言对齐的挑战。
3. StarCoder：引入了两个模型：StarCoderBase和StarCoder。StarCoderBase最初在包含各种编程语言的混合数据集上训练，没有显著重新加权数据。随后，StarCoderBase在额外350亿个Python代码token上进行进一步微调，从而开发出StarCoder。
4. DeepSeek-Coder-v1.5：源自DeepSeek-LLM，在2万亿个token上进行预训练，包括87%的源代码，10%的英文代码相关自然语言，和3%的中文自然语言语料库。
5. Code Llama：引入了一个复杂的训练框架，针对各种编码任务和模型大小量身定制。这些模型从Llama 2权重初始化，在由去重公共代码、关于代码的讨论和自然语言数据子集组成的数据集上进行DAP。
6. IRCoder：利用编译器中间表示来增强Code LLMs的多语言可转移性。通过在基于中间表示的代码上使用LoRA进行DAP，IRCoder实现了卓越的多语言编程指令遵循、增强的多语言代码理解和对提示扰动的increased鲁棒性。
7. Llama Pro：在代码和数学数据的组合上进行DAP。它通过动态添加transformer块的多个恒等副本来扩展原始Llama2架构。这些添加的块最初保留原始功能，并将为DAP进行调整。

这些研究强调了DAP对代码LLMs的重要性。然而，值得注意的是，现有Code LLMs的问题定义和常规架构可能给DAP部署带来兼容性挑战，需要在未来加以解决。

### 4.3 持续微调 ( Continual Fine-Tuning，CFT)
![表 3](https://cdn.nlark.com/yuque/0/2024/png/406504/1728526404758-8b74c563-0987-4153-8382-2d9e2b1bcd71.png)

持续微调 (CFT) 位于垂直连续性的底层，模型在这里针对来自不断演变的数据分布的连续同质任务进行训练。作为LLM的服务导向层，它不需要考虑进一步适应其他下游任务，这在很大程度上简化了优化目标：更好的适应和更少的遗忘。在LLMs时代，CFT中出现了新的计算范式，引起了研究社区的极大关注。这些主题包括：

1. **持续指令调整 (CIT)**：模型必须泛化到编码在指令中的新任务，需要语义理解。
2. **持续模型细化 (CMR)**：需要细粒度、可能是示例级别的解决方案，不同于任务级方法。
3. **持续模型对齐 (CMA)**：将模型与不断演变的人类偏好对齐，由于主观性质和缺乏明确的任务边界而具有挑战性。
4. **持续多模态大语言模型 (CMLLMs)**：解决复合架构设计并防止灾难性遗忘是关键挑战。

表3总结了现有的CFT研究，将研究分为上述子类别。该表包括增量学习类型（X-IL）、LLM架构、采用的CL技术和评估指标的详细信息。

#### 4.3.1 CFT的一般观察
审视LLMs背景下的持续学习格局，结合表3所示的结果，我们对CFT做出几个关键观察：

1. **OBS-1： 关注点从CIL明显转向TIL和DIL。**
    - CL社区长期以来普遍认为CIL是最具挑战性的CL场景，因为它要求模型同时预测上下文标签和上下文内标签，因此获得了社区的大部分关注。
    - 然而，在表3中列出的35篇论文中，只有3篇研究了CIL的CFT。
    - 研究重点的这种转变表明TIL和DIL在持续LLMs的实际应用中的重要性。
2. **OBS-2： 在CFT中，与CPT和DAP相比，CL技术得到更广泛的采用和明确探索。**
    - 在表3中，所有35篇论文都明确部署了CL技术，其中50%开发了新技术，这些技术不能轻易解释为现有经典CL技术的简单组合。
    - 例如，SAPT中的共享注意力学习框架、Larimar中部署的外部内存、AMA中实现Pareto最优的自适应模型平均方法等。
    - 这强调了持续学习被认为是开发有弹性和适应性LLMs的关键组成部分。

#### 4.3.2 通用持续微调 (General Continual Fine-Tuning，通用CFT)
研究人员长期以来一直在调查预训练LLMs在针对下游任务微调时的遗忘弹性现象，尽管一些发现相反。虽然预训练权重最初将模型定位在一个平坦的损失盆地中，有助于适应未来任务而不严重影响先前任务，但零或接近零的遗忘仅在表示级别观察到。这意味着虽然模型保留了区分任务特定表示的能力，但可能仍然忘记特定任务细节。因此，在实际应用中部署这些模型时需要额外措施。

许多研究超越了简单的顺序微调，利用LLMs固有的抗遗忘性质，同时避免采用过于复杂的CL技术。例如：

+ LR ADJUST提出了一种简单但有效的方法，动态调整学习率以缓解新语言知识对旧语言知识的覆盖。
+ SEQ*为在下游分类任务序列上微调LLMs引入了几种策略，如在预热后冻结LLM和旧分类器的参数，以及预先分配未来分类器等。

鉴于在CL中观察到的表示级别的最小遗忘，一些研究旨在通过在CFT期间引入表示级约束来解决表示空间和决策层之间的不匹配。NeiAttn就是这种方法的一个例子，它将分类任务公式化为掩码语言建模，并提出了一种相邻注意机制来对抗负面表示漂移。

另一条研究路线改进了预训练LLMs的输入/输出格式和网络架构，使其更适合CFT。例如：

+ CTR纳入了两个CL插件模块，包括任务特定模块（TSM）用于获取任务特定知识，以及知识共享模块（KSM）用于选择性转移先前学习的相似知识。
+ CIRCLE为各种类型的有问题代码手动设计多样化的提示模板，将它们统一为完形填空任务，并采用基于难度的重放来增强持续程序修复。
+ LFPT5通过将序列标记、文本分类和文本生成整合为文本到文本生成任务来解决终身少样本语言学习。在适应新任务时，它对从先前领域生成的伪样本进行提示调整。

#### 4.3.3 持续指令调整 (Continual Instruction Tuning，CIT)
虽然LLMs通常在广泛和多样的语料库上预训练，但它们可能在特定任务（如指令遵循）上遇到困难，尽管拥有一般知识。许多研究表明，指令调整（ Instruction Tuning，IT）可以显著提高LLMs遵循文本指令的能力，利用LLMs中预先存在的知识来弥合一般和特定任务性能之间的差距。最近的工作如WizardLM和CodecLM进一步定制合成数据以通过IT引导LLMs的行为。此外，IT增强了人类和LLMs之间的交互，提供了更自然的接口，使LLM输出更符合人类期望和偏好。

当指令调整数据作为流到来时，应该解决先前学习指令的遗忘问题。CT0代表了对LLMs进行持续指令调整（CIT）的开创性研究，在整个过程中对基础T0模型应用重放方法。许多后续研究专注于增强CIT过程中使用的重放方法。例如：

+ KPIG通过计算掩码部分的关键部分信息增益来动态选择重放数据，解决指令遵循中的"半听"问题。
+ SSR使用LLM生成合成实例进行重放，以较低的成本实现优于或可比的性能。

其他方法在CIT期间引入多种CL技术：

+ DynaInst将参数正则化与动态重放相结合，选择性地存储和重放实例和任务以增强结果。
+ InstructionSpeak采用负训练和重放指令来改善前向转移和后向转移。

一些方法结合PEFT：

+ 正交低秩适应（O-LoRA）在正交子空间中学习新任务，同时保留先前任务的LoRA参数，以最小化不同任务之间的干扰。
+ 共享注意力框架（SAPT）通过共享注意力学习和选择模块结合PET块和选择模块，同时解决灾难性遗忘和知识转移。

虽然基于正则化和基于架构的方法需要额外的参数存储和GPU内存，但它们与基于重放的方法一起，由于简单性和有效性而保留在CIT中。

#### 4.3.4 持续模型细化 ( Continual Model Refinement，CMR)
像人类一样，LLMs也容易出错，如不准确的翻译或过时的信息。直接微调模型以纠正这些错误可能会破坏其在先前学习任务上的性能。为克服这些挑战，提出了模型细化（也称为模型编辑，Model Editing），旨在纠正模型的错误，同时保持其在其他输入上的性能，仅使用适度的计算资源。

模型编辑的概念最初在提出"可靠性-局部性-效率"原则并提出梯度下降编辑器来高效解决它的研究中探索。后续研究扩展了这一原则，以编辑BERT基础语言模型和更大模型（如GPT-J-6B和T5-XXL）中的事实知识，使用梯度分解。这些方法通常更新模型参数的子集以改变特定输入的标签。此外，如讨论的基于内存的模型通过检索机制纳入编辑。

持续模型细化（CMR）的概念将模型细化水平扩展，顺序呈现更新的样本对$ (x_e, y_e, \hat{y}_e)_{e=1}^N $作为流。最初引入这个想法的研究评估了各种CL方法和动态采样算法。许多CMR方法采用检索机制：

+ GRACE使用语言模型的隐藏激活作为"键"，仅在输入$ x_0 $类似于更新样本对时激活更新的参数。
+ MELO通过整合LoRA提高了这种方法的效率。
+ Larimar用外部情节记忆增强LLM，将CMR建模为持续的内存刷新。

同时，一些方法仅专注于更新模型参数的子集：

+ WilKE解决了单编辑方法（如ROME）中"毒性积累和闪烁"的问题，通过知识感知层选择算法将其适应CL上下文。
+ WISE解决了现有终身模型细化方法中"不可能三角"（可靠性、局部性和泛化）的问题。它引入了一个侧面内存系统，实现知识分片和合并，成功同时实现了所有三个目标。

虽然所有这些工作开创了CMR研究，但LLMs的CMR探索仍然开放。一些研究强调了潜在问题：存储事实的位置可能与编辑的最佳位置不匹配。这挑战了几种现有方法使用的经典"定位和编辑"范式，可能成为CMR的重大突破。其他问题，包括这种问题设置是否适合LLMs，以及是否可以为LLMs开发更高内存/计算效率的CMR方法，还有待回答。

#### 4.3.5 持续模型对齐 (Continual Model Alignment，CMA)
模型对齐（MA）确保AI系统的行为和输出与人类价值观、伦理和偏好一致。MA可以大致分为两类：基于强化学习（RL）和基于监督学习（SL）。

+ 基于RL的方法训练模型做出由人类反馈强化的决策，使用奖励系统引导它们朝向理想结果。
+ 基于SL的方法直接在人类偏好数据集上训练模型，使其输出与展示的人类价值观一致。

两种方法都利用算法学习技术和人类反馈的组合来逐步细化模型行为。当LLMs经历MA阶段时，通常会发生先前知识的垂直遗忘。在某些研究中，作者将MA导致的这种灾难性遗忘现象称为"对齐税"。值得注意的是，即使单次MA也可能削弱模型的性能能力，因为它限制了模型对训练分布的较窄子集的响应。

持续模型对齐（Continual Model Alignment，CMA）旨在持续细化LLMs以与不断演变的人类价值观、伦理和数据保持一致。LLM训练在历史数据集上的静态性质可能导致模型输出与当前事实准确性、社会规范和标准之间的差异，使CMA成为维持其适应性和与当代背景对齐的关键过程。同样，有两种类型的CMA框架：基于RL和基于SL。

在基于RL的CMA领域，已经注意到两个重要贡献：

1. AMA识别了现有CL技术与RLHF之间的冲突，并提出自适应模型平均（AMA），自适应找到模型层组合的适当比例，以获得最大奖励和最小税收。
2. 持续近端策略优化（CPPO）提出了不同示例的加权策略，决定其策略增强或知识保留的使用，随时间缓解对齐税。

对于基于SL的CMA，持续最优策略拟合（COPF）提出了一种从直接策略优化（DPO）改编的解决方案，解决了其在CMA背景下潜在的次优策略拟合和过度优化风险。

#### 4.3.6 持续多模态大语言模型 (Continual Multimodal Large Language Models，CMLLMs)
多模态LLMs整合多种模态的数据，如文本、图像和视频，以增强对现实世界信息的理解。通常，MLLMs由模态特定的子模块组成，如预训练视觉编码器、大语言模型和用于跨模型对齐的投影器。这种对齐对MLLMs融合不同数据类型和促进其理解至关重要。

持续训练CLIP等多模态模型已经被长期研究，而持续训练MLLMs的问题仍然很少被探索。一些现有研究调查了持续训练MLLMs时灾难性遗忘的原因：

+ 对输入嵌入进行奇异值分解，揭示了不同输入嵌入之间的显著差异。这种差异导致模型学习与先前训练任务无关的信息，从而导致灾难性遗忘和负向转移。
+ 观察到少数类崩溃可能导致灾难性遗忘，当微调过程中多数类和少数类之间的不平衡比接近无穷大时。
+ 进一步确定幻觉是MLLMs性能下降的一个促成因素。

**持续微调MLLMs（Continual Fine-Tuning MLLMs）**

与传统的持续学习方法（涉及新任务的全模型微调）不同，MLLMs的持续微调专注于在适应新任务时细化特定层。考虑到预训练模型的强大能力，训练特定层就足够了，同时可以减少计算需求。所有上述研究集体表明，MLLMs仍然遭受灾难性遗忘，这表现在两个方面：

1. 沿垂直连续性方向，在针对下游任务微调后，预训练任务的性能下降。
2. 沿水平连续性轴线，在针对新任务微调后，先前微调任务的性能下降。

一些研究还观察到负向转移，即在学习新任务时未见任务的性能下降，表明模型泛化能力的下降。虽然传统的CL方法适用，但一些可能不会产生最佳结果，正如各种实验所证明的那样。例如，一些研究观察到基于重放和模型扩展策略在MLLMs持续微调的各种场景中始终有效，但基于正则化的方法仅在已经在多个任务上联合指令调整的模型上表现良好。

其他工作寻求为持续学习MLLMs开发特定解决方案：

+ EProj提出为MLLMs中的每个新任务扩展投影层，并利用任务相似性信息正则化（TIR）来增强性能。
+ Fwd-Prompt引入了一种提示调整方法，将提示梯度投影到残差空间和预训练子空间，以最小化任务之间的干扰并重用预训练知识，促进正向转移而不依赖于先前样本。
+ Model Tailor专注于预训练MLLMs在特定任务上微调后的遗忘问题，提出model tailor来补偿对增强目标任务性能至关重要的选定子集。
+ RebQ提出了一种名为Reconstruct before Query（RebQ）的新方法，利用预训练模型的多模态知识重建缺失模态的缺失信息。
+ 最近，MoE（Mixture-of-Experts）框架引起了注意，它类似于CL中的基于架构的方法。它为模型提供了从不同专家学习不同意图的能力，例如，CoIN首次引入MoELoRA来微调LLaVA，有效缓解了MLLMs的灾难性遗忘，结果证明了其有效性。

这些研究为持续学习MLLMs提供了有价值的见解和创新方法，但仍然存在许多挑战和机会。

## 5. 评估协议和数据集
### 5.1 评估协议
#### 5.1.1 解释持续LLMs中的基本持续学习评估指标
在一些文献中，$ \widetilde{OP} $被称为"示例准确度"、"整体准确度"或CMR中的"编辑成功率"。遗忘和向后转移的概念支撑了各种评估指标，如知识保留、不变知识的性能、平均增加困惑度（AP+）以及CMR中的测试和编辑保留率。我们将向前转移的表示扩展到垂直方向，以表示领域自适应预训练导致的下游任务性能改善（见表2）。向前转移在一些文献中也被称为时间泛化或知识转移。

#### 5.1.2 持续LLMs的评估指标
1. [**LAnguage Model Analysis (LAMA)**](https://paperswithcode.com/dataset/lama)：LAMA是一个设计用于探测语言模型中嵌入的世界知识的评估框架。它将每个世界事实转换为完形填空陈述，然后输入到语言模型中预测正确答案。LAMA已被扩展用于持续预训练，特别是那些在时间转移下的预训练。
2. **遗忘/(更新+获取)比率 (FUAR)**：由于预训练LLM的性能被分解为细粒度集合，OP成为一个过于笼统的指标，无法准确反映模型行为的平衡和权衡。为解决这个问题，CKL提出了一个联合评估指标FUAR。FUAR值为1表示知识遗忘和知识学习之间的平等权衡：平均每遗忘一个时间不变知识，就学习或更新一个知识。FUAR小于1表示高学习效率，即以遗忘一个时间不变知识为代价，获得了多于一个知识。
3. **X-Delta**：在TRACE中，作者提出了一组"X-Delta"指标，用于量化持续指令调整对LLMs特定能力的向前转移。设$ \{X_1, X_2, \cdots, X_M\} $为任务X的M个数据集。预训练LLM在这些任务上评估的基线性能表示为$ \{b^X_1, \cdots, b^X_M\} $。模型在不同于评估任务的一组任务上进行持续微调。在顺序训练过程中，学习任务t后模型在评估任务$ X_i $上的性能为$ R^X_{t,i} $。学习任务t后的X-Delta $ \Delta R^X_t $定义为：

$ \Delta R^X_t \triangleq \frac{1}{M}\sum_{m=1}^M (R^X_{t,i} - b^X_i) $

4. **NLG Score**：在持续模型对齐中，三个突出的指标用于评估自然语言生成（NLG）的不同方面是BLEU-4、METEOR和ROUGE-L。
    - BLEU-4：设计用于机器翻译，评估机器生成文本和参考文本之间n-gram的精确度，特别关注四字词序列以衡量流畅度和充分性。
    - METEOR：也针对机器翻译，但旨在通过考虑同义词和词干提取来改善与人类判断的相关性，从而提供更细致的翻译质量评估。
    - ROUGE-L：常应用于摘要任务，评估生成摘要与一组参考摘要之间的最长公共子序列，有效衡量基本内容的召回率。

每个指标都有其优势，针对特定类型的语言处理任务，反映文本生成质量的不同维度。

### 5.2 数据集
![表 4](https://cdn.nlark.com/yuque/0/2024/png/406504/1728527349697-f012fe6b-c738-45a5-ace2-47f394d4e677.png)

本节全面回顾了用于基准测试持续LLMs的可用数据集，如表4所示。值得注意的是，作者有意排除了用于垂直领域（如法律、医疗和金融）LLMs领域自适应预训练的数据集，除非它们专门设计用于持续领域自适应预训练。此外，他们省略了用于通用持续微调的数据集，因为这些已在现有工作中得到广泛研究。

#### 持续预训练 (CPT) 和领域自适应预训练 (DAP) 的数据集
目前研究缺乏广泛认可的基准来评估时间转移下的持续预训练LLMs。一些notable的数据集包括：

+ TimeLMs：利用收集到2022年的一系列Twitter语料库，每季度顺序预训练RoBERTa模型。
+ CC-RecentNews：作为CKL中LMs的未标记预训练数据，由近期新闻组成，作为单阶段数据集。
+ TWiki：从2021年8月至12月的Wikipedia文章中策划和清理的数据集，通过提供相邻快照之间的Diffsets促进增量学习的探索。

对于研究CPT和DAP中内容级分布转移的工作，研究人员经常求助于一组类似的公开可用数据集来构建他们自己的持续学习算法测试平台。

#### 持续指令调整的数据集
测量CIT的有效性至关重要，特别是因为传统评估指标可能不适用于LLMs：许多指标过于简单，无法全面评估模型持续学习的能力。需要新的基准和指标来评估旧知识的保留和新指令的整合。一些notable的基准包括：

+ TRACE：专为LLMs设计的持续学习基准，涵盖多语言能力、代码生成和数学推理等多样化任务。
+ CITB：另一个CIT基准，结合了学习和评估协议。它还证明了重放通常在所有方法中产生最佳性能。
+ CoIN：将基准扩展到MLLMs，纳入视觉-语言数据集的平衡和多样化指令集。

#### 持续模型细化的数据集
大多数持续模型细化的数据集可以分为两类：事实检查和问答。对于事实检查，模型被要求验证某些声明的真实性，通常建模为分类任务。关键数据集包括FEVER和VitaminC，两者都源自Wikipedia。对于问答，模型被要求提供具体答案而不是选择。零样本关系提取（zsRE）是最广泛使用的数据集，此外还有Natural Questions（NQ）和T-rex。

#### 持续模型对齐的数据集
在人类反馈强化学习（RLHF）领域，几个数据集在不同研究中普遍使用，用于评估模型在不同场景和持续学习条件下的适应和有效性。这些包括IMDB、HH-RLHF、Reddit TL：DR数据集，以及Common Sense QA、Reading Comprehension和Translation数据集。

#### 持续多模态大语言模型的数据集
继LLaVA之后，许多MLLMs采用指令调整模式，使评估与人类意图的对齐和推理知识保留成为可能。因此，传统任务如图像分类可以转换为VQA任务来评估MLLMs的能力。一些提议的基准包括：

+ MCIT：提出了第一个持续指令调整基准，Benchmark1和Benchmark2。
+ EMT：第一个分类评估框架，用于调查MLLMs中的灾难性遗忘。
+ CoIN：全面的基准，跨越8个任务类别，从两个角度评估MLLMs：指令遵循和通用知识。
+ UPMC-Food101-CMML和MM-IMDb-CMML：构建用于基准测试新颖的CMML任务，即在持续微调期间某些模态的数据缺失。

这些数据集和基准为研究持续学习LLMs提供了宝贵的资源，使研究人员能够系统地评估和比较不同方法的性能。

## 6. 讨论
本节深入探讨了传统持续学习的计算模式与大语言模型(LLMs)的训练和部署之间的交叉点。首先，我们将检查持续学习LLMs过程中出现的有趣特性。接下来，我们探讨三种增量学习类型在LLMs背景下的演变角色。随后，我们对比了持续LLMs中记忆的角色与传统持续学习中的角色。最后，我们以对该领域未来研究有前景方向的简明概述作为结束。

### 6.1 持续LLMs中出现的有趣特性
除了已经确立的预训练大语言模型与下游特定模型相比对灾难性遗忘的弹性之外，对LLMs持续训练时出现的其他有趣特性的探索还相对有限。虽然对持续训练的LLMs的新兴能力的调查在一定程度上吸引了社区的注意，但它们仍然相对有限。例如，在一项研究中观察到，当在一系列文档上顺序和循环微调时，大型模型表现出一种称为"预期恢复"的现象。这指的是LLMs能够在再次遇到它们之前恢复被遗忘的文档信息。这表明LLMs可能具有顺序记忆能力，这可能为研究记忆重放和更复杂的结构化学习环境铺平道路，随着模型参数的扩大。

### 6.2 传统增量学习类型
如第2.2节所述，流行的增量学习有三种类型。其中，类增量学习(CIL)历来吸引了社区的大量关注。然而，在持续预训练和适应大语言模型的背景下，我们观察到对CIL的兴趣减少，但对任务增量学习(TIL)和领域增量学习(DIL)的关注增加。

考虑到语言模型本质上是为内容生成而设计的，并且是使用下一个词预测的预文本生成任务进行预训练的，强调生成任务的模式并将传统CIL范式整合到更广泛的语言建模框架中，丢弃增量分类头是很自然的。例如，在词汇感知标签生成(VAG)中，CIL被重新定义为持续标签生成任务。这种方法利用预训练的编码器-解码器语言模型来生成类别标签。

然而，对传统CIL范式关注的减少并不意味着这些技术在LLMs持续学习领域没有影响。相反，许多当前的研究努力无意中采用了这些技术，表明它们在各种应用中被广泛采用。例如，诸如词汇扩展等技术可以被视为CIL中扩展分类头的扩展。这些CIL技术可以进一步集成到像Lifelong-MoE这样的系统中，在那里向transformer块添加新专家需要更新门控函数以包括新添加专家的路由。

领域增量学习(DIL)的重要性是不言而喻的，考虑到持续预训练(CPT)和领域自适应预训练(DAP)中共享的任务定义和输入-输出格式。由于动态扩展标记词汇可能带来额外的挑战，自然而然地关注理解输入语料库内的分布转移，同时保持词汇固定。

另一方面，任务增量学习(TIL)因其个性化LLM服务的潜力而引起重大兴趣。例如，用户可能希望有选择特定领域专家的选项，从而在整个推理过程中使任务ID可用。此外，TIL在指令调整中发挥关键作用，其中指令可以被视为自然语言编码的任务信息。值得注意的是，TIL和DIL之间的界限在持续指令调整中变得有些模糊。语言模型展示了为未见指令推断领域信息的能力，表明TIL和DIL在某些情况下的融合。

### 6.3 持续LLMs中记忆的角色
先前的持续学习研究，从人类学习模式中汲取灵感，主要强调过去数据的存储效率。有限内存大小设置下的持续学习引起了社区的极大关注。然而，这种关注在持续LLMs的背景下可能不再成立。在放宽内存约束的方向上，能够访问训练数据的机构可能选择保留完全访问权而不限制内存大小，考虑到内存存储的成本是可以承担的。在这种情况下，正如一些研究所强调的，挑战从存储效率转移到计算效率。

为实现持续学习目标，模型必须高效地适应新数据（高效适应）并选择关键经验进行重放（高效重放）。因此，有必要重新评估现有的内存约束，并通过限制更新次数和FLOP数量来优先考虑持续学习LLMs的计算效率优化。

在另一端，具有更严格内存约束的研究在现代LLMs的持续学习中仍然至关重要。如图1所示，LLMs的上游供应商通常不会提供带有发布模型权重的训练数据。因此，消费者必须在无法访问实际重放数据的情况下将这些模型适应到下游数据。在这种情况下应用了各种无重放的持续策略，例如从替代来源收集数据示例，利用LLMs的生成能力产生伪样本进行重放，以及在参数空间中实施正则化技术。

严格内存约束下的持续学习也受到数据隐私问题的驱动，在服务器端保留数据是被禁止的。在这些情况下，研究人员必须依赖在线持续学习方法，其中数据示例仅在作为流到达时用于训练，已经有许多努力在开发能够在这些约束下运行的LLMs。

### 6.4 未来展望方向
1. **持续LLMs的理论**：持续学习社区倾向于优先考虑经验研究而非理论探索。然而，一些研究试图为CL建立理论基础。例如，一些工作利用最优参数周围的二阶泰勒展开来推导基于参数差异的最大特征值和l2范数的任务间泛化误差界。另一条研究路线利用任务/领域差异构建多任务泛化界。例如，统一领域增量学习(UDIL)提出了域内和跨域蒸馏损失的上界，统一了各种基于重放的DIL技术。然而，考虑到LLMs的预训练和大规模性质，直接将这些现有理论应用于持续LLMs可能是不明智的。因此，在具有稳健理论保证的持续学习LLMs研究方面存在显著差距，并且从理论角度理解LLMs的遗忘行为也是如此。
2. **持续LLMs的高效重放知识保留**：训练大规模LLMs的计算资源通常是有限的。虽然存储预算理论上可以是无限的，但在没有特定设计的情况下重放过去的经验可能导致当前领域学习的低效更新，导致收敛缓慢。除了控制数据混合比例的稀疏重放解决方案外，还在探索持续LLMs的高效重放。例如，KPIG通过计算掩码段上的关键部分信息增益来增强重放效率，实现重放数据的动态选择。在另一项开创性工作中，引入了基于适应过程中输出变化的遗忘预测机制，后来用于持续模型细化(CMR)中的选择性重放。该工作验证了基于遗忘倾向过滤重放样本可以显著提高持续LLMs的知识保留率。然而，需要更复杂和准确的数据混合策略和高效重放样本选择机制，例如整个训练过程中的动态数据混合比率。因此，我们将LLMs的高效重放这一实际方向标记为未来的重要研究焦点。
3. **具有可控记忆的持续LLMs**：LLMs整套参数中固有的长期记忆通常缺乏可解释性和明确的可操作性，这在某些应用领域至关重要。例如，考虑一个场景，供应商在客户同意的情况下收集数据并持续利用这些数据更新LLMs。然而，如果一些用户后来撤回同意，从该部分数据获得的知识也必须撤回。对于持续预训练的大规模LLM，唯一的解决方案是回滚到包含这些用户数据之前的先前模型版本，并从那时起重新训练模型。这个"机器遗忘"的例子生动地说明了为LLMs配备外部可控记忆的好处。作为持续模型细化(CMR)的一部分，一些研究探索了持续学习的记忆系统。例如，Larimar建议将Kanerva Machine集成为多事实模型编辑的情节记忆。这个记忆系统支持基本操作如写入、读取和生成，以及高级操作如顺序写入和遗忘。它使一次性知识更新成为可能，无需昂贵的重新训练或微调。此外，其他记忆系统如Hopfield Networks也有望进行未来调查。
4. **具有自定义偏好的持续LLMs**：定制用户偏好对LLMs至关重要，尤其是在面向服务的情况下。用户通常需要在领域专业知识、伦理、价值观或表达语气之间进行不同的权衡。为个别用户高效构建定制LLMs并提供灵活调整选项是一项具有挑战性的任务。在这个方向上的早期尝试包括不精确贝叶斯持续学习(IBCL)，在某些假设下，通过在参数空间中组合两个模型后验，保证基于用户偏好生成Pareto最优模型。虽然经验验证在规模上有限，但这种方法为该领域的未来研究铺平了道路。

这些未来方向为持续学习LLMs的研究提供了丰富的机会，有望推动该领域向更高效、更可靠和更适应性强的系统发展。

## 7. 结论
本文提供了一个关于持续LLMs的全面调查，总结了从持续学习角度看LLMs训练和部署的最新进展。作者根据提出的现代分层持续学习LLMs的更广泛框架中的位置对问题和任务进行分类。虽然社区对这一领域表现出广泛和不断增长的兴趣，但作者也注意到几个缺失的基石，包括算法多样性和对大模型行为（如知识遗忘、转移和获取）的基本理解。通过全面而详细的方法，这项调查旨在激发更多从业者探索持续学习技术，最终为发展强大和自我进化的AI系统做出贡献。

这篇综述为研究人员和从业者提供了一个全面的框架，以理解持续学习在LLMs背景下的挑战和机遇。它不仅总结了现有的工作，还指出了未来研究的重要方向，为该领域的进一步发展提供了宝贵的指导。


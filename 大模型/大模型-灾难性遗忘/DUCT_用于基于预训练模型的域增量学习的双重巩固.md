> 原文：《DUAL CONSOLIDATION FOR PRE-TRAINED MODELBASED DOMAIN-INCREMENTAL LEARNING》
>

## 1. 引言
域增量学习(DIL)是一个重要的研究方向,旨在让模型能够逐步适应来自不同领域的新概念。近期预训练模型的发展为DIL提供了坚实的基础,但在学习新概念的过程中经常会导致对预训练知识的灾难性遗忘。具体来说,顺序更新模型可能会用最新领域的知识覆盖表示和分类器。因此,开发一个能够在整个学习过程中容纳所有见过的领域的表示及其相应分类器是至关重要的。

为此,本文提出了DUal ConsolidaTion (DUCT)方法,在表示和分类器两个层面上统一和巩固历史知识。通过合并不同阶段的骨干网络,我们创建了一个适合多个域递增学习的表示空间。合并后的表示作为一个平衡的中介,捕获所有见过的域的任务特定特征。此外,为了解决合并的嵌入和分类器之间的不匹配问题,我们引入了额外的分类器合并过程。利用类别语义信息,我们在最新的嵌入空间中估计旧域的分类器权重。通过合并历史分类器和估计的分类器,我们将它们与合并的嵌入空间对齐,从而促进增量分类。

## 2. 相关工作
### 2.1 持续学习/增量学习
持续学习/增量学习是机器学习领域的一个热门话题,旨在从流式数据中不断吸收新知识。它可以进一步分为三个子类别：

1. 任务增量学习(TIL)
2. 类增量学习(CIL)  
3. 域增量学习(DIL)

其中,TIL和CIL面临新类别的出现,需要在不遗忘的情况下学习它们。相比之下,DIL主要关注标签空间固定而输入数据涉及新域的场景。

在持续学习中,典型的解决方案包括：

+ 使用数据重放来回顾以前的知识
+ 使用知识蒸馏或参数正则化来维持现有知识
+ 使用偏差校正来纠正模型偏差
+ 随着数据的演变扩展网络结构

### 2.2 基于预训练模型的域增量学习
随着预训练技术的快速发展,预训练模型(PTMs)为DIL模型提供了强大的初始化,以提高特征的泛化能力。目前基于PTM的DIL方法主要采用视觉提示调优来调整预训练特征。代表性工作包括：

+ L2P：利用查询-键匹配机制选择实例特定提示
+ DualPrompt：联合学习任务特定和实例特定提示
+ S-Prompts：学习域特定提示,通过KNN搜索检索提示
+ CODA-Prompt：将提示选择过程修改为基于注意力的加权组合

除了提示调优外,还有一些方法直接在预训练特征上构建分类器。

### 2.3 模型合并
近期预训练的进展使得不同模型之间的权重插值成为模型编辑中的一种有用技术。大多数工作专注于多个模型的简单平均,旨在提高模型在单个任务上的性能和对分布外数据的鲁棒性。本文考虑合并预训练权重和任务向量参数,以培养所有见过域的多任务表示。

## 3. 从旧域到新域
### 3.1 域增量学习
在域增量学习中,模型面临一系列任务的数据流 $ D_1, D_2, ..., D_B $,其中 $ D_b = \{X_b, Y_b\} $ 是第b个训练任务,即 $ X_b = \{x_i\}_{i=1}^{n_b} $ 和 $ Y_b = \{y_i\}_{i=1}^{n_b} $。每个训练实例 $ x_i \in \mathbb{R}^D $ 属于类别 $ y_i \in \mathcal{Y} $。本文遵循无样本设置,即在第b个训练阶段,我们只能访问当前数据集 $ D_b $ 中的数据。在DIL中,标签空间 $ \mathcal{Y} $ 在整个学习过程中保持不变,而输入实例的分布从一个域到另一个域不断变化,即 $ p(X_b) \neq p(X_{b'}) $ 对于 $ b \neq b' $。DIL的目标是拟合一个模型 $ f(x) $,能够区分任何见过域中的类别：

$ f^* = \arg\min_{f \in \mathcal{H}} \mathbb{E}_{(x,y) \sim D_1^t \cup \cdots D_b^t} \mathbb{I}(y \neq f(x)) $

其中 $ \mathcal{H} $ 是假设空间,$ \mathbb{I}(\cdot) $ 是指示函数,如果表达式成立则输出1,否则输出0。$ D_b^t $ 表示任务b的数据分布。因此,DIL模型应该能够对新域的实例进行分类,同时不忘记以前的域。

本文假设有一个预训练的Vision Transformer (ViT)作为 $ f(x) $ 的初始化。为简单起见,我们将网络结构解耦为嵌入函数 $ \phi(\cdot): \mathbb{R}^D \rightarrow \mathbb{R}^d $ (即最后的[CLS]标记)和线性分类器 $ W \in \mathbb{R}^{d \times |\mathcal{Y}|} $。这样,输出表示为 $ f(x) = W^\top \phi(x) $,本文使用余弦分类器。分类器进一步解耦为 $ W = [w_1, w_2, ..., w_{|\mathcal{Y}|}] $,其中 $ w_j $ 表示第j类的分类器权重。虽然标签空间在学习过程中是静态的,但不同域中同一类的数据分布可能会产生显著的域间差距。因此,本文遵循现有工作,随着新任务的出现扩展分类器 $ W \in \mathbb{R}^{d \times b|\mathcal{Y}|} $,最终预测由 $ \arg\max_i w_i^\top \phi(x) \mod |\mathcal{Y}| $ 给出。

### 3.2 域增量学习中的基线方法
在域增量学习中,面对新域的传入数据集的一个简单解决方案是直接优化嵌入和分类器：

$ \min_{W \cup \phi} \sum_{(x,y) \in D_b} \ell(W^\top \phi(x), y) $

以预训练权重为初始化,方程(2)可以快速捕获新域内的判别特征。然而,由于嵌入 $ \phi(\cdot) $ 在不同域之间不断变化,它很快就会失去对以前域的泛化能力并遭受遗忘。

为了抵抗特征级遗忘,一个可行的解决方案是冻结预训练权重并附加轻量级模块来编码域特定知识,例如提示。一个代表性的工作L2P将一组提示组织为提示池(即Pool),并通过以下方式优化模型：

$ \min_{Pool \cup W} \sum_{(x,y) \in D_b} \ell(W^\top \bar{\phi}(x; Pool), y) + L_{Pool} $

其中 $ \bar{\phi}(x; Pool) $ 表示骨干网络冻结的提示特征表示,L_{Pool} 表示提示选择损失。方程(3)动态检索实例特定提示来调整表示,并学习分类器将特征映射到相应的类别。由于骨干网络权重被冻结,它可以减轻更新过程中的表示漂移。

讨论：虽然方程(2)和方程(3)采用不同的模型更新策略,但它们都遭受特征级遗忘。具体来说,通过方程(2)调整模型可以将任务特定知识编码到嵌入中,但也存在特征被新域完全覆盖的风险,因为没有保留以前知识的限制。此外,虽然方程(3)冻结了预训练权重,但附加的可训练提示也可能导致遗忘。由于提示特征依赖于实例特定的提示选择,为新域优化提示仍然有覆盖以前提示的风险。随着特征偏向最新域,分类器也会随之调整。因此,以前任务的分类器与不稳定且不断变化的特征不兼容,导致分类器级别的遗忘。

## 4. DUCT：用于域增量学习的双重巩固
观察到特征级和分类器级覆盖的风险,我们旨在从两个方面抵抗DIL中的遗忘。首先,缓解特征级遗忘需要以低成本获得统一的嵌入空间。由于顺序更新骨干网络或提示都会导致遗忘,需要一个适当的解决方案来充分利用所有历史特征并将它们合并为统一的特征。其次,随着表示从任务到任务的变化,分类器和嵌入之间的不匹配问题随着数据的演变变得更加严重。因此,需要一个分类器合并步骤来校准它们,使其与嵌入兼容。

在接下来的部分中,我们介绍表示合并和分类器合并的学习范式。最后,我们提供了详细的训练和推理指南。

### 4.1 表示合并
观察到在所有见过的域之间取得平衡的挑战,我们需要选择另一种特征更新策略来避免顺序覆盖并抵抗遗忘。让我们假设一个理想的场景,我们可以为每个传入域分别训练每个模型,得到一组模型 $ \{\phi_1(\cdot), W_1\}, ..., \{\phi_B(\cdot), W_B\} $。这些模型是通过方程(2)优化相同的PTM获得的,因此对每个特定域具有判别能力,可以被视为"域专家"。在一个过于简化的场景中,如果我们知道实例来自哪个域,我们可以直接使用相应的专家进行预测。然而,由于DIL中没有这种辅助信息,我们需要获得一个适合所有域的全能嵌入空间。这样,就不需要决定使用哪个嵌入,因为全能嵌入足够强大,可以捕获所有域特定的特征。

相应地,我们从模型合并社区得到启发,通过组合多个任务向量可以实现适合多个域的理想模型。我们将嵌入函数的相对变化表示为 $ \delta_i^\phi = \phi_i - \phi_0 $,其中 $ \phi_0 $ 是预训练模型的权重。因此,$ \delta_i^\phi $ 表示第i个域中权重的相对变化,我们可以通过以下方式构建统一嵌入：

$ \phi_i^m = \phi_0 + \alpha_\phi \sum_i \delta_i^\phi $

它将预训练权重与每个任务向量组合在一起。由于所有微调的嵌入都从相同的预训练权重开始,任务向量由于不同域之间的语义差距而具有低相似度。因此,添加这些权重使合并模型能够突出所有任务特定的特征,方程(4)提供了一种可行的方法来获得适合所有见过域的通用特征表示。

表示合并与任务相似度：虽然方程(4)提供了一种简单的方法来整合多个任务向量以构建统一嵌入,但它仍然缺乏衡量任务间相似度的考虑。例如,如果两个不同的域更相似,突出这些特征对于识别相应域的类别会更有帮助。因此,我们将任务相似度(Sim_{0,i})引入表示合并过程,并将方程(4)替换为：

$ \phi_i^m = \phi_0 + \alpha_\phi \sum_i Sim_{0,i} \delta_i^\phi $

具体来说,由于合并的嵌入是基于预训练权重和任务向量构建的,我们考虑预训练模型 $\phi_0$ 和后续模型 $\phi_i$ 的相似度来调整合并过程。一个简单的方法是直接测量 $\phi_0$ 和 $\phi_i$ 之间的余弦相似度。然而,由于预训练权重具有数百万维度,在这样的空间中由于维度灾难,相似度是无效的。为此,我们利用当前训练任务作为任务相似度的指标。具体来说,对于 $D_b$ 中的每个类别,我们使用骨干网络 $\phi_i$ 在其嵌入空间中提取类中心:



$$c_p^i = \sum_{j=1}^{|D_b|} \mathbb{I}(y_j = p)\phi_i(x_j) / \sum_{j=1}^{|D_b|} \mathbb{I}(y_j = p)$$



之后,我们可以在嵌入空间中获得两组类中心,即 $C_0 = [c_1^0, c_2^0, ..., c_{|Y|}^0]$, $C_i = [c_1^i, c_2^i, ..., c_{|Y|}^i]$。由于这些类中心是嵌入空间中的代表性点,我们计算成对类别相似度来表示任务相似度:



$$Sim_{0,i} = \frac{1}{|Y|}\sum_{j=1}^{|Y|} sim(c_j^0, c_j^i)$$



其中我们使用余弦相似度来计算中心间相似度 $sim(\cdot, \cdot)$。



表示合并的效果:图1(上)可视化了表示合并过程,其中合并的骨干网络可以统一多个域。通过方程(5),我们能够通过在参数空间中组合它们来构建适合所有任务的联合嵌入空间。由于DIL中的任务是逐个出现的,表示合并过程也可以增量进行,即 $\phi_i^m = \phi_{i-1}^m + \alpha_\phi Sim_{0,i}\delta_i^\phi$。这样,我们可以摆脱大量的内存成本,只需在内存中保留最多两个骨干网络。在每个训练阶段,我们首先通过方程(2)微调模型,然后进行表示合并来聚合表示。通过这种方式,我们获得了跨所有见过任务的统一嵌入空间。



### 4.2 分类器合并
在方程(5)中,我们设计了特征合并过程,通过累积任务向量来聚合多个微调模型。然而,仍然存在一个致命的问题,即合并的嵌入与分类器之间存在不匹配。由于分类器是为了将嵌入与相应的类别匹配而优化的,当骨干网络被替换为另一个时,匹配程度会急剧下降。回想一下3.1节中的背景信息,我们为每个域使用独立的分类器,我们需要设计额外的分类器合并过程,以将分类器与嵌入空间对齐。在每个训练阶段,我们将前面域的分类器表示为"旧分类器"(W_o),当前域的分类器表示为"新分类器"(W_n)。然后我们讨论如何将它们与合并的特征对齐。

新分类器重训练：在每个训练阶段,我们手头有训练数据D_b。因此,直观地将新分类器与合并的特征协调一致的方法是：

$ \min_{W_n} \sum_{(x,y) \in D_b} \ell(W_n^\top \bar{\phi}_i^m(x), y) $

其中合并的特征 $ \bar{\phi}_i^m(\cdot) $ 被冻结以避免进一步的不匹配,我们可以通过方程(8)将新分类器W_n对齐以匹配特征。

旧分类器传输：方程(8)将新分类器与最新的嵌入空间对齐。然而,仍然存在一个致命的问题,即旧分类器也与合并的嵌入空间不兼容。因此,旧域的先前知识将在增量学习过程中被遗忘。如果我们有大量先前域的训练实例,可以像方程(8)那样进行类似的校准过程。然而,由于无样本的限制,我们不能在内存中保存任何先前的实例。因此,我们需要找到另一种方法来估计旧分类器的相对校准权重。例如,如果我们有重新训练的分类器来对剪贴画风格的"狮子"进行分类,它大部分可以重复用于对真实照片风格的狮子进行分类。我们称这种关系为"语义信息",目标是利用这种信息来辅助旧分类器的对齐。我们将使用语义信息校准的分类器表示为 $ \hat{W}_o = T(W_n, S) $,即估计的分类器是通过使用语义信息S变换新分类器获得的。因此,还有两个核心问题需要解决：1)如何定义变换函数T? 2)如何定义语义信息S?

通过最优传输实现T：函数T编码了两个任务之间特征集之间的相关性。考虑到线性层的权重矩阵揭示了特征-类对之间的相对关系,最终的logit是所有特征的聚合,我们可以设计现有分类器的重组,用线性映射T ∈ R^(β×γ)。T编码了当前和先前域之间的跨任务相关性,其中T中较大的值表示更高的类别间相似度。由于决策是通过将分类器与相应特征匹配来做出的,我们可以利用并重组当前域中相似类别的权重来获得先前域的权重。例如,区分剪贴画风格狮子的重要特征应该被赋予更高的系数,以帮助对照片风格的狮子进行分类,反之亦然。

我们表示μ_1 ∈ Δ_β和μ_2 ∈ Δ_γ,其中Δ_d = {μ ： μ ∈ R_+<sup>d, μ</sup>⊤1 = 1}是d维单纯形。μ_1和μ_2表示各域之间每个类别的重要性,我们将它们设置为均匀分布。为了将一组类别从旧域映射到新域之间的另一组,我们进一步引入成本矩阵Q ∈ R_+^(β×γ)来指导转换。Q_{i,j}的较大权重表示我们需要支付更多成本来重用第i类的分类器来辅助第j类。因此,矩阵T可以表示为两个分布的耦合,旨在以最低的传输成本连接不同域的类别。因此,T可以通过最小化以下目标获得：

$ \min_T \langle T, Q \rangle \quad s.t. \quad T1 = \mu_1, T^\top1 = \mu_2, T \geq 0 $

这是最优传输(OT)的Kantorovich公式。优化方程(9)使我们能够以低成本显示如何跨域移动概率质量。由于目标是重用重新训练的分类器W_n来调整先前的分类器W_o,我们设置μ_1 ∈ Δ_{|Y|}和μ_2 ∈ Δ_{|Y_o|}具有均匀的类边缘,其中|Y_o|表示W_o中的类别数。求解方程(9)产生不同域之间的对齐,我们可以将T应用于新分类器以获得估计的旧分类器,即 $ \hat{W}_o = W_n T $。

使用语义信息定义传输成本：求解方程(9)需要适当定义跨域成本,即Q。较高的成本表示将分类器传输到目标类别的效果较差,反之亦然。为此,我们设计了一种简单而有效的方法来衡量这种类别间信息。具体来说,在每个阶段的训练之前,我们使用预训练的骨干网络φ_0通过方程(6)在其嵌入空间中提取类中心,即[c_1<sup>0, c_2</sup>0, ..., c_{|Y|}<sup>0]。这些类中心表示相应类别的最具代表性的嵌入。由于φ_0是用广泛的数据集优化的,我们依赖其泛化能力来反映任务间的相似性。如果两个类别相似,它们的嵌入也应该位于彼此附近。因此,我们计算类中心之间的欧几里得距离作为传输成本,即Q_{i,j} = ∥c_i</sup>0 - c_j<sup>0∥_2</sup>2。这里的类i和j来自不同的域。有了成对的传输成本,我们就能够优化方程(9)得到传输计划T。之后,我们使用旧分类器和传输的分类器之间的插值进行合并：

$ W_o^m = (1 - \alpha_W)W_o + \alpha_W \hat{W}_o = (1 - \alpha_W)W_o + \alpha_W W_n T $

分类器合并的效果：分类器合并过程分两步进行,即新分类器重训练和旧分类器传输。第一步旨在将新分类器与统一嵌入对齐,而第二步旨在用校准的分类器重组旧分类器。通过这种方式,我们获得了与合并特征兼容的分类器,从而有利于在统一嵌入空间中进行识别。

### 4.3 DUCT总结
我们在算法1中总结了训练步骤。具体来说,面对新的训练任务,我们用φ_0提取类中心。之后,我们优化模型并分别合并特征和分类器。然后使用合并的模型([W_o<sup>m; W_n]</sup>⊤φ_i^m)进行测试。我们使用Sinkhorn算法来求解方程(9)中的OT问题。

关于内存成本的讨论：在训练过程中,我们需要在内存中保留两个模型,即历史合并的骨干网络φ_i<sup>m和用于学习当前任务的在线骨干网络φ_i。然而,在每个域的学习过程之后,当前运行的模型被合并到φ_i</sup>m中(方程(5)),不需要保留在内存中。在推理过程中,我们使用合并的骨干网络φ_i<sup>m和分类器[W_o</sup>m; W_n]进行分类,这与单个骨干网络相同。因此,DUCT可以公平地与其他方法进行比较。

## 5. 实验
在本节中,我们在基准数据集上进行实验,将DUCT与现有最先进的方法进行比较。我们还提供了关于组件和参数鲁棒性的消融研究,以分析不同模块的效果。嵌入空间的可视化也验证了我们提出方法的有效性。

### 5.1 实验设置
数据集：遵循基于PTM的域增量学习的基准设置,我们在Office-Home、DomainNet、CORe50和CDDB-Hard上评估性能。具体来说,Office-Home有4个域,DomainNet有6个域,CORe50有11个域,CDDB-Hard有5个域。我们考虑5个任务顺序来打乱这些域,以进行全面的评估。

比较方法：在比较中,我们考虑两类方法,包括基于样本的方法(Replay、iCaRL、MEMO)和最先进的无样本DIL方法(SimpleCIL、L2P、DualPrompt、CODA-Prompt、EASE、RanPAC和S-iPrompt)。我们为所有比较方法使用相同的预训练骨干网络。

实现细节：我们使用PyTorch和Pilot在NVIDIA 4090上部署实验。我们考虑两种典型的预训练权重,即ViT-B/16-IN21K和ViT-B/16-IN1K。两者都使用ImageNet21K预训练,后者进一步在ImageNet1K上微调。我们使用SGD优化器优化DUCT,batch size为128,训练15个epoch。学习率设置为0.001。我们使用herding算法为基于样本的方法选择每类10个样本。在DUCT中,我们设置合并参数α_φ = 0.5, α_w = 0.5。

性能度量：我们主要考虑A_B(学习最后一个阶段后的性能)和$ \bar{A} = \frac{1}{B}\sum_{b=1}^B A_b $(所有增量阶段的平均性能)进行比较。我们还考虑遗忘度量来衡量DIL中的相对遗忘程度。

### 5.2 基准比较
我们在图2中报告了四个基准数据集的结果。从这些结果可以推断,DUCT在最终准确率上始终优于其他比较方法1~7%。需要注意的是,起始点的差异是由于不同的调整技术,这些技术不能直接对齐,因为这些方法中可训练参数的数量不同。

具体来说,顺序微调模型会遭受灾难性遗忘,即使从预训练模型开始。为了补偿遗忘现象,我们观察到几种基于样本的方法(Replay、iCaRL和MEMO)对基线显示稳定的改进。然而,由于其中一些工作是为类增量学习场景设计的,扩展或蒸馏目标可能不适合当前设置。然后我们将DUCT与专门为PTM设计的方法进行比较,发现基于提示的方法(L2P、DualPrompt、CODA-Prompt和S-iPrompt)的性能低于我们的方法。我们还观察到DUCT与各种预训练权重兼容,并在ViT-B/16 IN1K(a<sub>d)和IN21K(e</sub>f)上显示稳定的改进。

此外,我们还在表1和图3(a)中报告了五个任务顺序中的增量性能(平均值和标准差)。从表中可以推断,DUCT在不同任务顺序中表现稳健,相对于其他最先进的方法显示出稳定的改进。

### 5.3 进一步分析
遗忘度量：图2和表1主要关注准确率度量,它使用所有见过的域来衡量相对性能。除了准确率度量外,我们还遵循之前的工作使用遗忘度量,它捕获先前域中现有知识的稳定性。如图3(b)所示,我们报告了CDDB数据集上所有比较方法的遗忘度量。从图中可以推断,一些方法遭受严重的遗忘,例如Finetune和MEMO,表明它们不适合域增量学习场景。我们还观察到,基于提示的方法冻结骨干网络以抵抗特征漂移,这解决了遗忘现象。然而,DUCT仍然显示出最少的遗忘(即0.12),表明其强大的性能。

参数鲁棒性：DUCT中有两个主要的合并步骤,即方程(5)中的表示合并和方程(10)中的分类器合并。这些合并步骤包括一组模型/分类器之间的合并参数,即骨干网络合并比率α_φ和分类器合并比率α_W。在这一部分中,我们对这些参数的选择进行消融实验,以研究DUCT对它们变化的鲁棒性。具体来说,我们在{0.1, 0.25, 0.5, 0.75, 1.0}中选择α_φ和α_W,得到25种参数组合。我们在Office-Home数据集上使用这些参数组合进行实验,并在图3(c)中显示平均性能。



从图中可以推断,DUCT总体上对参数变化具有鲁棒性。此外,由于合并参数旨在在旧知识和新知识之间进行权衡,我们发现较小的值(如0.1)或较大的值(如1.0)表现不佳。这些边缘参数的糟糕表现也表明了合并过程的重要性,因为较小的值或较大的值相当于在合并过程中忽略了部分重要信息。相比之下,它们都倾向于选择一个适中的值,即在{0.25, 0.5}左右选择它们显示出更好的性能。因此,我们建议将α_φ = 0.5, α_w = 0.5作为默认设置。



组成部分：在本节中,我们进行实验以研究DUCT中每个模块在CDDB数据集上的重要性。如表2所示,"Baseline"表示直接冻结嵌入并提取类中心作为分类器,由于与下游域的域间差距,它显示出较差的性能。然后我们利用方程(4)顺序合并骨干网络以合并表示,并将其表示为Variation 1。结果显示,添加表示合并过程大大提高了性能,表明这种统一表示在DIL中的优越性。然而,当将方程(4)切换到方程(5)时,我们发现Variation 2进一步提高了性能,表明任务相似度有助于获得通用嵌入空间。之后,我们将其与方程(8)中的分类器重训练过程结合,称为Variation 3。结果显示,合并特征与分类器之间的不匹配削弱了性能,将新分类器与嵌入对齐有助于DIL。当将DUCT与Variation 3进行比较时,我们发现旧分类器传输对于恢复前期知识和抵抗遗忘也至关重要,它将最终准确率提高了2%。消融研究验证了DUCT中不同模块的有效性。



可视化：在本节中,我们可视化嵌入空间以展示DUCT在DomainNet数据集上的有效性。我们考虑一个两阶段域增量学习场景,每个阶段包含五个类别,并使用t-SNE来可视化表示空间在DUCT之前和之后的情况。我们使用点来表示第一个域中的类别,使用三角形来表示第二个域中的类别。我们使用阴影区域来表示由分类器权重获得的决策边界,以表示不同的类别。



如图4所示,尽管DUCT之前的嵌入空间显示出有竞争力的性能,但存在两个主要缺陷：1)黄色和红色点之间存在混淆区域,表明随着数据的演变,先前的类别被部分遗忘。2)同一类别的嵌入空间仍然位于不同的区域,例如紫色和绿色类别,这对于域增量学习来说并不理想。然而,当应用DUCT来合并特征和分类器时,上述问题在图5中得到了解决。具体来说,由于合并的嵌入适合所有任务,先前域的遗忘得到了缓解。此外,统一的嵌入空间充分地将不同域的相同类别放在一起,有利于最终的推理。



这些分析结果进一步证明了DUCT方法在参数选择、各组成部分以及嵌入空间表示方面的有效性和鲁棒性。






> 原文：《<font style="color:rgb(41, 38, 27);background-color:rgb(245, 244, 239);">An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks</font>》
>

## 1. 引言
灾难性遗忘是一个影响神经网络以及其他学习系统(包括生物和机器学习系统)的问题。当一个学习系统首先在一个任务上训练，然后在第二个任务上训练时，它可能会忘记如何执行第一个任务。例如，一个使用凸目标函数训练的机器学习系统在第二个任务训练结束时总是会达到相同的配置，无论它是如何初始化的。这意味着一个在两个不同任务上训练的SVM将完全忘记如何执行第一个任务。

人类学习的一个得到广泛支持的模型表明，新皮质神经元使用一种容易发生灾难性遗忘的算法进行学习，而新皮质学习算法由一个虚拟经验系统补充，该系统重播存储在海马体中的记忆，以不断强化最近未执行的任务。作为机器学习研究人员，我们可以从中得到的教训是，我们的学习算法可以容忍遗忘，但可能需要补充算法来减少信息损失。设计这样的补充算法取决于理解我们当代主要学习算法所经历的遗忘特征。

## 2. 相关工作
近年来，灾难性遗忘并不是神经网络的一个被广泛研究的属性。这个属性在过去被深入研究过，但自2006年深度学习复兴以来就没有受到太多关注。Srivastava等人(2013)重新popularized了研究现代深度神经网络这一方面的想法。

然而，该工作的主要重点不是研究灾难性遗忘，所以实验是有限的。在每种情况下只训练了一个神经网络。网络都使用相同的超参数，并且使用相同的启发式选择的停止点。只使用了一对任务，所以不清楚这些发现是否仅适用于具有相同种类和程度相似性的任务对，还是这些发现可以推广到许多种类的任务对。只使用了一种训练算法，即标准梯度下降。本文超越了所有这些限制，通过训练具有不同超参数的多个网络，使用验证集停止，使用三个具有不同任务相似性配置文件的任务对进行评估，并在实验集中包括dropout算法。

## 3. 方法
### 3.1 Dropout
Dropout是最近引入的神经网络训练算法。Dropout旨在正则化神经网络以提高其泛化性能。

Dropout训练是对标准随机梯度下降训练的修改。当每个例子在学习过程中呈现给网络时，网络的输入状态和隐藏单元状态乘以二进制掩码。掩码中的零导致一些单元从网络中移除。每次呈现一个例子时，这个掩码都是随机生成的。掩码的每个元素独立于其他元素采样，使用某个固定概率p。在测试时，不丢弃任何单元，每个单元的输出权重乘以p，以补偿该单元比训练期间更频繁出现的情况。

Dropout可以被看作是一种非常有效的方法，用于训练指数级多的共享权重的神经网络，然后平均它们的预测。这个过程类似于bagging，有助于减少泛化误差。学习的特征必须在许多不同模型的上下文中良好工作，这也有助于正则化模型。

Dropout是一个非常有效的正则化器。在引入dropout之前，减少神经网络泛化误差的主要方法之一就是简单地通过使用少量隐藏单元来限制其容量。Dropout使得训练明显更大的网络成为可能。例如，作者进行了25次随机超参数搜索实验，以找到最佳的两层整流器网络(Glorot et al., 2011a)来分类MNIST数据集。当使用dropout训练时，根据验证集表现最佳的网络比不使用dropout训练的最佳网络参数多56.48%。

作者假设，最佳运行的dropout网络的增加大小意味着它们比传统神经网络更不容易出现灾难性遗忘问题，传统神经网络通过限制容量使其刚好足以执行第一个任务来进行正则化。

### 3.2 激活函数
神经网络的每个隐藏层将某个输入向量x转换为输出向量h。在所有情况下，这是通过首先计算突触前激活z = Wx + b来完成的，其中W是可学习参数的矩阵，b是可学习参数的向量。然后突触前激活z通过激活函数转换为突触后激活h：h = f(z)。然后h作为下一层的输入提供。

作者研究了以下激活函数：

1. Logistic sigmoid:

$ \forall i, f(z)_i = \frac{1}{1 + \exp(-z_i)} $

2. Rectified linear (Jarrett et al., 2009; Glorot et al., 2011a):

$ \forall i, f(z)_i = \max(0, z_i) $

3. Hard Local Winner Take All (LWTA) (Srivastava et al., 2013):

$ \forall i, f(z)_i = g(i, z)z_i $

   这里g是一个门控函数。z被分成大小为k的不相交块，如果z_i是其组内的最大元素，则g(i, z)为1。如果多个元素并列最大，我们随机打破平局。否则g(i, z)为0。

4. Maxout (Goodfellow et al., 2013b):

$ \forall i, f(z)_i = \max_j \{z_{ki}, ..., z_{k(i+1)-1}\} $

作者对这四种激活函数分别使用了两种算法进行训练，总共八种不同的方法。

### 3.3 随机超参数搜索
公平比较不同的深度学习方法很困难。大多数深度学习方法的性能是多个超参数的复杂非线性函数。对于许多应用，最先进的性能是由人类实践者为某些深度学习方法选择超参数获得的。人类选择对比较方法是有问题的，因为人类实践者可能更擅长为他们熟悉的方法选择超参数。人类实践者也可能有利益冲突，倾向于为他们喜欢的方法选择更好的超参数。

自动选择超参数允许更公平地比较具有复杂超参数依赖关系的方法。然而，自动选择超参数具有挑战性。网格搜索受维度灾难的影响，需要指数级的实验来探索高维超参数空间。在这项工作中，作者使用随机超参数搜索(Bergstra & Bengio, 2012)代替。这种方法实现简单，仅使用25个实验就可以在MNIST等简单数据集上获得大致最先进的结果。

## 4. 实验
所有实验都遵循相同的基本形式。对于每个实验，定义两个任务："旧任务"和"新任务"。作者检查了先在旧任务上训练然后在新任务上训练的神经网络的行为。

对于每个任务定义，作者对两种算法运行相同的实验套件：随机梯度下降训练和dropout训练。对于每种算法，尝试了四种不同的激活函数：logistic sigmoid、整流器、hard LWTA和maxout。

对于这八种条件中的每一种，随机生成25组随机超参数。在所有情况下，使用一个具有两个隐藏层和一个softmax分类层的模型。搜索的超参数包括每层的max-norm约束(Srebro & Shraibman, 2005)的大小，用于初始化每层权重的方法和与该方法相关的任何超参数，每层的初始偏置，控制饱和线性学习率衰减和动量增加计划的参数，以及每层的大小。

作者没有搜索一些已知良好值的超参数。例如，对于dropout，丢弃隐藏单元的最佳概率通常为0.5左右，丢弃可见单元的最佳概率通常为0.2左右。作者在所有实验中使用了这些众所周知的常数。这可能会减少通过搜索获得的最大可能性能，但它使搜索函数在只有25个实验的情况下运行得更好，因为较少的实验会产生显著失败。

作者尽最大努力使不同方法之间的超参数搜索具有可比性。对于SGD和dropout，作者总是使用相同的超参数搜索。对于不同的激活函数，超参数搜索之间存在一些细微差异。所有这些差异都与参数初始化方案有关。

在所有情况下，作者首先在"旧任务"上训练，直到验证集误差在最后100个epoch中没有改善。然后恢复对应于最佳验证集误差的参数，并开始在"新任务"上训练。作者训练直到旧验证集和新验证集的并集上的错误在100个epoch内没有改善。

在运行完所有8种条件的25个随机配置实验后，作者绘制了一条可能性前沿曲线，显示了在旧任务上获得每个测试误差量的新任务上的最小测试误差量。具体来说，这些图是通过绘制一条曲线来生成的，该曲线跟踪所有25个模型在新任务训练过程中遇到的所有(旧任务测试误差，新任务测试误差)对的点云的左下前沿，每次通过训练集后生成一个点。

### 4.1 输入重新格式化
许多自然发生的任务在必须理解的基本结构方面非常相似，但输入以不同的格式呈现。

例如，考虑在已经学会理解西班牙语之后学习理解意大利语。这两个任务共享作为自然语言理解问题的更深层次的基本结构，而且，意大利语和西班牙语的语法相似。然而，每种语言中的具体单词是不同的。因此，学习意大利语的人从拥有语言一般结构的预先存在的表示中受益。挑战在于学习将新单词映射到这些结构中（例如，将意大利语单词"sei"附加到动词"to be"的第二人称变位的预先存在的概念上），而不损害理解西班牙语的能力。

为了测试这种学习问题，作者设计了一对简单的任务，其中任务相同，但输入格式不同。具体来说，作者使用MNIST分类，但对旧任务和新任务使用不同的像素排列。因此，这两个任务都受益于拥有诸如笔画检测器或笔画组合成数字的概念。然而，任何单个像素的含义都是不同的。网络必须学会将新的像素集合与笔画联系起来，而不会显著扰乱旧的更高层次概念，或者擦除旧的像素与笔画之间的连接。

分类性能结果在图1中呈现。在这个任务对上，使用dropout改善了所有模型的两任务验证集性能。作者在图2中展示了dropout对最优模型大小的影响。

### 4.2 相似任务
接下来，作者考虑了当两个任务不完全相同，但语义相似，并使用相同的输入格式时会发生什么。为了测试这种情况，作者使用了两个产品类别的Amazon评论(Blitzer et al., 2007)的情感分析作为两个任务。任务只是将产品评论的文本分类为正面或负面情感。作者使用了与(Glorot et al., 2011b)相同的预处理。

分类性能结果在图3中呈现。在这个任务对上，使用dropout改善了所有模型的两任务验证集性能。作者在图4中展示了dropout对最优模型大小的影响。

### 4.3 不相似任务
最后，作者考虑了当两个任务在语义上不相似时会发生什么。为了测试这种情况，作者使用Amazon评论作为一个任务，MNIST分类作为另一个任务。为了使两个任务具有相同的输出大小，作者只使用了MNIST数据集的两个类别。为了使它们具有相同的验证集大小，作者随机抽样了剩余的MNIST验证集示例（因为MNIST验证集最初比Amazon验证集大，作者不希望Amazon数据集性能估计的方差高于MNIST的方差）。作者之前预处理的Amazon数据集有5,000个输入特征，而MNIST只有784个。为了使两个任务具有相同的输入大小，作者使用PCA降低了Amazon数据的维度。

分类性能结果在图5中呈现。在这个任务对上，使用dropout同样改善了所有模型的两任务验证集性能。作者在图6中展示了dropout对最优模型大小的影响。

## 5. 讨论
实验表明，使用dropout训练总是有益的，至少在作者使用的相对较小的数据集上是这样。Dropout改善了所有三个任务对上所有八种方法的性能。Dropout在新任务上的性能、旧任务上的性能以及平衡这两个极端的权衡曲线上的点方面都表现最好，对于所有三个任务对都是如此。

Dropout对遗忘的抵抗力可能部分解释为可以用dropout训练的大型模型。在输入重新格式化的任务对和相似任务对上，dropout从未减少作者尝试的四种激活函数中任何一种的最佳模型大小。然而，dropout似乎还有一些额外的属性可以帮助防止遗忘，作者还没有解释。在不相似任务实验中，dropout改善了性能但减小了大多数激活函数的最佳模型大小，而在其他任务对上，它偶尔对最佳模型大小没有影响。

唯一最近关于灾难性遗忘的工作(Srivastava et al., 2013)认为，激活函数的选择对网络的灾难性遗忘属性有显著影响，特别是当使用随机梯度下降训练时，hard LWTA在这方面优于logistic sigmoid和整流线性单元。

在作者更广泛的实验中，发现激活函数的选择对性能的影响不如训练算法的选择一致。当作者对不同类型的任务对进行实验时，发现激活函数的排名非常依赖于问题。例如，logistic sigmoid在某些条件下是最差的，但在其他条件下是最好的。这表明只要在计算上可行，就应该始终交叉验证激活函数的选择。

作者也否定了hard LWTA特别抵抗灾难性遗忘的一般性观点，或者它使标准SGD训练算法更能抵抗灾难性遗忘。例如，当在输入重新格式化任务对上使用SGD训练时，hard LWTA的可能性前沿曲线除了sigmoid之外比所有激活函数都差。在相似任务对上，使用SGD的LWTA在作者考虑的所有八种方法中是最差的，无论是在新任务上的最佳性能、旧任务上的最佳性能，还是在达到可能性前沿图原点附近的点方面。

然而，hard LWTA在某些情况下确实表现最好（例如，在不相似任务对的新任务上有最佳性能）。这表明在超参数搜索中包含hard LWTA作为众多激活函数之一是值得的。然而，LWTA在作者的三个任务对中从未是最左边的点，所以它可能只在遗忘是问题的顺序任务设置中有用。

当计算资源有限，无法尝试多种激活函数时，作者建议使用dropout训练的maxout激活函数。这是唯一一种在作者考虑的所有三个任务对的性能权衡图的左下前沿上出现的方法。

## 结论
本研究对现代神经网络中的灾难性遗忘问题进行了全面的实证调查。通过比较不同的训练算法和激活函数，以及考察不同任务关系对灾难性遗忘的影响，作者得出了几个重要结论：

1. Dropout 训练算法在所有实验中都表现最佳，无论是在适应新任务、记忆旧任务，还是在这两个极端之间的权衡方面。
2. 激活函数的选择对性能的影响不如训练算法一致，其效果很大程度上取决于具体任务和任务之间的关系。
3. Maxout 激活函数在使用 dropout 训练时，是唯一一种在所有考虑的任务对中都出现在性能权衡前沿的激活函数。
4. 研究结果否定了先前认为 hard LWTA 特别抵抗灾难性遗忘的观点。
5. 研究强调了在实际应用中交叉验证激活函数选择的重要性。

这项研究为理解和缓解神经网络中的灾难性遗忘问题提供了宝贵的见解，也为未来的研究指明了方向。

最后，作者建议在资源有限的情况下，使用 dropout 训练的 maxout 网络可能是一个好的选择，因为它在各种任务和设置中都表现出了良好的性能和适应性。


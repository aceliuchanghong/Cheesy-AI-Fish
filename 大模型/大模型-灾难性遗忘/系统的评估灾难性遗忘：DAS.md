> 原文：《Continual Pre-training of Language Models》
>

## 1. 引言
本文研究了语言模型(LMs)的持续预训练问题,特别是持续领域自适应预训练(continual DAP-training)。先前的研究表明,使用领域语料进一步预训练LM可以提高该领域下游任务的性能。本文提出了一种新方法,可以持续地使用一系列无标签领域语料来DAP-train LM,以适应这些领域并提高它们的下游任务性能。

该方法的主要创新点在于：

1. 提出了一种软掩码机制,直接控制LM的更新。
2. 提出了一种新的代理方法来保留原始LM中的通用知识。
3. 对比先前学习的领域知识(包括预训练LM中的通用知识)和当前完整网络的知识表示,以实现知识集成。

该方法不仅克服了灾难性遗忘,还实现了知识迁移以提高下游任务性能。实验结果证明了所提方法的有效性。

## 2. 相关工作
### 2.1 DAP-training
DAP-training可以通过直接更新LM或仅训练一小组额外参数来实现。例如：

+ 直接更新LM的方法：Xu等(2019),Sun等(2019),Lee等(2020),Alsentzer等(2019),Gururangan等(2020),Chakrabarty等(2019),Ke等(2022b)
+ 训练额外参数的方法：
    - Pfeiffer等(2020),Wang等(2020a),Ke等(2021a;b;c)训练adapters  
    - Gu等(2021)训练prompt

虽然adapter和prompt可能有效,但在这些额外模块之间传递知识通常具有挑战性且可能不准确。作者的方法属于前一类,直接更新LM。这对持续学习来说很具挑战性,因为存在灾难性遗忘问题。据作者所知,这一类中还没有关于持续学习的系统。

### 2.2 持续学习
大多数持续学习方法都是为了克服灾难性遗忘：

1. 正则化方法(如EWC)计算每个参数对先前任务的重要性,并使用正则化器来惩罚变化之和。
2. 重放方法保留或生成旧任务的一些数据,并在学习新任务时使用它们。
3. 参数隔离方法为不同任务/领域分配神经元、参数或子网络,并在任务学习中对它们进行掩码。

在NLP中,持续学习已被用于槽填充、语言学习、情感分析、主题建模、问答和文本分类等任务。但都不是用于DAP-training。一些最近的持续学习论文涉及LMs：

+ Madotto等(2020)为不同领域学习单独的adapters,因此没有灾难性遗忘或知识迁移。
+ DEMIX(Gururangan等, 2021)用最接近的旧adapter初始化新adapter。
+ CPT(Ke等, 2022a)和ELLE(Qin等, 2022)与作者的工作最相关。但CPT使用参数隔离方法学习和保护每个任务,这是不够的。它还需要在下游任务微调时提供领域ID。ELLE必须从预训练LM本身开始,而不是像作者的方法那样从预训练的LM开始。它还使用大内存(每个领域1GB)来存储重放数据(包括预训练数据),并为每个领域扩展网络。作者的方法都不需要这些。

### 2.3 神经网络剪枝
网络中的许多参数是冗余的,可以被剪枝。现有方法包括丢弃绝对值小的参数、累积梯度和彩票假说等。然而,这些方法不能直接应用,因为我们需要保留不仅个别领域知识,还有LM中的通用知识。对于通用知识,由于没有任何预训练数据,作者提出了一种基于鲁棒性的代理方法。对于领域知识,作者采用了一种剪枝方法,但将重要性用作软掩码,因为我们想累积知识而不是压缩LM。

### 2.4 对比学习
对比学习通过最大化正样本对的相似性和最小化负样本对的相似性来学习良好的表示：

$ L_{contrast} = -\frac{1}{N}\sum_{n=1}^N \log \frac{e^{(sim(q_n,q_n^+)/\tau)}}{\sum_{j=1}^N e^{(sim(q_n,q_j^+)/\tau)}} $

其中N是批量大小,τ是温度参数,sim(·)是相似度度量,q_n和q_n<sup>+是正样本对x_n和x_n</sup>+的表示。

作者的方法对比先前学习的领域知识和预训练LM中的通用知识与完整知识(包括先前领域和当前领域知识),以实现互补效果。

## 3. 提出的DAS技术
DAS的持续DAP-training基于两个主要思想：

1. 通过基于单元重要性的软掩码来保留LM中重要的通用语言知识和从先前领域学到的知识,以克服灾难性遗忘,同时促进跨任务知识迁移。
2. 鼓励模型学习当前领域和先前领域的互补表示,以实现知识集成。

整个学习过程包括两个主要功能：(i)初始化和(ii)持续学习。

(i)计算单元对LM中通用语言知识的重要性。这在持续学习开始之前完成。

(ii)用于持续学习,包括两个步骤：(a)领域训练和(b)重要性计算。

(a)利用到目前为止累积的重要性分数(包括原始LM中通用知识和从先前领域学到的知识的重要性)和当前领域的输入数据来学习该领域,并实现上述(1)和(2)。

(b)计算当前领域的重要性分数,以供未来使用。

### 3.1 初始化：计算单元对通用知识的重要性
这个初始化函数计算Transformer中单元(注意力头和神经元)对原始LM中通用知识的重要性。

Transformer的关键组成部分是多头注意力层、中间层和输出层。作者用"layer"或l来表示这三层中的任何一层,因为他们的方法对这三层的处理类似。

#### 层中单元的重要性
已经发现层中并非所有单元都重要。作者引入了一个虚拟参数g_l来计算层l中单元的重要性。这些称为虚拟参数,因为每个g^(k)初始化为1。我们只需要每个参数的梯度来计算其对应单元的重要性,不需要更新任何参数。

$ \hat{o}_l = g_l \otimes o_l $

其中o_l指层l的输出(可以是上述三层中的任何一层)。⊗表示元素乘法,即g_l中的每个变量g_{l,i}对应层中的一个单元(神经元或注意力头)。

作者采用了Michel等(2019)的基于梯度的重要性检测方法。给定N个样本的数据集D = {(x_n, y_n)}_{n=1}^N (y_n是x_n的类标签,因为Michel等(2019)致力于监督学习),层中神经元或头的重要性用基于梯度的代理分数来估计：

$ I_l = \frac{1}{N}\sum_{n=1}^N |\frac{\partial L_{impt}(x_n, y_n)}{\partial g_l}| $

其中L_{impt}是一个特定于任务的损失函数。注意虚拟参数g_l初始化为全1,且不会改变。这是因为我们只需要其在所有数据上的平均梯度∇_g_l(等式3中||内的项)来计算重要性,而不会用梯度来更新虚拟参数。在训练中(3.2节和图1(B)),可以丢弃虚拟参数。得到的I_l与g_l大小相同,每个条目对应一个单元(神经元或注意力头)的重要性。

虽然等式3提供了一种可能的方法,但它不能直接应用。如果我们使用手头的领域数据并采用MLM损失作为L_{impt},∇_g_l只给出了领域特定知识的重要性。然而,为了计算单元对LM中通用知识的重要性(这是我们的目标),我们需要原始的预训练LM数据来计算L_{impt}。在实践中,LM用户无法访问这些数据。此外,等式3需要标签,但我们的领域语料在DAP-training中是无标签的。为了解决这些问题,作者提出了一个代理KL散度损失(L_{proxy})来替代L_{impt},以学习单元对通用知识的重要性。

#### 代理KL散度损失
作者提出使用模型鲁棒性作为代理,即尝试检测对LM鲁棒性重要的单元。它们的梯度∇_g_l然后表示鲁棒性和对LM模型的重要性。理由如下：如果一个I_{l,i}<sup>(0)(层l中单元i的重要性)有很高的值,那么它对LM的鲁棒性很重要,因为它的变化会导致LM产生很大变化。因此它是一个重要的单元。相反,如果I_{l,i}</sup>(0)很小,它就是一个不太重要的单元。

为了计算LM的鲁棒性,作者取当前领域数据的一个子集{x_n<sup>sub},将x_n</sup>sub输入LM两次以获得两个表示,然后计算它们之间的KL散度：

$ L_{impt} = KL(f_{LM}^1(x_n^sub), f_{LM}^2(x_n^sub)) $

其中f_{LM}<sup>1和f_{LM}</sup>2是具有不同dropout掩码的LM。我们不需要添加任何额外的dropout来实现这两个,因为Transformer已经在全连接层和注意力概率上放置了dropout掩码。因此,只需将相同的输入输入到Transformer两次就会得到具有不同dropout掩码的两个表示。由于dropout类似于添加噪声,两个表示之间的差异可以视为LM的鲁棒性。

### 3.2 训练：通过软掩码和对比损失学习新领域
回顾一下,作者希望在DAP-training期间使用累积的重要性I_l<sup>(≤t-1)来保留LM中学到的知识,当学习领域t时,这包括通用知识的重要性I_l</sup>(0)(3.1节)和已学习的每个领域k(k可以是{1...t-1}中的任何领域)的领域特定知识I_l^(k)的重要性(3.3节)。这是通过基于累积重要性对学习进行软掩码来实现的。

#### 累积重要性
在学习完任务t-1后,通过元素级最大值(EMax)累积重要性：

$ I_l^(≤t-1) = EMax({I_l^(t-1), I_l^(≤t-2)}) $

其中t指当前任务ID,I_l<sup>(≤t-2)指任务t-2时先前累积的重要性。我们不需要保存I_l</sup>0和所有{I_l<sup>(k)}_{k=1}</sup>{t-1}来计算等式5。我们只保存每个任务训练后增量累积的重要性。

#### 软掩码单元
给定层l的累积重要性I_l^(≤t-1)和DAP-training损失L_{DAP-train}(通常是MLM损失;作者还在等式7中提出了一个额外的损失),我们约束(或软掩码)其相应的梯度(∇_l)流如下：

$ \hat{\nabla}_l = (1 - I_l^(≤t-1)) \otimes \nabla_l $

如3.1节所述,我们扩展(通过复制)重要性I_l<sup>(≤t-1)以匹配∇_l的维度,以应用于所有相关参数。这是软掩码,因为I_l</sup>(≤t-1)中的每个元素都是[0,1]范围内的实数(不是二进制{0,1}),这给了模型调整任何单元的灵活性。

上述软掩码仅在反向传播中应用,但不在前向传播中应用,这鼓励了知识迁移,因为每个领域训练都可以利用从所有过去领域学到的知识。为了进一步鼓励模型从累积知识(I_l^(≤t-1))和完整知识(包括累积和当前领域知识)中学习良好的表示,作者引入了一种对比学习方法来鼓励互补表示。

#### 整合先前学习的知识和当前领域知识
软掩码有助于防止遗忘先前学习的知识。作者希望通过整合新知识和学习到的知识来进一步促进知识迁移。作者提出对比先前学习的知识和完整知识(包括先前学习的知识和当前领域知识)。注意,对比不能对共享的过去知识做任何操作,因为它受到软掩码的保护。因此,它有效地推动当前领域知识远离过去的知识,以形成互补。这是基于当前领域数据完成的,如下所示。

#### 对比学习的知识和完整知识
作者用o_full表示不考虑任何重要性的LM输出,它指完整知识。进一步将乘以重要性的LM输出(即I_l^(≤t-1) ⊗ o_l)表示为o_prev,它指先前学习的知识。通过使用o_full作为锚点,使用具有不同dropout的o_full作为正样本(表示为o_full+),使用o_prev作为负实例来对比这两者。

形式上,给定o_full_n, o_full+_n和o_prev_n,对比损失为(sim(·)是余弦相似度)：

$ L_{contrast} = -\frac{1}{N}\sum_{n=1}^N \log \frac{e^{sim(o_{full_n}, o_{full+_n})/\tau}}{\sum_{j=1}^N (e^{sim(o_{full_n}, o_{full+_j})/\tau} + e^{sim(o_{full_n}, o_{prev_j})/\tau})} $

与等式1相比,第二项被添加到分母中,即先前学习知识中的表示作为额外的负样本。图1(B)显示了一个从o_full指向自身的红色箭头,表示正样本来自输入两次。指向o_prev的虚线红色箭头表示对比完整知识和先前学习知识的负样本。

#### 最终损失函数
最终的DAP-training损失结合了应用提出的软掩码后的掩码语言模型(MLM)损失和提出的对比损失(λ是一个超参数)：

$ L_{DAP-train} = L_{MLM} + \lambda L_{contrast} $

### 3.3 计算单元对当前领域的重要性
在训练完新的/当前领域t之后,我们通过对该领域应用等式3来学习单元重要性。我们不需要像等式4中那样使用任何代理来计算L_{impt},因为我们可以直接使用当前领域数据。具体来说,我们随机采样当前领域数据的一个子集{(x_n<sup>sub, y_n</sup>sub)},其中x_n<sup>sub是输入,y_n</sup>sub是MLM自监督损失中的掩码标记。然后我们可以通过将L_{MLM}插入等式3中的L_{impt}轻松计算重要性I_l<sup>(t)。得到的I_l</sup>(t)将在下一个任务中通过与先前累积的重要性累积(等式5)和软掩码学习(等式6)来使用。

## 4. 实验
作者使用RoBERTa作为LM。遵循标准评估设置,在训练完一个领域后,其训练数据会被丢弃。在所有领域增量学习后,通过在所有领域的下游任务上微调来评估最终模型。

### 4.1 数据集和基线
#### 数据集
表1显示了6个用于DAP-training的无标签领域语料库和它们对应的6个下游任务分类数据集的统计信息。其中3个是关于评论的：Yelp Restaurant、Amazon Phone和Amazon Camera;3个是学术论文：ACL Papers、AI Papers和PubMed Papers。它们对应的下游任务分类数据集是：Restaurant、Phone、Camera、ACL、AI和PubMed。

#### 基线
作者使用了16个基线,包括非持续学习(Non-CL)和持续学习(CL)基线。所有CL基线最初都是为学习监督数据设计的,除了DEMIX。作者对它们进行了调整,并将它们的主干网络替换为RoBERTa。

非CL基线：这里的每个基线都为每个任务构建一个单独的模型。

1. Pool：将所有领域的数据池化在一起,为所有领域训练一个模型。
2. RoBERTa：使用RoBERTa进行下游任务微调,不进行DAP-training。
3. DAP-RoBERTa：使用Gururangan等(2020)中的现有DAP-training方法(MLM)分别对每个领域进行后训练。
4. DAP-Adapter：在Transformer中为每个领域添加adapter层进行DAP-training。只有添加的adapters是可训练的。在下游任务微调中,RoBERTa和adapters都是可训练的。
5. DAP-Prompt：来自Lester等(2021)。在DAP-training中,RoBERTa(LM)是固定的,只训练prompts。在下游任务微调中,LM和训练好的prompt都是可训练的。

CL基线：作者使用了2个朴素基线,它们不断学习更多领域,没有机制来处理灾难性遗忘或迁移。  
6. NCL(朴素CL)：持续DAP-train RoBERTa;  
7. NCL-Adapter：持续DAP-train一组adapters。

8个基线是CL系统：  
8. DEMIX：为每个新领域添加一个新的adapter,并用最接近新领域的先前adapter初始化它;  
9. BCL：使用胶囊网络;  
10. CLASSIC：使用对比学习;  
11. KD：知识蒸馏;  
12. EWC：一种流行的基于正则化的方法;  
13. DER++：一种基于知识蒸馏的重放方法;  
14. HAT：一种有效的参数隔离方法;  
15. HAT-All：HAT的一个变体,使用LM的所有特征来执行下游任务;  
16. HAT-Adapter：在adapters内使用HAT。

### 4.2 结果分析和消融研究
由于篇幅限制,实现细节在附录C中给出。表2报告了所有15个系统在6个数据集上的下游任务微调结果。我们可以看到,所提出的DAS在平均性能上优于所有基线,并且在知识迁移方面(负遗忘率)也取得了最佳结果。

1. DAS略优于Pool。这可能是因为：(a)一些领域差异很大(例如相机评论和ACL论文),导致Pool中存在一些负迁移。(b)DAS可以在通用和先前领域知识受软掩码保护的情况下学习。
2. DAS同时实现了防止遗忘和知识迁移。那些只关注防止遗忘的基线(KD、EWC、DER++)表现较差,因为它们牺牲了准确性来避免灾难性遗忘。那些执行知识迁移的基线(BCL、CLASSIC和DEMIX)取得了更好的结果,但仍不如DAS。DEMIX的迁移非常弱。BCL虽然可以避免灾难性遗忘同时实现一些迁移,但仍比NCL弱。总的来说,CL基线都不如DAS,因为它们没有方法来鼓励知识迁移,或者必须依赖adapters。
3. 直接在LM内学习领域有助于DAS取得比基于adapter和prompt的方法更好的结果。DAS优于基于adapter的系统(DAP-Adapter、NCL-Adapter和HAT-Adapter)和基于prompt的系统(DAP-Prompt)。这是因为adapters和prompts没有足够的可训练参数,它们也是随机初始化的,可能很难训练。
4. 使用完整LM来学习所有任务而不是使用子网络(HAT-based方法)使DAS更有效。HAT表现不佳,表明它不适合DAP-training,正如第1节所讨论的。即使我们使用所有特征(不仅仅是对应子网络的特征),我们仍然得到较差的结果(HAT-All),因为DAP-training中使用的特征(在LM子网络中)与下游任务微调中使用的特征(来自整个LM的特征)不同。

#### 知识迁移和避免遗忘
为了了解模型在灾难性遗忘和知识迁移方面的表现,作者比较了遗忘率(forget R.)：

$ \frac{1}{t-1}\sum_{k=1}^{t-1} A_{k,k} - A_{t,k} $

其中A_{k,k}是在其领域k刚DAP-trained后的下游任务准确率,A_{t,k}是在DAP-training最后一个领域t之后领域k的下游任务准确率。作者对除最后一个领域外的所有下游任务取平均,因为最后一个领域没有遗忘。遗忘率越高,遗忘越多。负率表示正知识迁移。显然,DAS具有最强的负遗忘率,表明它在防止遗忘和知识迁移方面都做得很好。NCL、NCL-Adapter、DEMIX、EWC、KD和DER++都遭受了一些遗忘。HAT没有遗忘但学习效果不佳。HAT和BCL没有遗忘但在迁移方面较弱。

#### 代理KL散度损失的有效性
作者在初始化函数(3.1节)中使用代理KL散度损失来计算单元对LM中通用知识的重要性。作者通过两种实验来提供证据,说明这种代理方法的有效性。

1. 与D_0的样本集进行比较。在某些情况下,持续DAP-training用户可能有用于预训练LM的数据D_0。然后我们可以从D_0中采样一个子集来计算参数对LM中通用知识的重要性。然而,由于作者没有用于预训练RoBERTa的D_0,他们使用Wiki数据作为D_0的样本集。作者选择它是因为它是一个具有广泛主题覆盖的通用数据集,并且用于预训练LM,它的大小与作者的领域数据相似(约700M)。作者进行了两个实验：  
(a) DAS(Wiki+MLM)：在初始化阶段使用MLM作为损失来计算单元的重要性(以识别通用知识),就像持续学习部分中的任何其他领域一样。  
(b) DAS(Wiki+KL)：在初始化阶段使用KL散度,就像提出的代理方法一样。

结果显示在表3中。我们可以看到DAS(Wiki+KL)的表现与DAS相似,但优于DAS(Wiki+MLM)。这表明提出的代理KL散度更有效。MLM实际上将LM适应到Wikipedia数据,这可能不足以代表用于预训练LM的原始数据。结果,它最终识别出的知识只适用于Wikipedia数据。相比之下,提出的代理KL散度利用随机dropout掩码并测量鲁棒性,这与特定领域的关系较小,因此更好地反映了原始LM中的(通用)知识。

2. 比较从不同领域语料库计算的通用知识。作者还提供了一些间接证据来展示代理方法对计算LM中通用知识的单元重要性的有效性。他们进行了一个单独的非CL实验,比较使用不同领域数据应用代理后的注意力头重要性分数向量。对于每个领域i,他们将其重要性向量与每个其他领域的重要性向量进行比较,然后平均余弦相似度得到领域i的值。得到Restaurant为0.92,ACL、AI和Phone均为0.91,PubMed为0.89,Camera为0.92。我们看到不同领域给出了相似的重要性值,这间接表明作者的代理可以近似识别共同的通用知识。

#### 消融实验
作者想知道提出的(1)初始化(3.1节),(2)软掩码和(3)对比学习是否有帮助。为了回答(1),作者进行了DAS(w/o initialization)的消融,其中移除了初始化,直接进行持续学习,不考虑LM中的通用知识。为了回答(2),作者进行了两个消融：(1)DAS(w/o softmask),其中移除了软掩码,只使用基于等式7的对比学习(分母中的第二项被移除);(2)DAS(random)使用随机生成的重要性分数来进行软掩码和对比学习。为了回答(3),作者进行了两个消融：(i)DAS(w/o contrast),其中移除了对比损失,只根据重要性进行软掩码;(ii)DAS(domain-specific),其中对比领域特定知识和学习到的知识(3.2节)。

表4显示完整的DAS在平均性能和大多数领域上是最好的,表明每个组件都有贡献。其他观察结果包括：

1. DAS的收益部分来自保留的通用知识。我们可以看到DAS(w/o initialization)的平均性能较差;
2. 软掩码有帮助,因为DAS(w/o softmask)比DAS差。这是合理的,因为软掩码可以保留学习到的领域。此外,作者基于梯度的掩码是有信息的,因为DAS(random)比DAS差;
3. 对比学习是有效的,因为DAS(w/o contrast)和DAS(domain-specific)都表现较差,表明DAS中的对比学习可以帮助学习良好的表示。

## 5. 结论
本文提出了一种新方法DAS用于LM的持续DAP-training。它有三个关键思想：

1. 通过根据单元重要性进行软掩码来保留重要的先前知识,以克服灾难性遗忘并促进知识迁移。
2. 使用新颖的代理方法来计算LM中通用知识的单元重要性。
3. 学习互补表示以实现知识整合。

作者提出了一系列技术来实现这些目标。大量实验表明了DAS的有效性。当前方法在学习中涉及两个功能。在未来,作者将研究如何将它们结合起来以进一步改进结果。

## 附录
### A. 数据集详情
表1中已经显示了每个数据集的样本数量。这里提供了4种下游任务的额外细节：

1. (Phone, Camera和Restaurant)方面情感分类(ASC)：给定一个方面或产品特征(例如,相机评论中的画质)和包含该方面的领域或产品类别(例如,相机)的评论句子,分类该句子是否表达了关于该方面的正面、负面或中性(无意见)情感或极性(对于Phone和Camera数据集,数据中只有负面和正面极性)。
2. (ACL)引文意图分类：给定一个引用句(包含引用的句子),分类该句子是否表达了"背景"、"动机"、"使用"、"扩展"和"比较或对比未来"中的一种引用功能。
3. (AI)关系分类：给定一个包含一对实体的句内词序列跨度,分类该跨度是否表达了"特征of"、"连接"、"评估for"、"下位词of"、"用于"、"部分of"和"比较"中的一种关系。
4. (PubMed)化学-蛋白质相互作用分类：给定一个包含一对化学物质和蛋白质的跨度,分类该跨度是否表达了"下调剂"、"底物"、"间接上调剂"、"间接下调剂"、"激动剂"、"激活剂"、"产物of"、"激动剂-激活剂"、"抑制剂"、"上调剂"、"底物产物of"、"激动剂-抑制剂"和"拮抗剂"中的一种化学-蛋白质相互作用。

### B. 基线详情
[此部分内容与原文中已提供的基线描述基本相同,不再重复]

### C. 实现细节
架构：作者采用RoBERTa_BASE作为主干LM。DAP-training应用了一个掩码语言模型头。RoBERTa的下游任务微调遵循标准做法。对于三个ASC任务(见表1),作者采用Xu等(2019)中的ASC公式,其中方面(例如"声音")和评论句子(例如"声音很棒")通过连接。

超参数：除非另有说明,所有实验中使用相同的超参数。最大输入长度设置为164,这对所有数据集都足够。DAP-training和下游任务微调都使用Adam优化器。最大序列长度也设置为164。

DAP-training：学习率设置为1e-4,批量大小为256。按照Gururangan等(2020)和Xu等(2019)的做法,每个领域训练2.5K步,大约是对领域数据的一次完整遍历。用于计算L_{impt}以确定3.1节和3.3节中头部重要性的数据子集{x_n^sub}设置为1.64百万个标记,这在实验中已经足够。等式8中的λ设置为1,等式7中的τ设置为0.05。

下游任务微调：学习率设置为1e-5,批量大小为16。对Restaurant的下游任务微调数据集训练5个epoch;对ACL、AI和PubMed训练10个epoch;对Phone和Camera训练15个epoch。作者简单地取最后一个epoch的结果,假设没有验证集。作者凭经验发现,上述epoch数可以给出稳定和收敛的结果。

### D. 不同顺序的DAP-training
表2报告了顺序Restaurant → ACL → AI → Phone → PubMed → Camera的结果。作者还研究了顺序如何影响结果。由于DAP-training的计算密集性质,作者只报告了最佳基线(NCL)和DAS在5种不同顺序下的结果。表5显示了NCL和DAS在5种不同顺序下的结果。我们可以看到DAS始终优于NCL,证明了DAS的有效性。

### E. 标准差
表6报告了表2(在主文中)中DAS和所考虑的基线在5次随机种子运行中相应结果的标准差。我们可以看到DAS的结果是稳定的。一些基线(例如AI中的RoBERTa,Camera中的DAP-RoBERTa)可能有相当大的标准差。

表7报告了表4(在主文中)中DAS及其变体在5次随机种子运行中相应结果的标准差。我们可以看到DAS及其变体的结果是稳定的。

总的来说,这些附录提供了更多关于实验设置、数据集细节和结果稳定性的信息,进一步支持了主文中的结论。DAS方法在不同的领域顺序和多次运行中都表现出了一致的优势,证明了其在持续DAP-training任务中的有效性和稳定性。


> 原文：《Overcoming catastrophic forgetting in neural networks 》
>

## 摘要
本文提出了一种名为弹性权重巩固（Elastic Weight Consolidation， EWC）的算法，可以让神经网络在学习新任务的同时保护先前任务的知识，从而避免灾难性遗忘。EWC的工作原理是通过选择性地降低对先前任务重要的权重的可塑性。具体实现上，EWC通过一个软的二次约束，将每个权重拉回其旧值，拉回的程度与该权重对先前任务性能的重要性成正比。

在分析上可处理的设置中，我们证明EWC可以保护网络权重免受干扰，从而提高与普通梯度下降相比记忆保留的比例。在任务具有共享结构的情况下，用EWC训练的网络可以重用网络的共享组件。我们进一步展示了EWC可以有效地与深度神经网络结合，支持在具有挑战性的强化学习场景（如Atari 2600游戏）中进行持续学习。

## 1. 引言
实现人工通用智能要求智能体能够学习和记住许多不同的任务。这在现实世界的设置中尤其困难：任务序列可能没有明确标记，任务可能会不可预测地切换，任何单独的任务可能在很长时间内都不会重复出现。因此，智能体必须具备持续学习的能力：即能够连续学习任务而不忘记如何执行先前训练的任务。

持续学习对人工神经网络提出了特殊的挑战，因为它们倾向于在学习新任务（如任务B）时突然丧失对先前学习的任务（如任务A）的知识。这种现象被称为灾难性遗忘，特别发生在网络按顺序在多个任务上训练时，因为对任务A重要的网络权重被改变以满足任务B的目标。

尽管机器学习，特别是深度神经网络最近在各种领域取得了令人印象深刻的进展，但在实现持续学习方面几乎没有进展。目前的方法通常确保训练期间所有任务的数据同时可用。通过在学习过程中交错多个任务的数据，不会发生遗忘，因为网络的权重可以被联合优化以在所有任务上都有良好的表现。在这种范式下（通常被称为多任务学习范式），深度学习技术已被用于训练能够成功玩多个Atari游戏的单个智能体。如果任务是按顺序呈现的，多任务学习只能在数据被记录在情景记忆系统中并在训练期间重放给网络的情况下使用。这种方法（通常被称为系统级巩固）对于学习大量任务是不切实际的，因为在我们的设置中，它需要存储和重放的记忆量与任务数量成正比。因此，缺乏支持持续学习的算法仍然是发展人工通用智能的一个关键障碍。

与人工神经网络形成鲜明对比的是，人类和其他动物似乎能够以持续的方式学习。最近的证据表明，哺乳动物大脑可能通过保护新皮层回路中先前获得的知识来避免灾难性遗忘。当老鼠获得新技能时，一部分兴奋性突触被加强;这表现为神经元树突棘体积的增加。关键的是，这些增大的树突棘在随后学习其他任务时仍然存在，解释了几个月后仍能保持性能。当这些棘被选择性地"擦除"时，相应的技能就被遗忘了。这提供了因果证据，表明支持这些加强突触保护的神经机制对于保持任务性能至关重要。这些实验发现——连同级联模型等神经生物学模型——表明新皮层中的持续学习依赖于任务特定的突触巩固，通过使一部分突触变得不那么可塑从而在长时间尺度上保持稳定，知识得以持久编码。

## 2. 弹性权重巩固（EWC）算法
![](https://cdn.nlark.com/yuque/0/2024/png/406504/1728457357935-0e32873d-16f2-48a1-84e0-b21256da7f2a.png)

### 2.1 算法原理
EWC算法的灵感来自于任务特定的突触巩固，为人工智能提供了一个独特的持续学习问题的解决方案。该算法基于特定权重对先前任务的重要性来减缓这些权重的学习。

具体来说，EWC通过在学习新任务时添加一个约束项来保护旧任务的性能。这个约束项是一个二次惩罚，可以想象为一个将参数锚定到先前解的弹簧，因此被称为弹性。重要的是，这个弹簧的刚度不应对所有参数都相同;相反，它应该对最影响任务A性能的参数更大。

### 2.2 算法推导
从概率的角度来看，优化参数等同于找到给定某些数据D的参数最可能的值。我们可以使用贝叶斯规则从参数的先验概率p（θ）和数据的概率p（D|θ）计算这个条件概率p（θ|D）：

log p（θ|D） = log p（D|θ） + log p（θ） - log p（D）

注意，给定参数的数据对数概率log p（D|θ）就是所考虑问题的负损失函数-L（θ）。假设数据被分为两个独立的部分，一部分定义任务A（DA），另一部分定义任务B（DB）。然后，我们可以重新排列等式：

log p（θ|D） = log p（DB|θ） + log p（θ|DA） - log p（DB）

注意，左边仍然描述了给定整个数据集的参数的后验概率，而右边只依赖于任务B的损失函数log p（DB|θ）。因此，关于任务A的所有信息必须已经被吸收到后验分布p（θ|DA）中。

这个后验概率必须包含关于哪些参数对任务A重要的信息，因此是实现EWC的关键。真实的后验概率是难以处理的，所以我们参考Mackay关于拉普拉斯近似的工作，将后验概率近似为高斯分布，其均值由参数θ*A给出，对角精度由Fisher信息矩阵F的对角线给出。

Fisher信息矩阵F有三个关键性质：

1. 它等价于损失函数在最小值附近的二阶导数
2. 它可以仅从一阶导数计算，因此即使对于大型模型也易于计算
3. 它保证是半正定的

给定这个近似，我们在EWC中最小化的函数L（θ）是：

L（θ） = LB（θ） + Σi λ/2 Fi （θi - θ*A，i）^2

其中LB（θ）只是任务B的损失，λ设置旧任务相对于新任务的重要性，i标记每个参数。

当移动到第三个任务（任务C）时，EWC将尝试保持网络参数接近任务A和B的学习参数。这可以通过两个单独的惩罚项或通过注意两个二次惩罚项的和本身就是一个二次惩罚项来实现。

### 2.3 算法实现
EWC算法的具体实现步骤如下：

1. 在每个任务切换时计算Fisher信息矩阵。
2. 对于每个任务，添加一个惩罚项，锚点由参数的当前值给出，权重由Fisher信息矩阵乘以一个缩放因子λ给出。λ通过超参数搜索进行优化。
3. 只对经历过至少2000万帧的游戏添加EWC惩罚。
4. 在训练新任务时，损失函数变为：  
L（θ） = LB（θ） + Σi λ/2 Fi （θi - θ*A，i）^2  
其中LB（θ）是新任务B的损失，第二项是EWC惩罚项。
5. 在反向传播时，除了常规梯度外，还要考虑EWC惩罚项的梯度。

通过这种方式，EWC可以在学习新任务的同时保护旧任务的重要权重，从而实现持续学习。

## 3. 实验结果
![](https://cdn.nlark.com/yuque/0/2024/png/406504/1728457384309-7b8ecaac-633f-4492-a20e-30e6da987b6a.png)

### 3.1 随机模式实验
作为初步演示，我们训练了一个线性网络来将随机（即不相关的）二进制模式与二进制结果关联起来。虽然这个问题在重要方面与我们稍后检查的更现实的设置不同，但这种情况允许分析解，因此提供了EWC和普通梯度下降之间关键差异的洞察。

在这种情况下，总Fisher信息矩阵的对角线与观察到的模式数量成正比;因此在EWC的情况下，学习率随着观察到更多模式而降低。遵循ref. 15，我们将一个记忆定义为被保留，如果其信噪比（SNR）超过某个阈值。

图2上部显示了使用梯度下降（蓝线）和EWC（红线）获得的第一个观察到的模式的SNR。起初，两种情况下的SNR非常相似，遵循斜率为-0.5的幂律衰减。当观察到的模式数量接近网络容量时，梯度下降的SNR开始呈指数衰减，而EWC保持幂律衰减。梯度下降观察到的指数衰减是由于新模式干扰旧模式;EWC保护不受这种干扰，并增加保留记忆的比例（图2下部）。

### 3.2 MNIST监督学习实验
接下来，我们研究了EWC是否可以让深度神经网络学习一系列更复杂的任务而不发生灾难性遗忘。具体来说，我们在几个监督学习任务上按顺序训练了一个全连接多层神经网络。在每个任务内，我们以传统方式训练神经网络，即通过打乱数据并以小批量处理它们。然而，在每个任务上训练固定数量后，我们不允许在该任务的数据集上进行进一步训练。

我们根据先前在持续学习文献中使用的方案，从分类手写数字的MNIST数据集构建了一系列任务。对于每个任务，我们生成一个固定的随机排列，通过该排列对所有图像的输入像素进行混洗。因此，每个任务的难度相等，但需要不同的解决方案。

图3A显示，仅使用普通随机梯度下降（SGD）训练这一系列任务会导致灾难性遗忘。蓝色曲线显示了两个不同任务的测试集性能。在训练机制从第一个任务（A）切换到第二个任务（B）的时刻，任务B的性能迅速下降，而任务A的性能急剧上升。随着训练时间的增加和后续任务的添加，任务A的遗忘进一步加剧。这个问题不能通过使用固定的二次约束对每个权重进行正则化来解决（绿色曲线，L2正则化）：在这里，任务A的性能下降不那么严重，但任务B无法正确学习，因为约束平等地保护所有权重，留给学习B的空间很小。然而，当我们使用EWC时，并因此考虑到每个权重对任务A的重要性，网络可以很好地学习任务B而不忘记任务A（红色曲线）。这正是图1中示意性描述的行为。

先前尝试解决深度神经网络持续学习问题的方法依赖于仔细选择网络超参数，结合其他标准正则化方法，以缓解灾难性遗忘。然而，在这个任务上，它们只在最多两个随机排列上取得了合理的结果。使用与ref. 24类似的交叉验证超参数搜索，我们比较了传统的dropout正则化和EWC。我们发现，仅使用dropout正则化的随机梯度下降是有限的，并且无法扩展到更多任务（图3B）。相比之下，EWC允许按顺序学习大量任务，错误率只有适度增长。

鉴于EWC允许网络有效地将更多功能压缩到固定容量的网络中，我们可能会问它是否为每个任务分配完全独立的网络部分，还是通过共享表示以更有效的方式使用容量。为了评估这一点，我们通过测量每对任务各自Fisher信息矩阵之间的重叠（Fisher重叠）来确定每个任务是否依赖于相同的权重集。小重叠意味着两个任务依赖于不同的权重集（即EWC将网络的权重划分为不同任务）;大重叠表示权重被用于两个任务（即EWC实现了表示共享）。图3C显示了重叠作为深度的函数。作为一个简单的对照，当网络在两个非常相似的任务上训练时（两个版本的MNIST，其中只有几个像素被置换），这些任务在整个网络中依赖于相似的权重集（灰色虚线曲线）。当两个任务彼此更不相似时，网络开始为两个任务分配单独的权重（黑色虚线）。然而，即使对于大的置换，网络更接近输出的层确实被两个任务重用。这反映了置换使输入域非常不同，但输出域（即类别标签）是共享的事实。

### 3.3 Atari强化学习实验
![](https://cdn.nlark.com/yuque/0/2024/png/406504/1728457401905-7bde79c5-07c2-4180-a5d2-4567d08e725d.png)

接下来，我们测试EWC是否可以支持在更具挑战性的强化学习（RL）领域进行持续学习。在RL中，智能体动态地与环境交互，以发展一种最大化累积未来奖励的策略。我们询问深度Q网络（DQN）——一种在这些具有挑战性的RL设置中取得令人印象深刻的成功的架构——是否可以与EWC结合，成功支持经典Atari 2600任务集中的持续学习。

具体来说，每个实验由10个从DQN达到或超过人类水平的游戏中随机选择的游戏组成。在训练时，智能体长时间暴露于每个游戏的经验中。游戏的呈现顺序是随机的，并允许多次回到同一游戏。我们还会定期测试智能体在所有10个游戏上的得分，而不允许智能体在这些游戏上进行训练（图4A）。

值得注意的是，以前的强化学习持续学习方法要么依赖于增加网络容量，要么依赖于在单独的网络中学习每个任务，然后用这些网络来训练一个可以玩所有游戏的单一网络。相比之下，这里呈现的EWC方法使用具有固定资源（即网络容量）的单一网络，并且计算开销最小。

除了使用EWC保护先前获得的知识外，我们还利用RL领域来解决成功持续学习系统所需的更广泛的要求集：特别是，需要更高级的机制来推断当前正在执行的任务，检测和整合遇到的新任务，并允许快速灵活地在任务之间切换。在灵长类动物大脑中，前额叶皮层被广泛认为通过维持任务上下文的神经表征来支持这些能力，这些表征对感觉处理、工作记忆和行动选择施加自上而下的选通影响。

受这些证据的启发，我们通过额外的功能增强了DQN智能体，以处理切换任务上下文。EWC算法需要知道正在执行的任务，因为它告知当前哪些二次约束是活跃的，以及在任务上下文改变时应该更新哪个二次约束。为了推断任务上下文，我们实现了一个在线聚类算法，该算法无需监督训练，并基于遗忘目标过程（FMN）。我们还允许DQN智能体为每个推断的任务维护单独的短期记忆缓冲区。这些允许使用经验回放机制离线学习每个任务的动作值。因此，整个系统具有两个时间尺度的记忆：在短时间尺度上，经验回放机制允许DQN的学习基于交错和不相关的经验。在更长的时间尺度上，通过使用EWC在任务之间巩固知识。最后，我们允许少量网络参数是特定于游戏的。特别是，我们允许网络的每一层都有特定于每个游戏的偏置和每元素乘法增益。

图4比较了使用EWC的智能体（红色）与不使用EWC的智能体（蓝色）在10个游戏集上的性能。我们将性能衡量为所有10个游戏的总人类标准化分数;每个游戏的分数被限制在1，使得总最高分数为10（在所有游戏上至少达到人类水平），0意味着智能体与随机智能体一样好。如果我们仅依赖于普通梯度下降方法，如ref. 25中所述，智能体永远不会学会玩多个游戏，而遗忘旧游戏造成的损害意味着总人类标准化分数保持在1以下。然而，通过使用EWC，智能体确实学会了玩多个游戏。作为对照，我们还考虑了如果我们明确提供给智能体真实任务标签（图4B，棕色），而不是依赖通过FMN算法学习的任务识别（图4B，红色）对智能体的好处。这里的改进只是适度的。

虽然使用EWC增强DQN智能体允许它按顺序学习多个游戏而不受灾难性遗忘的影响，但它没有达到通过训练10个独立的DQN获得的分数（图S3）。这可能的一个原因是，我们基于参数不确定性的一个可处理近似（Fisher信息）为每个游戏巩固权重。因此，我们寻求empirically测试我们估计的质量。为此，我们在单个游戏上训练了一个智能体，并测量扰动网络参数如何影响智能体的得分。无论智能体在哪个游戏上训练，我们都观察到相同的模式，如图4C所示。首先，智能体总是对由Fisher信息对角线的逆shaped的参数扰动（蓝色）更加鲁棒，而不是均匀扰动（黑色）。这验证了Fisher信息的对角线是估计参数重要性的一个好方法。在我们的近似中，在零空间中扰动应该对性能没有影响。然而，我们empirically观察到在这个空间中扰动（橙色）与在逆Fisher空间中扰动具有相同的效果。这表明我们对某些参数不重要的估计过于自信：因此，当前实现的主要限制很可能是它低估了参数不确定性。

## 4. 讨论
我们提出了一种算法EWC，它允许在新学习过程中保护先前任务的知识，从而避免灾难性遗忘。它通过选择性地降低权重的可塑性来实现这一点，因此与突触巩固的神经生物学模型有某些相似之处。我们将EWC实现为一个软的二次约束，其中每个权重被拉回其旧值，拉回程度与其对先前学习任务性能的重要性成正比。在分析上可处理的设置中，我们证明EWC可以保护网络权重免受干扰，从而增加与普通梯度下降相比保留的记忆比例。在任务共享结构的程度上，用EWC训练的网络重用网络的共享组件。我们进一步展示了EWC可以有效地与深度神经网络结合，以支持在具有挑战性的强化学习场景（如Atari 2600游戏）中进行持续学习。

EWC算法可以基于贝叶斯方法进行学习。形式上，当有一个新任务要学习时，网络参数受到先验的调节，这个先验是给定先前任务数据的参数后验分布。这使得在先前任务poorly约束的参数上可以有快速学习率，而在那些关键参数上有慢速学习率。

之前已经有工作使用二次惩罚来近似旧数据集的部分，但这些应用仅限于小型模型。具体来说，ref. 35使用随机输入来计算能量表面的二次近似。他们的方法很慢，因为它需要在每个样本上重新计算曲率。ref. 36中描述的ELLA算法需要计算和求逆与正在优化的参数数量相等的维度的矩阵;因此它主要应用于线性和logistic回归。相比之下，EWC的运行时间在参数数量和训练样本数量上都是线性的。我们只能通过使用参数真实后验分布的粗略拉普拉斯近似来实现这种低计算复杂度。尽管计算成本低廉且empirically成功——即使在具有挑战性的RL领域设置中——我们使用后验方差的点估计（如在拉普拉斯近似中）确实构成了一个显著的弱点（图4C）。我们最初的探索表明，人们可能可以通过使用贝叶斯神经网络来改进这个局部估计。

虽然本文主要集中于构建一种受神经生物学观察和理论启发的算法，但考虑算法的成功是否可以反馈到我们对大脑的理解也很有启发性。特别是，我们看到EWC与两种突触可塑性的计算理论之间有相当大的相似之处。

突触可塑性的级联模型构建突触状态的动力学模型，以理解可塑性和记忆保持之间的权衡。级联模型与我们的方法有重要的不同。特别是，它们旨在延长系统稳态下的记忆寿命（即观察无限多刺激的极限）。因此，它们允许突触变得更加或更少可塑，并建模保留和遗忘记忆的过程。相比之下，我们解决了一个更简单的问题，即在从空网络开始时保护网络免受干扰。事实上，在EWC中，权重只能变得更受约束（即更少可塑）随时间推移，因此我们只能建模记忆保留而不是遗忘。因此，当观察到的随机模式数量超过网络容量并达到稳态时，EWC开始表现甚至比普通梯度下降更差（图2下部）。此外，EWC模型——像标准Hopfield网络一样——容易在网络容量饱和时出现全黑灾难现象，导致无法检索任何先前的记忆或存储新的经验。值得注意的是，在我们设计EWC的更现实条件下，我们没有观察到这些限制——可能是因为网络在这些范式中远未达到容量饱和。

尽管存在这些关键差异，EWC和级联共享了通过调节突触的可塑性来延长记忆寿命的基本算法特征。虽然先前关于级联模型的工作将亚稳态与增强和抑制事件的模式联系起来——即突触水平的测量——但我们的方法关注于决定每个突触可能被巩固程度的计算原则。可能通过实验区分这些模型，因为在级联模型中突触的可塑性取决于增强事件的速率，而在EWC中则取决于任务相关性。

在这方面，我们在这里提供的视角与最近的一个提议一致，即每个突触不仅存储其当前权重，还隐式表示其对该权重的不确定性。这个想法基于以下观察：突触后电位在幅度上高度可变（暗示在计算过程中从权重后验采样）且那些更可变的突触更容易增强或抑制（暗示更新权重后验）。虽然我们没有在这里探讨从后验采样的计算益处，但我们的工作与权重不确定性应该影响学习率的观点一致。我们更进一步，强调巩固高精度权重使长时间尺度上的持续学习成为可能。对于EWC，每个突触必须存储三个值：权重本身、其方差和其均值。有趣的是，大脑中的突触也携带不止一条信息。例如，短期可塑性的状态可能携带方差信息。与早期可塑性阶段相关的权重可能编码当前突触强度，而与晚期可塑性阶段或巩固阶段相关的权重可能编码平均权重。

## 5. 结论
能够连续学习任务而不忘记是生物和人工智能的核心组成部分。在这项工作中，我们展示了一种支持持续学习的算法——从突触巩固的神经生物学模型中获得灵感——可以与深度神经网络结合，在一系列具有挑战性的领域中取得成功的表现。通过这样做，我们证明了当前关于突触巩固的神经生物学理论确实可以扩展到大规模学习系统。这提供了初步证据，表明这些原则可能是大脑学习和记忆的基本方面。


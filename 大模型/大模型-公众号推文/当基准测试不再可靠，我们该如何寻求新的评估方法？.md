在过去的几年里，在迁移学习的进步的推动下，NLP中的模型变得更加强大。性能急剧提高的一个后果是现有基准被抛在后面。最近的模型“已经超过了测试它们的基准”，在SuperGLUE和SQuAD等标准基准上迅速达到超人的性能。这是否意味着我们已经解决了自然语言处理？远非如此。

然而，评估NLP模型性能的传统做法，使用单一指标，如准确率或BLEU，依赖静态基准和抽象任务公式，鉴于模型惊人地强大的肤浅自然理解语言能力，不再有效。因此，我们需要重新思考如何设计基准和评估模型，以便它们仍然可以作为未来进步的有用指标。

## 什么是基准？
> "数据集是我们领域的望远镜."-阿拉文·乔希
>

该术语最初的用法是指测量人员在石头结构上做的水平标记，可以在其中放置角铁，以形成水平杆的“工作台”。象征性地说，基准是指可以比较事物的标准参考点。在机器学习或自然语言处理中使用的基准通常有几个组成部分：它由一个或多个数据集、一个或多个相关指标以及一种汇总性能的方法组成。

基准设定了评估社区商定的不同系统性能的标准。为了确保基准被社区接受，许多最近的基准要么选择一组有代表性的标准任务，如GLUE或XTREME，要么积极向社区征求任务建议，如SuperGLUE、GEM或BIG-Bench。

对于该领域的人来说，基准是追踪进展的至关重要的工具，阿拉文德·乔希说，没有基准来评估我们模型的性能，我们就像“天文学家想要看到星星，但拒绝建造望远镜”。

对于从业者和局外人来说，基准测试为一个领域提供了一个客观视角，使他们能够识别有用的模型并跟踪一个领域的进展。例如，《2021年人工智能指数报告》使用SuperGLUE和SQuAD作为自然语言处理整体进展的代理。

在有影响力的基准上达到人类表现通常被视为一个领域的关键里程碑。AlphaFold 2在CASP 14竞赛中通过实验方法达到性能竞争力标志着结构生物学领域的重大科学进步。

## 基准测试简史
> "创建好的基准比大多数人想象的要难。"-约翰·马西
>

基准测试被用来评估计算系统性能的历史由来已久。标准性能评估公司（SPEC）成立于1988年，是致力于对计算机硬件性能进行基准测试的最古老的组织之一。至关重要的是，SPEC得到了该领域最重要公司的支持。每年，它都会发布不同的基准测试集，每个基准测试集由多个程序组成，性能以每秒数百万条指令的几何平均值（MIPS）来衡量。

SPEC最近的一个特定于ML的类似物是MLCommons，它组织了MLPerf系列专注于模型训练和推理的性能基准。与SPEC类似，MLPerf拥有学术界和工业界的广泛支持，建立在之前百度的DeepBench或斯坦福的DAWNBench等个人绩效衡量努力的基础上。

对于DARPA和NIST等美国机构来说，基准在衡量和跟踪科学进步方面发挥了至关重要的作用。早期的自动语音识别（自动语音识别）基准如TIMIT和总机从1986年开始由DARPA资助并由NIST协调。后来在ML的其他领域如MNIST有影响力的基准也是基于NIST的数据。

对于语言技术和信息检索（IR），NIST举办了DARPA资助的TREC系列研讨会，涵盖了广泛的轨道和主题，如下所示。TREC组织了基于克兰菲尔德在20世纪60年代开创的评估范式的竞赛，在该范式中，模型根据一组测试集进行评估，包括文档、问题和人类相关性判断。由于主题之间的性能差异很大，因此对许多主题进行平均得分。TREC的“标准、广泛可用和精心构建的数据集为IR的进一步创新奠定了基础”（瓦里安，2008年）。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1704869379495-058d4f41-fb2e-4aa3-b59b-6f7d3dd9af06.png)

许多最近有影响力的基准测试，如ImageNet、SQuAD或SNLI，规模很大，由数十万个例子组成，由资金充足的大学的学术团体开发。在深度学习时代，这种大规模数据集被认为是推动研究进步的支柱之一，NLP或生物学等领域见证了它们的“ImageNet时刻”。

随着模型变得更加强大和通用，基准变得更加application-oriented，越来越多地从单任务转向多任务和单域转向多域基准。这些趋势的关键例子是从关注核心语言任务（如词性标注和依赖句法分析）过渡到更接近现实世界的任务（如面向目标的对话和开放域问答；多任务数据集（如GLUE）的出现；以及多模态数据集（如WILDS）。

然而，虽然在MNIST或Switchboard等经典基准上实现超人的性能花了超过15年的时间，但模型在发布后大约一年内在GLUE和SQuAD 2.0等较新的基准上实现了超人的性能，如下图所示。与此同时，我们知道这些基准旨在测试的功能，如一般问答，远未得到解决。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1704869378971-796889e0-2e91-4f32-9d3e-f12c8bc023a2.png)

导致这些基准饱和的另一个因素是，与早期基准相比，最近数据集的局限性和注释人工制品被更快地识别出来。在SNLI中，注释器被证明依赖于启发式，这允许模型在许多情况下仅使用假设做出正确的预测，而在SQuAD上训练的模型受制于对抗性插入的句子。

最近的一个趋势是对抗性数据集的发展，如对抗性NLI（[Nie et al.，2020](https://aclanthology.org/2020.acl-main.441/?ref=ruder.io)），以及其他创建示例对当前模型来说很困难的数据集。[Dynabench](https://dynabench.org/?ref=ruder.io)（[Kiela et al.，2021](https://aclanthology.org/2021.naacl-main.324.pdf?ref=ruder.io)），最近的开源平台旨在促进此类数据集的创建。此类基准的一个好处是它们可以动态更新，以便随着新模型的出现而具有挑战性，因此不会像静态基准那样容易饱和。

## 指标很重要
> “当你能够衡量你所谈论的东西并用数字来表达它时，你就知道你在讨论什么。但是当你不能衡量它并用数字来表达它时，你的知识就非常贫乏和不令人满意。”——开尔文勋爵
>

在衡量绩效方面，指标发挥着重要的作用，但往往被低估。对于分类任务，准确率或F分指标似乎是显而易见的选择，但根据应用程序，不同类型的错误会产生不同的成本。对于细粒度的情感分析，混淆积极和非常积极可能不会有问题，而混淆非常积极和非常消极会有问题。克里斯·波茨强调了一系列实际例子，比如F分等指标不足，其中许多是在错误代价更高的情况下。

设计一个好的指标需要领域专业知识。MLPerf衡量将模型训练到特定于数据集的性能目标所需的挂钟时间，这一衡量标准由两个最终用例提供信息，以及在模型之间比较其他效率指标（如FLOPS）的难度。在自动语音识别中，最初只使用正确转录单词的百分比（类似于准确率）作为指标。社区后来决定错误率，即替换+删减+插入引用字数substitutions+deletions+insertionsnumber of words in reference因为它直接反映了纠正转录错误的成本。

正如Mark Liberman所强调的那样，为长达数十年的研究而设计的度量标准与为实际应用的近期发展而设计的度量标准之间存在很大差异。对于发展十年规模的技术，我们需要高效的度量标准，只要它们指向我们遥远目标的大致方向，这些度量标准就可以是粗糙的。此类度量标准的例子有自动语音识别中的单词错误率（它假设所有单词都同等重要）和机器翻译中的BLEU（它假设词序并不重要）。相比之下，对于实用技术的评估，我们需要在设计时考虑到特定应用的要求，并且可以考虑不同类型的错误类别的度量标准。

近年来，模型性能的快速提升使我们在许多应用中从十年长到短期制度。然而，即使在这个更application-oriented的环境中，我们仍然依赖于迄今为止用于衡量长期研究进展的相同指标。在最近的荟萃分析中，[玛丽等人（2021）](https://aclanthology.org/2021.acl-long.566.pdf?ref=ruder.io)发现，2019-2020年间，82%的机器翻译论文仅基于BLEU进行评估——尽管在过去十年中，已经为MT评估提出了108个替代指标，其中许多指标与人类判断更相关。随着模型变得越来越强大，像BLEU这样的指标不再能够准确地识别和比较表现最佳的模型。

虽然自然语言生成（NLG）模型的评估是出了名的困难，但标准的基于N元重叠的指标，如ROUGE或BLEU，更不适合具有丰富形态学的语言，这些语言将被分配相对较低的分数。

NLG最近的一个趋势是开发自动度量，如BERTScore（[Zhang et al.，2020](https://openreview.net/forum?id=SkeHuCVFDr&ref=ruder.io)），利用大型预训练模型的力量。这种方法最近的修改通过将更大的权重分配给更困难的tokens，即仅由少数MT系统正确翻译的tokens，使其更适合近期MT评估（[詹et al.，2021](https://aclanthology.org/2021.acl-short.5.pdf?ref=ruder.io)）。

为了继续取得进展，我们需要能够更新和完善我们的指标，用application-specific的指标取代高效的简化指标。例如，最近的创业板基准明确将指标作为一个组成部分，应该随着时间的推移而改进，如下所示。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1704869378447-9140b8d1-9952-408a-8555-f50a97ef8485.png)

建议：

1. 考虑更适合下游任务和语言的指标。
2. 考虑突出下游环境权衡的指标。
3. 随着时间的推移更新和完善指标。

## 考虑下游用例
> “[…]基准测试塑造了一个领域，无论好坏。好的基准测试与实际应用保持一致，但坏的基准测试不一致，迫使工程师在做出有助于最终用户的改变和只有助于营销的改变之间做出选择。”——大卫·帕特森
>

自然语言处理技术越来越多地应用于许多现实世界的应用领域，从创造性的自我表达到欺诈检测和推荐。因此，在设计基准时考虑到这种现实世界的设置是关键。

基准测试的数据组成和评估协议应该反映现实世界的用例，正如Ido Dagan所强调的那样。例如，对于关系分类，FewRel数据集缺乏一些重要的现实属性。对于IMDb数据集上的二元情感分类，只考虑高度极化的正面和负面评论，标签是完全平衡的。在信息检索中，在不相关的文档之前检索相关是必要的，但不足以用于现实世界的使用。

作为NLP社会责任的第一条规则，克里斯·波茨提出“完全按照你说的去做”。作为该领域的研究人员，我们应该清楚地传达基准测试的表现反映了什么，以及这与现实环境的对应关系。鲍曼和达尔（2021）以类似的方式认为，基准测试的良好表现应该意味着任务的强大领域内表现。

然而，任务的实际应用可能会使模型面临与其训练分布不同的数据。因此，评估模型的稳健性以及它对这种分布外数据的泛化程度是关键，包括具有时间偏移的数据和来自其他语言变体的数据。

鉴于NLP研究中有限的语言多样性（Joshi et al.，2020），更重要的是不要将英语视为评估的单一语言。在设计基准时，在极小点收集其他语言的测试数据可能有助于突出新的挑战并促进语言包容。同样，在评估模型时，在问题回答和总结等任务中利用越来越多的非英语语言数据集可以提供模型通用性的额外证据。

最终，考虑语言技术当前和未来现实应用的挑战可能会为许多新的评估和基准提供灵感。基准测试是我们领域最有影响力的人工制品之一，通常会带来全新的研究方向，因此对它们来说，反映我们技术的现实世界和潜在雄心勃勃的用例至关重要。

建议：

1. 设计基准测试及其评估，使其反映真实世界的用例。
2. 评估域内和域外泛化。
3. 收集数据并评估其他语言的模型。
4. 从语言技术的实际应用中汲取灵感。

## 细粒度评估
> “不管人们多么希望性能是一个单一的数字，即使是没有分布的正确均值也可能会误导人，错误的均值肯定也好不到哪里去。”-约翰·马西
>

技术的下游用例也应该告知我们用于评估的指标。特别是，对于下游应用程序，通常不是一个单一的指标，而是需要考虑一系列约束。Rada Mihalcea呼吁从仅仅关注准确率转向关注现实世界场景的其他重要方面。在特定环境中什么是重要的，换句话说，NLP系统的效用，最终取决于每个单独用户的需求。

社会需求通常没有在机器学习研究中得到强调。然而，对于现实世界的应用来说，模型不表现出任何有害的社会偏见尤为重要。因此，以特定任务的方式测试这种偏见应该成为算法开发和模型评估的标准部分。

对实际应用很重要的另一个方面是效率。根据应用，这可能与样本效率、FLOPS和内存约束都有关系。在resource-constrained环境中评估模型通常可以带来新的研究方向。例如，NeurIPS 2020上的效率QA竞赛展示了检索增强和大量弱监督问答对的好处。

为了更好地理解我们的模型的优势和劣势，我们还需要跨单个指标进行更细粒度的评估，突出显示模型擅长和不擅长的示例类型。[ExplainaBoard](http://explainaboard.nlpedia.ai/?ref=ruder.io)实现了跨不同任务的模型性能细粒度细分，如下所示。获得模型性能更细粒度估计的另一种方法是为特定现象和模型行为创建测试用例，例如使用CheckList框架。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1704869379114-76a9484b-2aac-4dbb-b5e6-7096f15df3f7.png)

由于单个指标可能存在缺陷，因此跨多个指标进行评估是关键。当对多个指标进行评估时，通常会对分数进行平均以获得单个分数。单个分数有助于一目了然地比较模型，并为社区以外的人提供了评估模型性能的清晰方法。然而，使用算术平均值并不适合所有目的。SPEC使用几何平均值，这在聚合本质上是指数值（例如运行时）时很有用。

另一种选择是使用加权和，并使用户能够为每个组件定义自己的权重。[DynaBench](https://dynabench.org/?ref=ruder.io)使用这样的动态加权来加权模型性能的重要性，但也考虑模型吞吐量、内存消耗、公平性和稳健性，这使用户能够有效地定义自己的排行榜，如下所示。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1704869379312-29eae59b-92ee-4c59-bedf-96f3d5132be5.png)

建议：

1. 不再使用单一指标进行性能评估。
2. 评估社会偏见和效率。
3. 对模型执行细粒度的评估。
4. 考虑如何聚合多个指标。

## 基准性能的长尾
鉴于当前模型在分布内示例上表现出奇的好，现在是时候将我们的注意力转移到分布的尾部，异常值和非典型示例上了。与其只考虑平均情况，我们应该更关心最坏情况和模型表现最差的数据子集。

随着模型变得越来越强大，模型性能不同从而能够区分强模型和最佳模型的示例比例将会越来越小。为了确保对这一长尾示例的评估是可靠的，基准需要足够大，以便可以检测到性能的微小差异。重要的是要注意，较大的模型并不是在所有示例中都一致更好。

作为替代方案，我们可以开发一些机制，让我们能够在很少的例子中识别出最好的系统。这在评估许多系统性能成本高昂的环境中尤其重要，例如人类对自然语言生成的评估。[Mendonça et al.（2021）](https://aclanthology.org/2021.acl-long.242/?ref=ruder.io)将其视为MT背景下的在线学习问题。

基准还可以专注于注释更具挑战性的示例。这是最近的对抗性基准所采取的方向。这样的基准，只要不偏向于特定的模型，就可以成为从自然分布中采样的常规基准的有用补充。这些方向受益于主动评估方法的发展，以识别或生成最突出和最有判别力的示例，以评估模型性能以及可解释性方法，以允许注释者更好地理解模型的决策边界。

由于基准的预算（以及规模）通常保持不变，统计显著性测试变得更加重要，因为它使我们能够可靠地检测系统之间定性相关的性能差异。

为了进行可靠的比较，基准的注释应该是正确和可靠的。然而，随着模型变得越来越强大，许多看起来像模型错误的实例可能是数据中模棱两可的真实例子。[鲍曼和达尔（2021）](https://aclanthology.org/2021.naacl-main.385.pdf?ref=ruder.io)强调模型如何利用这些分歧的线索在基准上达到超人的表现。

如果可能，基准应该旨在收集多个注释以识别模棱两可的示例。这些信息可能提供有用的学习信号，并且有助于错误分析。鉴于这种模棱两可，报告标准指标（如注释者之间的一致性）就更加重要了，因为这为模型在基准上的性能提供了上限。

建议：

1. 在基准测试中包括许多和/或困难样本。
2. 进行统计显著性检验。
3. 为不明确的示例收集多个注释。
4. 报告注释者之间的协议。

## 大规模连续评估
> "当一项措施成为目标时，它就不再是一项好措施."古德哈特定律
>

GLUE等多任务基准已经成为进展的关键指标，但这种静态基准集合很快就会过时。建模进步通常也不会导致跨任务的统一进展。虽然模型在大多数GLUE任务上取得了超人的性能，但在CoLA等一些任务上仍存在与5人一致的差距。在XTREME上，模型在跨语言检索方面的改进要大得多。

鉴于模型改进的快速步伐，我们需要更灵活的模型评估机制。具体来说，除了DynaBench等动态单任务评估之外，定义模型尚未达到人类性能的基准数据集的动态集合将是有用的。该集合应由社区管理，随着模型达到人类性能，数据集将被删除或降低权重，并定期添加新的具有挑战性的数据集。这种集合需要版本化，以实现学术审查周期之外的更新，并实现与先前方法的可复制性和比较。

现有的多任务基准测试，如[GEM](https://gem-benchmark.com/?ref=ruder.io)，明确旨在成为一个“活的”基准测试，通常包括大约10-15个不同的任务。鉴于不断发布的新数据集的数量，与其将基准测试限制在一小部分有代表性的任务中，不如包括更大的NLP任务横截面可能更有用。鉴于NLP中任务的多样性，这将提供更强大和最新的模型性能评估。百度的[LUGE](https://www.luge.ai/?ref=ruder.io)是朝着如此大的中文自然语言处理任务集合迈出的一步，目前由28个数据集组成。

任务集合可以以各种方式分解，提供对模型能力更细粒度的评估。如果任务或任务数据子集根据它们正在测试的行为进行分类，这种分解可能特别有见地。[BIG-Bench](https://github.com/google/BIG-bench?ref=ruder.io)是最近的语言模型探测协作基准，包括按关键字分类。

这种大规模多任务评估的一个关键挑战是可访问性。任务需要以通用输入格式可用，以便它们可以轻松运行。此外，任务应该高效运行，或者即使没有太多计算，基础设施也需要可用于运行任务。

另一个需要考虑的问题是，这样的集合有利于大型通用模型，这些模型通常由财力雄厚的公司或机构训练。然而，这种模型已经被用作大多数当前研究工作的起点，一旦训练好，可以通过微调、蒸馏或剪枝更有效地使用。

建议：

1. 考虑收集和评估大量、多样化、版本化的NLP任务集合。

## 结论
为了跟上建模的进步，我们需要重新审视许多被默认的基准测试实践，例如依赖F1分数和BLEU等简单的指标。为此，我们应该从语言技术的实际应用中获得灵感，并考虑这些设置对我们的模型提出的限制和要求。我们还应该更加关心分布的长尾，因为这是许多应用将观察到改进的地方。最后，我们应该更加严格地评估我们的模型，依赖多种指标和统计显著性测试，这与当前趋势相反。



原文：[https://www.ruder.io/nlp-benchmarking/](https://www.ruder.io/nlp-benchmarking/)


> 原文：[https://lilianweng.github.io/posts/2024-02-05-human-data-quality/](https://lilianweng.github.io/posts/2024-02-05-human-data-quality/)
>

<font style="color:rgb(31, 31, 31);">对于现代深度学习模型训练来说，高质量数据是至关重要的燃料。任务特定的标签数据大多来自于人工标注，例如分类任务，或者 LLM 对齐训练中可以转换为分类格式的 RLHF 标注。文章中提到的许多机器学习技术都有助于提高数据质量，但从根本上说，人类的数据收集需要注重细节并仔细执行。业界普遍认识到高质量数据的重要性，但不知不觉中存在着一种微妙的印象：“所有人都想做模型方面的工作，而不愿去做数据方面的工作”（Sambasivan等人，2021 年）。</font>

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1711093316445-6c4a71bf-b281-486e-8ae5-dc9eb6dffffb.png)

## 人工标注者 ↔ 数据质量
人工数据收集包含一系列操作步骤，每个步骤都会影响数据质量：

+  **任务设计：** 设计清晰易懂的任务流程，降低复杂性。详细的指导方针固然有用，但过于冗长复杂的指南反而需要大量培训才能发挥作用。 
+  **标注者选拔和培训：** 选择技能匹配、标注结果一致的标注者。培训环节必不可少，培训完成后还需要定期提供反馈并进行校准环节。 
+  **数据收集和汇总：** 这一阶段可以引入更多机器学习技术，用于清理、过滤和智能整合数据，以识别真正的标签。 

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1711093349616-be29b0c6-3af8-4a0f-b4fd-26b5518efd4f.png)

## <font style="color:rgb(31, 31, 31);">民声的力量：众智的力量</font>
<font style="color:rgb(31, 31, 31);">拉丁语谚语“Vox populi, vox Dei”（意为“民声即天意”）体现了人民声音的重要性。1907 年，一篇名为《民声》的短篇论文发表于《自然》杂志。该论文记录了每年展览会上的一项活动，人们猜测一头肥牛的重量并有机会赢得奖品。论文将所有猜测值的中位数视为“民声”，结果发现该数值非常接近牛的真实重量。作者总结道：“我认为这个结果比人们预想的更能证明民主判断的可信度。” 这可能是关于“众包”（即“集体智慧”）如何发挥作用的最早论述。</font>

<font style="color:rgb(31, 31, 31);">将近 100 年后，Callison-Burch (2009) 开展了一项早期研究，利用亚马逊土耳其机器人平台 (AMT) 进行非专家的人工评估，用于机器翻译 (MT) 任务，甚至让非专家创建新的参考译文。人工评估的设置很简单：每个参与者会看到一个源句子、一个参考译文和来自 5 个机器翻译系统的 5 个译文。他们需要将 5 个译文从优到劣进行排序。每个任务由 5 个参与者完成。</font>

<font style="color:rgb(31, 31, 31);">毫无疑问，一些人会为了产出数量而提供低质量的标注，充当“刷子”。因此，在衡量专家和非专家之间的一致性时，需要应用不同的加权方案来降低“刷子”的影响：(1) “专家加权”：使用他们在 10 个黄金示例上的与专家的同意率；(2) “非专家加权”：依靠他们在整个数据集上与其他参与者的同意率。</font>

<font style="color:rgb(31, 31, 31);">在更难的任务中，非专家人工标注者被要求创建新的参考译文。Callison-Burch 将任务设计为两个阶段，第一阶段参考机器翻译输出创建新译文，第二阶段则过滤掉看起来像是机器翻译生成的译文。最终，众包翻译与专家翻译的关联性高于专家与机器翻译输出的关联性。</font>

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1711093416478-e94d1e29-5e18-44a2-8c3d-7ef6ef5c3d2d.png)

上图展示了 Callison-Burch (2009) 研究中用于评估一致性得分的衡量方法。（左图）该部分强调了由于一致性是通过比较每一对翻译 (“A > B”， “A = B”， “A < B”) 来判断的，因此偶然一致率为 1/3 (33%)。 可达到的最高一致性得分由人类专家的评审一致性得分设定上限。（右图） 该部分展示了不同来源译文的 BLEU 得分对比 (BLEU 得分是一种用于评估机器翻译质量的指标)。 由语言数据联盟 (LCD) 提供的专家译文作为基准。 (图像来源：Callison-Burch 2009)

## <font style="color:rgb(31, 31, 31);">标注者一致性</font>
<font style="color:rgb(31, 31, 31);">通常，我们认为注释的目标是单个真实值，并尝试根据一个金标准答案和一致的标准来评估质量。一种常见用于查找可靠真实标签的方法是从多个标注者那里收集多个标签。假设每个标注者的质量水平都不同，我们可以使用注释的加权平均值，但权重由熟练程度得分确定。此得分通常近似于一个标注者与其他标注者的一致性程度。</font>

<font style="color:rgb(31, 31, 31);">常见的一致性评分方法包括：</font>

+ <font style="color:rgb(31, 31, 31);">多数表决：采用多数表决是最简单的聚合方式，相当于取一组标签的众数 (mode)。在这种情况下，每个标注者贡献都相同。</font>
+ <font style="color:rgb(31, 31, 31);">原始一致性 (Tratz & Hovy, 2010): 原始一致性计算与他们一致的其他人的百分比。这与多数表决间接相关，因为大多数类的所有成员预计都会获得较高的标注者间一致性得分。</font>
+ 柯恩Kappa系数 (Landis & Koch, 1977): 该系数用于衡量标注者间一致性，公式为：κ = (p(o) - p(e)) / (1 - p(e))，其中 p(o) 为原始一致性得分，p(e) 为偶然一致性得分。柯恩Kappa系数加入了修正偶然一致性的项，但如果某个标签出现频率过高，则该修正项可能会被高估。
+ <font style="color:rgb(31, 31, 31);">概率图模型：概率图模型是一种用于建模注释决策中不同因素的框架，例如任务难易程度、任务潜在主题、标注者偏差、标注者置信度等，然后由此预测真实标签。Zheng et al. (2017) 比较了用于众包任务真实值推断的 17 种算法，其中大部分都属于概率图模型。</font>
    - <font style="color:rgb(31, 31, 31);">MACE（多标注者能力估计；Hovy et al. 2013）是利用图模型估计某人充当“刷子”可能性的一个早期例子。“刷子”通过提供随机标签来优化完成任务的数量以获得更高的报酬。在激励措施错位的情况下，一些标注者可能会表现为“刷子”，这不足为奇。MACE 的目标是识别“刷子”。</font>

## <font style="color:rgb(31, 31, 31);">标注者分歧与两种范式</font>
<font style="color:rgb(31, 31, 31);">上面描述的聚合过程依赖于这样一个假设：存在唯一的“金标准答案”，因此我们可以据此评估标注者表现。然而，在许多领域，尤其是涉及安全、社会或文化议题时，人们可能会产生分歧，这种分歧通常是合理的。此时，关键在于我们更倾向于严格遵循规则还是拥抱多样性。</font>

<font style="color:rgb(31, 31, 31);">Aroyo & Welty (2015) 讨论了人类标注收集实践中的一系列“误区”，并发现这些误区都有些不准确。他们的核心发现包括：</font>

+ <font style="color:rgb(31, 31, 31);">一些样本往往存在多种正确解读。我们需要通过例如让多人评估标注质量来获得多元视角。</font>
+ <font style="color:rgb(31, 31, 31);">分歧并不总是坏事。我们应该减少因错误或设计不当的流程导致的分歧，但其他分歧可以为我们提供丰富的信息。</font>
+ <font style="color:rgb(31, 31, 31);">如果分歧源于任务定义不清，则应改进说明。然而，更详细的指南并不能解决意见与生俱来的多样性。</font>
+ <font style="color:rgb(31, 31, 31);">专家并不总是比普通人更好，但在考虑重要因素方面可能会存在很大差距。</font>
+ <font style="color:rgb(31, 31, 31);">“金标准”标注可能会随着时间的推移而改变，尤其是在涉及时事或新闻的领域。</font>

<font style="color:rgb(31, 31, 31);">稍后，Rottger et al. (2021) 将这种差异归纳为两种用于主观自然语言处理任务的数据标注范式。</font>

<font style="color:rgb(31, 31, 31);">描述性和规范性标注范式是用于主观自然语言处理任务数据标注的两种截然不同的方法。</font>

**<font style="color:rgb(31, 31, 31);">描述性范式</font>**

+ **<font style="color:rgb(31, 31, 31);">定义：</font>**<font style="color:rgb(31, 31, 31);"> 鼓励标注者主观性，尝试对多种观点进行建模。</font>
+ **<font style="color:rgb(31, 31, 31);">优点：</font>**
    - <font style="color:rgb(31, 31, 31);">有助于识别条目中主观性强弱；</font>
    - <font style="color:rgb(31, 31, 31);">拥抱多样性；</font>
    - <font style="color:rgb(31, 31, 31);">更符合标准的 NLP 设置。</font>
+ **<font style="color:rgb(31, 31, 31);">缺点：</font>**
    - <font style="color:rgb(31, 31, 31);">无法使用标注者分歧等指标来衡量数据质量或标注者表现；</font>
    - <font style="color:rgb(31, 31, 31);">不适用于训练针对单一预设行为输出的模型。</font>

**<font style="color:rgb(31, 31, 31);">规范性范式</font>**

+ **<font style="color:rgb(31, 31, 31);">定义：</font>**<font style="color:rgb(31, 31, 31);"> 抑制标注者主观性，尝试一致地应用一种观点。</font>
+ **<font style="color:rgb(31, 31, 31);">优点：</font>**
    - <font style="color:rgb(31, 31, 31);">质量控制更容易进行，可以通过测量分歧或进行标签聚合来实现；</font>
    - <font style="color:rgb(31, 31, 31);">更符合标准的 NLP 设置。</font>
+ **<font style="color:rgb(31, 31, 31);">缺点：</font>**
    - <font style="color:rgb(31, 31, 31);">创建高质量的标注指南昂贵且具有挑战性，实际上这些指南永远不会完美；</font>
    - <font style="color:rgb(31, 31, 31);">训练标注者熟悉并正确应用指南也具有挑战性；</font>
    - <font style="color:rgb(31, 31, 31);">无法捕捉可解释的观点多样性，也无法一致地编码一种特定观点。</font>

<font style="color:rgb(31, 31, 31);">描述性范式的优势：理解效应并纳入不同视角。描述性范式使我们能够理解许多重要效应并纳入不同的视角。例如，Goyal等人 (2022) 研究发现，标注者身份（例如非裔美国人、LGBTQ）是他们如何将身份相关内容标记为有害内容的一个统计显着因素。议题本身也可以是导致观点差异的主要驱动因素。</font>

<font style="color:rgb(31, 31, 31);">Wang等人 (2023) 研究了人类评估人工智能对话系统安全性的过程，并比较了信任与安全 (T&S) 专业人员和众包标注者的标签结果。他们特意收集了与众包标注者相关的丰富元数据，例如人口统计信息或行为信息。通过比较 T&S 专家标签和众包标注，他们发现一致性得分会因语义主题和严重程度而异：</font>

+ <font style="color:rgb(31, 31, 31);">不同主题间的一致性得分差异很大，从暴力/血腥内容的 0.96 到个人话题的 0.25 不等。</font>
+ <font style="color:rgb(31, 31, 31);">在使用“良性”、“可辩论”、“中等”和“极端”四个标签选项的情况下，对于“极端”和“良性”对话，一致性得分更高。</font>

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1711094006975-9642624b-a8af-4093-9500-a6d0d1ac0da0.png)

<font style="color:rgb(31, 31, 31);">张 et al. (2023) 提出了一个标注者分歧的分类法，用于分析分歧的根本原因。在列出的原因中，应该避免因随机错误或个人水平不一致导致的分歧。例如，当一个标注者在多次被要求对相同任务进行标注时给出不同的标签，其中一些很可能是人为错误造成的。基于这一直觉，分歧解卷积方法 (Gordon et al. 2021) 通过将每个人的观点固定在其主要的标注上，从而鼓励标注者内部的一致性，以此将稳定的观点与错误区分开来。</font>

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1711094040917-79679ba1-e042-48ba-a414-db8532b6937a.png)

<font style="color:rgb(31, 31, 31);">陪审团学习 (Jury Learning) 通过模拟陪审团程序，根据标注者的特征对其标注行为进行建模。首先，我们准备了一个包含标签以及每个标注者人口统计特征的数据集，并以此训练模型学习预测每个标注者（作为潜在陪审团成员）生成的标签。在决策时，实践者可以指定陪审团的组成，以确定抽样策略。最终决定是通过汇总来自多次试验的陪审团成员的标签来做出。</font>

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1711094109820-e0d74faf-db1d-4ca6-9787-304852ea09e3.png)

<font style="color:rgb(31, 31, 31);">陪审团学习模型是一种常用于推荐用例的深度 & 交叉网络 (DCN)。该模型通过联合训练学习评论嵌入、标注者嵌入以及组 (标注者特征) 嵌入。文本内容由预训练的 BERT 模型进行处理，该模型也进行联合微调，但为了避免过拟合，微调时间较短。</font>

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1711094122040-17beac89-61ae-4a59-9af0-8ccc38f6e4a1.png)

<font style="color:rgb(31, 31, 31);">他们使用“毒性多样性数据集”进行了实验，并将陪审团学习与仅使用一个微调过的 BERT 模型进行比较，后者不利用元数据来预测单个标注者的标签。性能通过 MAE（平均绝对误差）进行衡量。在整个测试集和每个分组细分中，陪审团学习都始终优于不考虑标注者的基线模型。</font>

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1711094150255-005c7ec7-dd0c-4bcd-855b-ea87f42ad166.png)

## <font style="color:rgb(31, 31, 31);">数据质量 </font><font style="color:rgb(31, 31, 31);">↔</font><font style="color:rgb(31, 31, 31);"> 模型训练</font>
<font style="color:rgb(31, 31, 31);">数据集构建完成后，许多方法可以根据训练动态帮助识别错误标签。需要注意的是，我们仅关注用于查找并排除具有潜在错误标签的数据点的的方法，而不是关于如何使用带有噪声数据训练模型的方法。</font>

### <font style="color:rgb(31, 31, 31);">影响函数</font>
<font style="color:rgb(31, 31, 31);">影响函数是稳健统计学中的一项经典技术 (Hampel, 1974)，用于通过描述模型参数在我们以无限小的量增加训练数据点的权重时如何变化来衡量训练数据点的影响。Koh & Liang (2017) 引入了将该概念应用于深度神经网络的思想。</font>

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1711094205203-39ac4e9e-a2c7-49a5-bedb-b43829bad9ba.png)

<font style="color:rgb(31, 31, 31);">利用影响函数，我们可以使用闭式公式衡量单个数据点对模型参数和损失函数的影响。这可以帮助近似留一法重新训练，而无需实际运行所有重新训练过程。为了识别错误标记的数据，我们可以测量 Δ_up,loss(x_i, w)，该值近似于如果将 x_i 从训练集中删除，其预测误差。</font>

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1711094256428-9f830de7-d71e-4695-a46d-e7a4dfcbfb8c.png)

<font style="color:rgb(31, 31, 31);">尽管影响函数有闭式解，但由于海森矩阵向量乘积的逆很难计算，因此仍然难以扩展到处理更大规模的数据集。Grosse et al. (2023) 尝试使用一种替代方法，即 EK-FAC（特征值校正 Kronecker 分解近似曲率；George et al. 2018）逼近。</font>

### <font style="color:rgb(31, 31, 31);">训练过程中的预测变化</font>
<font style="color:rgb(31, 31, 31);">另一类方法是跟踪训练过程中模型预测的变化，以识别看起来难以学习的案例。“数据地图”（Swayamdipta et al. 2020）在训练过程中跟踪模型行为动态的两个属性，以分析数据集的质量：</font>

+ <font style="color:rgb(31, 31, 31);">置信度：模型对真实标签的置信度，定义为各个 epoch 中模型对真实标签的平均预测概率。他们还使用了一个粗粒度的指标“正确率”，定义为模型在各个 epoch 中预测正确标签的次数的比例。</font>
+ <font style="color:rgb(31, 31, 31);">可变性：置信度的变化，定义为各个 epoch 中模型对真实标签的预测概率的标准差。</font>

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1711094326496-7dc24bd8-403c-40c5-9113-79eef2bd6dd5.png)

<font style="color:rgb(31, 31, 31);">难以学习的样本（置信度低、可变性低）更容易被错误标记。研究人员使用包含 1% 翻转标签数据的 WinoGrande 数据集进行了实验。经过重新训练后，翻转的实例会转移到置信度较低和可变性稍微较高的区域，这表明难以学习的区域包含错误标记的样本。基于此，我们可以使用仅包含置信度分数的相等数量的翻转标签和干净样本来训练分类器（论文中没有同时使用置信度和可变性作为特征的原因尚不清楚）。然后，这个简单的噪声分类器可以用于原始数据集，以识别潜在的错误标记实例。</font>

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1711094380485-21a046ce-d2d6-4faf-a9f0-bf734d7a649f.png)

<font style="color:rgb(31, 31, 31);">然而，我们不应该将所有难以学习的样本都视为错误的。事实上，论文假设具有歧义性（高可变性）和难以学习（低置信度、低可变性）的样本对于学习更具信息量。实验表明它们对 OOD泛化性能良好，即使与 100% 的训练集相比，在 OOD 评估上也能取得更好的结果。</font>

<font style="color:rgb(31, 31, 31);">为了研究神经网络是否倾向于忘记先前学习的信息，Mariya Toneva 等人 (2019 年) 设计了一个实验：他们跟踪模型在训练过程中的每个样本的预测，并计算每个样本从正确分类到错误分类或反之的转换次数。然后可以相应地对样本进行分类，</font>

+ <font style="color:rgb(31, 31, 31);">可遗忘的（冗余的）样本：如果类别标签在训练 epoch 之间发生变化。</font>
+ <font style="color:rgb(31, 31, 31);"> 不可遗忘的样本：如果类别标签分配在训练 epoch 之间保持一致。 这些样本一旦被学习就不会被忘记。</font>

<font style="color:rgb(31, 31, 31);">他们发现，有大量不可遗忘的示例，一旦学会就永远不会被忘记。 具有噪声标签或具有“不常见”特征（视觉上难以分类）的图像是最容易被遗忘的示例之一。 实验通过实验证明，可以安全地删除不可遗忘的示例，而不会影响模型性能。</font>

<font style="color:rgb(31, 31, 31);">在实现过程中，仅当样本包含在当前训练批次中时才会计算遗忘事件; 即，他们计算后续迷你批次中相同示例的展示次数之间的遗忘。每个样本的遗忘事件数量在不同的种子之间相当稳定，可遗忘的示例在训练后期首次学习的趋势很小。 还发现遗忘事件可以在整个训练期间和架构之间转移。</font>

<font style="color:rgb(31, 31, 31);">Pleiss等人 (2020 年) 基于这样一个假设开发了一种名为 AUM（边缘下区域）的方法来识别错误标签：假设一张 BIRD 图像被错误标记为 DOG。梯度更新会鼓励从其他 BIRD 图像到这张 BIRD 图像进行泛化，而 DOG 标签会提供错误的监督信号来鼓励更新走向另一个方向。 因此，梯度更新信号中存在泛化和（错误）预测之间的 tension（张力）。</font>

## <font style="color:rgb(31, 31, 31);">噪声交叉验证 (Noisy Cross-Validation，NCV)</font>
<font style="color:rgb(31, 31, 31);">噪声交叉验证 (NCV) 方法 (Chen et al. 2019) 是用于处理可能包含错误标签的数据集的一种技术。其工作原理如下：</font>

1. **<font style="color:rgb(31, 31, 31);">划分数据集:</font>**<font style="color:rgb(31, 31, 31);"> 将数据集随机分成两半。</font>
2. **<font style="color:rgb(31, 31, 31);">训练与预测:</font>**<font style="color:rgb(31, 31, 31);"> 仅使用一半数据集中的数据训练模型。然后，使用该模型来预测另一半数据中的数据点的标签。</font>
3. **<font style="color:rgb(31, 31, 31);">识别干净样例:</font>**<font style="color:rgb(31, 31, 31);"> 如果第二个一半中的数据点的原始标签与其在第一个一半上训练的模型的预测标签匹配，则认为该数据点是 "干净的"。假设干净的样例即使在使用数据集的不同部分训练时也能被模型正确分类。</font>

**<font style="color:rgb(31, 31, 31);">迭代噪声交叉验证 (Iterative Noisy Cross-Validation，INCV)</font>**<font style="color:rgb(31, 31, 31);"> 进一步扩展了这一概念。它迭代地执行 NCV：</font>

+ <font style="color:rgb(31, 31, 31);">在每次迭代中识别为干净的样本都会添加到可信候选者集合中。</font>
+ <font style="color:rgb(31, 31, 31);">相反，会导致误判的样本被认为是噪声并从进一步的考虑中删除。</font>

<font style="color:rgb(31, 31, 31);">这个过程通过识别并删除可能错误标记的数据点来帮助细化数据集。</font>

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1711094515971-29498eb0-aa2f-40ff-af68-6934d224318b.png)


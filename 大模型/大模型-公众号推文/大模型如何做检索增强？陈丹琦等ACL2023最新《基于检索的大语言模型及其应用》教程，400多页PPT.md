# <font style="color:rgba(0, 0, 0, 0.9);">ACL2023陈丹琦等《基于检索的大语言模型及其应用》教程，附全部PPT下载</font>
<font style="color:rgb(55, 65, 81);background-color:rgb(247, 247, 248);">语言模型（LMs）如GPT-3和PaLM在各种自然语言处理（NLP）任务中展现了令人印象深刻的能力。然而，仅依靠它们的参数来编码丰富的世界知识需要庞大的参数数量和大规模计算，并且它们经常难以学习到长期记忆的知识。此外，这些参数化的LMs从根本上无法随着时间的推移进行适应，它们经常会产生幻觉，并可能泄露训练语料库中的私人数据</font>，其幻觉问题阻碍了其在特定场景的应用，例如医学、法律、金融等。

<font style="color:rgb(55, 65, 81);background-color:rgb(247, 247, 248);">大模型的幻觉问题一方面由模型本身的能力的导致，另一个很重要的方面在于知识的不确定性。例如“美国总统是谁？”，在不同的时间同一个问题具有不同的答案，而模型学习到的知识一般都是固定的。</font>

<font style="color:rgb(55, 65, 81);background-color:rgb(247, 247, 248);">模型编辑（Modeling Editing）是一种解决模型知识更新的热门研究方向，通过特定的手段改变模型的参数，使其能够适应变化的知识，但目前效果上还不太稳定，应用场景非常受限。</font>

<font style="color:rgb(55, 65, 81);background-color:rgb(247, 247, 248);">为了克服这些限制，人们越来越关注于基于检索的LMs，这些模型将非参数化的数据存储库（例如来自外部语料库的文本片段）与它们的参数化对应物结合起来。基于检索的LMs可以比没有检索的LMs在性能上大幅超越，并且需要更少的参数，可以通过替换检索语料库来更新知识，并为用户提供引用，以便轻松验证和评估预测结果。</font>

为此ACL2023进行了“基于检索的大语言模型及其应用”的专题讲座。详情内容可见官网，附件中为讲座PPT。

![](https://cdn.nlark.com/yuque/0/2023/png/406504/1689064893357-0d118f7a-62a1-4e8d-8ec7-1575c803201c.png)

网址：[https://acl2023-retrieval-lm.github.io/](https://acl2023-retrieval-lm.github.io/)


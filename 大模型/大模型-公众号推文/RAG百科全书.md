## 概览
+ 检索增强生成（RAG）是一种通过结合外部知识来增强语言模型生成的技术。
+ 这通常是通过从大型文档语料库中检索相关信息并使用该信息来指导生成过程实现的。
+ 让我们深入研究下面的细节。

## 动机
+ 在许多情况下，客户拥有大量专有文档，例如技术手册，需要从这些海量内容中提取特定信息。这项任务就像大海捞针一样困难。
+ 最近，OpenAI 推出了一种名为 GPT-4 Turbo 的新型模型，该模型号称能够处理大型文档，有可能解决这方面的需求。然而，由于“迷失中央 (Lost In The The Middle)”现象的存在，该模型的效率并不是很高。这种现象类似于我们阅读整本圣经，却记不起撒母耳记之后的内容，该模型也倾向于忘记其上下文窗口中间部分的内容。
+ 为了克服这一限制，人们开发了另一种称为检索增强生成 (Retrieval-Augmented-Generation, RAG) 的方法。这种方法涉及为文档中的每个段落创建一个索引。当用户提出查询时，系统会迅速识别最相关的段落，然后将它们输入到大型语言模型 (LLM) 例如 GPT-4 中。这种仅提供选定段落而不是整个文档的策略可以防止大型语言模型信息过载，并显着提高结果的质量。

## 神经检索
+ 在深入探讨检索增强生成 (RAG) 模型之前，让我们花点时间整体了解神经检索技术。
+ 神经检索是一种利用神经网络将查询与相关文档匹配的信息检索模型。它会将查询和文档编码成稠密向量的表示形式，并计算它们之间的相似度得分。这使神经检索能够超越简单的词法匹配，而是可以捕捉语义上的相关性。
+ 神经检索代表着信息检索领域的一次重大转变，从传统的基于关键词的检索系统转向能够理解文本数据中潜在含义和关系的系统。下面我们将详细解释神经检索的工作原理及其重要性：
+ 神经检索通常的工作流程如下：
    1. 向量编码：
        * 查询和文档都将被转换为高维空间中的向量。这个过程由神经网络编码器完成，这些编码器经过训练可以捕捉文本的语义本质。
        * 在训练过程中，这些模型通常会接触到大量文本，从而学习词语和短语之间复杂的模式和关系。
    2. 语义匹配：
        * 使用诸如余弦相似度等度量来计算查询向量和文档向量之间的相似度。这可以让系统根据内容的含义而不是仅仅关键词的重叠来判断哪些文档与查询最相关。
        * 这个过程可以捕捉到细微的关系，例如传统方法可能错过的同义词或相关概念。
+ 神经检索的优势：
    - 神经检索可以理解术语的使用上下文，从而在查询或文档具有歧义或多重含义时进行更准确的检索。
    - 它们擅长处理长而复杂的查询，因为它们可以理解整体的意图，而不是仅仅孤立的术语。
    - 许多神经检索模型都使用多语言数据集进行训练，使它们能够有效地处理不同语言的查询。
+ 挑战和注意事项：
    - 神经网络模型，特别是用于编码大型文档的模型，无论是训练还是推理都需要巨大的计算能力。
    - 神经检索的性能在很大程度上取决于它们所训练的数据，并且它们可能会继承训练数据中存在的偏差。
    - 保持文档表示的最新状态是一个挑战，尤其对于动态变化的内容而言。

## 检索增强生成 (RAG) 流程
+ RAG 允许大型语言模型 (LLM) 利用外部知识源（例如数据库）中的知识和信息，而这些信息不一定包含在 LLM 的权重参数中。
+ RAG 利用检索器来找到相关的上下文以调整 LLM 的条件。通过这种方式，RAG 可以利用相关文档来增强 LLM 的知识库。
+ 这里的检索器可以是以下几种类型之一，具体取决于是否需要语义检索：
    - **向量数据库：** 通常使用诸如 BERT 等模型将查询嵌入为稠密向量表示。或者，可以使用 TF-IDF 等传统方法进行稀疏嵌入。然后根据词频或语义相似性进行搜索。
    - **图数据库：** 从文本中提取实体关系来构建知识库。这种方法很精确，但可能需要完全匹配查询，这在某些应用程序中可能会过于严格。
    - **既有数据库 (Regular SQL database)**： 提供结构化数据存储和检索，但可能缺乏向量数据库的语义灵活性。
+ 下面来自 Damien Benveniste 博士的图片讨论了 RAG 使用图数据库与矢量数据库之间的区别。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1714026079116-0f96aa52-8f4b-4423-827b-d4dc94b61e7a.png)

+ 在Damien Benveniste的博文中，他指出与向量数据库相比，图数据库更适用于检索增强生成 (RAG) 任务。
+ 向量数据库使用大型语言模型 (LLM) 编码的向量对数据进行划分和索引，允许检索语义上相似的向量，但可能会获取到不相关的信息。
+ 另一方面，图数据库会从文本中提取实体关系来构建知识库，从而使检索更加准确简洁。但是，这种方式需要完全匹配查询，这可能会带来限制。
+ 一种可能的解决方案是结合两种数据库的优势：在图数据库中使用向量表示来索引解析的实体关系，从而实现更灵活的信息检索。目前尚不清楚是否存在这样的混合模型。
+ 检索之后，可能还需要进一步过滤候选结果，可以通过添加排序和/或精细排序层，来剔除不符合业务规则、未针对用户个性化、与当前上下文无关或超出响应长度限制的候选结果。
+ 让我们简洁地总结一下 RAG 的流程，然后再深入探讨其优缺点：
    1. **构建向量数据库**：RAG 首先将内部数据集转换为向量并存储在向量数据库（或您选择的数据库）中。
    2. **用户输入**：用户以自然语言形式输入查询，寻求答案或内容完成。
    3. **信息检索**：检索机制扫描向量数据库以识别与用户查询（也已嵌入为向量）语义相似的片段。然后将这些片段提供给大型语言模型 (LLM) 以丰富其生成响应的上下文。
    4. **数据组合**：从数据库中选择的數據片段与用户初始查询结合起来，创建一个扩展的提示符。
    5. **生成文本**：然后将包含附加上下文的扩展提示符提供给 大型语言模型 (LLM)，由其创建最终的、具有上下文感知的响应。
+ [下图](https://vectara.com/retrieval-augmented-generation-everything-you-need-to-know/)展示了 RAG 的工作原理概览。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1714026244378-a150f4b5-530d-429d-b1b5-d7a3dd207aff.png)

## RAG 的优点
+ 那么为什么应该在应用程序中使用 RAG 呢？
    - RAG 通过为大型语言模型 (LLM) 提供访问外部知识库的权限，使其能够利用权重参数中可能没有的知识和信息。
    - RAG 无需重新训练模型，从而节省了时间和计算资源。
    - 即使标记数据有限，它仍然有效。
    - 然而，RAG 也存在缺点，其性能取决于检索器知识库的全面性和准确性。
    - RAG 最适合数据丰富但标记数据稀少的场景，并且非常适合像虚拟助手这样需要实时访问特定信息（例如产品手册）的应用程序。
        * **数据丰富但标记数据稀少的场景**：RAG 在有大量可用数据但大部分数据未分类或标记为可用于训练模型的格式时很有用。例如，互联网上拥有大量文本，但其中大部分 אינם (bù yín) 以直接回答特定问题的方式组织。
        * 此外，RAG 非常适合像虚拟助手这样的应用程序：像 Siri 或 Alexa 这样的虚拟助手需要从各种来源获取信息来实时回答问题。它们需要理解问题、检索相关信息，然后生成连贯准确的回复。
        * **需要实时访问特定信息（例如产品手册）**：这是一个 RAG 模型特别有用的场景。想象一下，您向虚拟助手询问有关产品的特定问题，例如“如何重置我的 XYZ 品牌恒温器？” RAG 模型会首先从产品手册或其他资源中检索相关信息，然后使用这些信息生成清晰简洁的答案。
        * RAG 模型非常适合信息丰富但未经过整理或标记的应用程序。
+ 下面，让我们来看看介绍 RAG 的论文以及论文最初是如何实现该框架的。

## RAG vs. Fine-tuning
+ 下表比较了 RAG 与微调。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1714026476751-9a2c59da-0d4c-4d04-92ce-67470fdb8f4b.png)

+ 总结表格内容如下：
    1. 检索增强生成 (RAG) 系统可以为大型语言模型 (LLM) 系统提供事实性、可控访问和实时的信息。微调 (Fine-tuning) 无法做到这一点，因此两者并不冲突。
    2. 微调 (而非 RAG) 则可以调整大型语言模型的风格、语气和词汇，使语言风格能够匹配所在的领域。
    3. 优先关注 RAG。成功的 LLM 应用必须将专业数据连接到 LLM 工作流程中。一旦拥有完整的应用程序，则可以添加微调来改善系统的风格和词汇。如果 RAG 与数据的连接构建不当，微调将无法挽救局面。

## RAG 集成
+ 利用 RAG 系统的集成可以大幅提升模型生成丰富且符合上下文准确性的文本能力。下面是如何运作的更详细解释：
    - **知识来源**：RAG 模型从外部知识库中检索信息来增强其在特定领域的知识。这些知识库可以包含来自维基百科、书籍、新闻、数据库等领域的段落、表格、图像等信息。
    - **来源组合**：在推理阶段，多个检索器可以从不同的语料库中提取相关内容。例如，一个检索器搜索维基百科，另一个检索器搜索新闻源。它们的检索结果会合并成一个候选池。
    - **排序**：模型根据候选内容与上下文的相关性对其进行排序。
    - **选择**：选择排名高的候选内容作为条件，为语言模型生成文本做准备。
    - **集成**：针对不同语料库进行专门训练的 RAG 模型可以进行集成。然后合并它们的输出，并对其进行排序和投票表决。
+ **通过池化和集成**，多种知识来源可以增强 RAG 模型。仔细的排序和选择有助于整合这些不同的来源，从而改善生成结果。
+ **在使用多个检索器时，**需要注意在合并它们以形成响应之前对来自每个检索器不同的输出进行排序。这可以通过多种方式实现，例如使用 LTR 算法、多臂老虎机框架、多目标优化，或根据特定的业务用例进行排序。

## 选择向量数据库：特征矩阵帮你做决定
+ 为了比较众多向量数据库产品，需要借助一个特性矩阵来突出不同向量数据库之间的差异，并指导在不同场景下选择合适的数据库。
+ [VectorHub](https://vdbs.superlinked.com/) 提供的 向量数据库对比 是一个很好的资源，它涵盖了 37 家厂商和 29 项特性（截至本文撰写时）。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1714026686027-fb60f455-3fea-488c-a57b-77c0740a8c36.png)

+ 作为辅助资源，下表 ([链接](https://docs.google.com/spreadsheets/d/170HErOyOkLDjQfy3TJ6a3XXXM1rHvw_779Sit-KT7uc/edit#gid=0)) 展示了一些主流向量数据库在不同特性维度上的对比：

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1714026721383-9ec004c6-9890-46c8-96b7-722071d96f8d.png)

+ 完整电子表格请[点击此处访问](https://docs.google.com/spreadsheets/d/170HErOyOkLDjQfy3TJ6a3XXXM1rHvw_779Sit-KT7uc/edit#gid=0)。

## 构建检索增强生成 (RAG) 管道
+ 下图直观展示了 RAG 的三个步骤：数据摄取、检索和综合/生成响应。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1714026822113-8c3072f2-9dd3-4e50-9e54-ec2def257874.png)

以下部分我们将详细介绍这些关键步骤。

## Ingestion
### 分块
+ 分块是指将用于提示和检索的文档分割成更小、更易管理的片段或块。这些块的定义可以是固定大小，例如特定字符数、句子或段落数。
+ 在 RAG 中，每个分块都将被编码成嵌入向量以便检索。更小、更精确的块可以实现用户查询内容之间更精细的匹配，从而提高检索信息的准确性和相关性。
+ 较大的块可能包含无关信息，从而引入噪音并可能降低检索精度。通过控制块大小，RAG 可以平衡综合性和精确性。
+ 因此，接下来可能会自然而然地产生疑问，如何为您的用例选择合适的块大小？ RAG 中的块大小选择至关重要。它既要足够小以确保相关性并减少噪音，又足够大以保持上下文的完整性。下面让我们看看引用自 Pinecone 的几种方法：
    - **固定大小分块**： 只需决定块中标记的數量以及它们之间是否有重叠。块之间的重叠可以确保块之间语义上下文损失最小。这种方法计算成本低且易于实现。

```python
text = "..." # your text
from langchain.text_splitter import CharacterTextSplitter
text_splitter = CharacterTextSplitter(
    separator = "\n\n",
    chunk_size = 256,
    chunk_overlap  = 20
)
docs = text_splitter.create_documents([text])
```

    - **上下文感知切分**：上下文感知切分利用文本的内在结构来创建更具意义和上下文相关性的文本块。实现此目标的方法有多种：
        1. **句子拆分**：这种方法适用于针对句子级内容进行嵌入优化的模型。句子拆分可以使用不同的工具和技术：
            + **朴素拆分**： 这是一种基本的方法，使用句点和换行符来拆分句子。例如：

```plain
text = "..."  # 文本内容
句子 = text.split(".")
```

            + **NLTK (自然语言工具包)**： NLTK 是一个用于语言处理的综合性 Python 库。NLTK 包含一个句子划分器，可以有效地将文本拆分成句子。例如：

```plain
text = "..."  # 文本内容
from langchain.text_splitter import NLTKTextSplitter
分词器 = NLTKTextSplitter()
句子 = 分词器.split_text(text)
```

            + **spaCy**： spaCy 是用于自然语言处理任务的高级 Python 库，它提供高效的句子分割功能。例如：

```plain
text = "..."  # 文本内容
from langchain.text_splitter import SpacyTextSplitter
分词器 = SpacyTextSplitter()
句子 = 分词器.split_text(text)
```

        2. **递归切分**：递归切分是一种迭代的方法，它使用各种分隔符以分层方式拆分文本。它通过递归应用不同的标准来适应创建大小或结构类似的文本块。LangChain 的示例：

```plain
text = "..."  # 文本内容
from langchain.text_splitter import RecursiveCharacterTextSplitter
分词器 = RecursiveCharacterTextSplitter(
    chunk_size = 256,  # 文本块大小
    chunk_overlap = 20   # 文本块重叠部分
)
文档 = 分词器.create_documents([text])
```

        3. **专业格式切分**：对于 Markdown 或 LaTeX 等格式化内容，可以应用专业格式切分来保持原始结构：
            + **Markdown 切分**： 识别 Markdown 语法并根据结构划分内容。例如：

```plain
from langchain.text_splitter import MarkdownTextSplitter
markdown_text = "..."  # Markdown 文本内容
分词器 = MarkdownTextSplitter(chunk_size=100, chunk_overlap=0)
文档 = 分词器.create_documents([markdown_text])
```

            + **LaTeX 切分**： 解析 LaTeX 命令和环境，在切分内容的同时保留其逻辑组织。
+ “经验法则是，如果文本块对于人类来说即使没有周围的上下文也说得通，那么对于语言模型来说也是一样的。因此，找到语料库中文档的最佳切分大小对于确保搜索结果的准确性和相关性至关重要。”

### Embeddings
+ 在正确分块之后，下一步是进行嵌入 (embedding)。在 RAG 中，嵌入是指将用户查询 (提示符) 和知识库中的文档转换为一种可以有效比较相关性的格式。这个过程对于 RAG 根据用户查询从其知识库中检索最相关的信息至关重要。以下是嵌入通常的工作方式：
+ 选择最适合您的任务的嵌入模型的一种方法是查看 Hugging Face 的大规模文本嵌入基准 (MTEB) 排行榜。 接下来是稀疏嵌入和稠密嵌入的使用问题，让我们分别看看它们的优点：
+ **稀疏嵌入：** 稀疏嵌入（例如 TF-IDF）非常适合于字面匹配场景。适用于关键字相关性至关重要的应用。它在计算上更省力，但可能无法捕获文本中更深层的语义含义。
+ **语义嵌入： **语义嵌入（例如 BERT 或 SentenceBERT）本身非常适合 RAG 的用例。
    - **BERT：** 适用于捕获文档和查询中的上下文细微差别。 与稀疏嵌入相比，需要更多计算资源，但提供更丰富的语义嵌入。
    - **SentenceBERT：** 适用于上下文和句子级含义很重要的场景。它在 BERT 的深度上下文理解和对简洁、有意义的句子表示的需求之间取得了平衡。这通常是 RAG 的 preferred route (首选方案)。

**Sentence Embeddings: 本质与缘由**

背景：与基于词级别的模型（例如 BERT）的区别

+ 首先，让我们概述一下Sentence Transformers与基于词级别的嵌入模型（例如 BERT）的区别。
+ Sentence Transformers是传统 BERT 模型的改进版本，专门用于生成整个句子的嵌入（即句子嵌入）。它们训练方法的主要区别在于：
    1. **目标: **BERT 训练目标是预测句子中的掩码词和下一个句子的预测。它针对理解句子中的单词及其上下文进行优化。而Sentence Transformers则专门训练来理解整个句子的含义。它们生成的嵌入使具有相似含义的句子在嵌入空间中更加接近。
    2. **嵌入层面: **主要区别在于嵌入的层面。BERT 为句子中的每个标记（单词或子词）提供嵌入，而Sentence Transformers则为整个句子提供单个嵌入。
    3. **训练数据和任务: **BERT 主要使用包含理解上下文单词任务的大型文本语料库进行训练，而Sentence Transformers通常使用包含句子对的数据集进行训练。这种训练侧重于相似性和相关性，教模型如何理解和比较整个句子的含义。
    4. **孪生网络和三重网络结构: **Sentence Transformers经常使用孪生网络或三重网络结构。这些网络涉及处理句子对或三元组，并调整模型，使相似的句子具有相似的嵌入，而不相似的句子具有不同的嵌入。这不同于 BERT 的训练，BERT 的训练本质上不涉及单独句子的直接比较。
    5. **针对特定任务的微调: **Sentence Transformers经常针对语义相似性、 paraphrase 识别或信息检索等特定任务进行微调。这种微调更注重句子级别的理解，而不是 BERT，BERT 可能针对更广泛的 NLP 任务（例如问答、情感分析等）进行微调，重点关注单词或词组级别的理解。
    6. **适用性: **BERT 和类似模型对于需要标记级别理解的任务（例如命名实体识别、问答）更通用，而Sentence Transformers更适合依赖句子级别理解的任务（例如语义搜索、句子相似性）。
    7. **生成句子嵌入或相似性任务的效率: **在标准 BERT 中，生成句子嵌入通常涉及将隐藏层之一的输出（通常是第一个标记 [CLS]）作为整个句子的表示。但是，这种方法并不总是适用于句子级别的任务。Sentence Transformers经过专门优化，可以生成更具意义和可用的句子嵌入，因此对于涉及句子相似性计算的任务更有效。由于它们每个句子生成单个向量，因此计算句子之间的相似度得分在计算上比基于词级别的模型更省力。
+ BERT 是一个通用语言理解模型，重点关注词级别的上下文，而Sentence Transformers则专门用于理解和比较整个句子的含义，使其更适用于需要句子级语义理解的任务。

**相关内容：Sentence Transformers vs. 词级别嵌入模型的训练过程**

+ 让我们深入了解Sentence Transformers如何与 BERT 等词级别嵌入模型采用不同的训练方式。
+ Sentence Transformers旨在生成句子级别的嵌入，这与 BERT 等词级别嵌入模型截然不同。下面概述了它们的训练过程及其与词级别模型的区别：
    1. **模型架构: **Sentence Transformers通常以类似于 BERT 或其他 Transformer 架构的基准模型作为起点。然而，它们侧重于为整个输入句子输出单个嵌入向量，而不是针对每个单独的词元。
    2. **训练数据: **它们使用各种数据集进行训练，这些数据集通常包含句子对或句子组，并且句子之间的关系（例如相似性、 paraphrase）是已知的。
    3. **训练目标: **BERT 预训练于掩码语言建模（预测缺失词）和下一个句子预测等目标上，这些目标侧重于理解词元级别的上下文。而Sentence Transformers则专门训练来理解句子级别的上下文和关系。它们的训练目标通常是使语义相似句子的嵌入距离最小化，同时使语义相异句子的嵌入距离最大化。这可以通过诸如三重损失、余弦相似性损失等对比损失函数来实现。
    4. **输出表示: **在 BERT 中，句子级别的表示通常来源于特殊标记（例如 [CLS]）的嵌入，或者通过池化词元嵌入（平均、最大池化或连接它们）得到。Sentence Transformers则被设计为直接输出有意义的句子级表示。
    5. **针对下游任务的微调: **Sentence Transformers可以针对特定任务进行微调，例如语义文本相似性，在该任务中，模型学习生成能够捕捉整个句子细微含义的嵌入。
+ Sentence Transformers经过专门优化，可以在句子级别生成表示，重点捕获句子的整体语义，这使得它们特别适用于句子相似性和聚类等任务。这与 BERT 等词级别模型形成对比，后者更侧重于理解和表示单个词元在其更广泛的上下文中的含义。

**将 Sentence Transformers 应用于 RAG 中**

+ 现在，让我们来看看为什么 Sentence Transformers 成为 RAG 生成嵌入模型的首选。
+ RAG 利用 Sentence Transformers 的理解和比较句子语义内容的能力。这种集成在需要模型在生成响应之前检索相关信息的情况下尤其有用。以下是 Sentence Transformers 在 RAG 设置中发挥作用的方式：
    - **改进文档检索: **Sentence Transformers 被训练生成捕获句子语义含义的嵌入。在 RAG 设置中，这些嵌入可以用来将查询（例如用户的问题）与数据库中最相关的文档或段落进行匹配。这至关重要，因为生成的响应质量往往取决于检索到的信息的相关性。
    - **高效语义搜索: **基于传统关键词的搜索方法可能难以理解查询的上下文或语义细微差别。Sentence Transformers 通过提供具有语义意义的嵌入，可以进行超越关键词匹配的更细致搜索。这意味着 RAG 的检索组件可以找到与查询语义相关的文档，即使这些文档不包含确切的关键词。
    - **理解上下文以生成更好响应: **通过使用 Sentence Transformers，RAG 模型可以更好地理解输入查询和潜在源文档内容的上下文和细微差别。这能生成更准确和符合语境的响应，因为模型的生成部分拥有更多相关且理解良好的信息来处理。
    - **信息检索的可扩展性: **Sentence Transformers 可以通过预先计算所有文档的嵌入来有效处理大型文档数据库。这使检索过程更快、更具可扩展性，因为模型在运行时只需计算查询的嵌入，然后快速找到最接近的文档嵌入即可。
    - **增强生成过程: **在 RAG 设置中，生成部分受益于检索部分提供相关、语义丰富的な (na) 信息的能力。这使语言模型能够生成不仅在上下文上准确，而且比模型本身训练数据涵盖更广泛的信息所支持的响应。
+ Sentence Transformers 通过支持更有效的语义搜索和信息检索，增强了 RAG 模型与大型语言模型 (LLMs) 的检索能力。这在需要理解和生成基于大量文本数据的响应的任务（例如问答、聊天机器人和信息提取）方面带来了性能提升。

## Retrieval
让我们来看三种不同的检索方法：标准检索、句子窗口检索和自动合并检索。每种方法都各有优缺点，其适用性取决于 RAG 任务的需求，包括数据集的性质、查询的复杂性以及响应中特异性与上下文理解之间的平衡。

### 标准检索 / 简单检索方法
+ 如下图所示 (来源)，标准流水线将相同的文本块用于索引/嵌入和输出合成。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1714028384426-9b99a559-a7e3-4be0-84c8-97d65bb7ca91.png)

检索方法的优缺点 (在大型语言模型 (LLMs) 的检索增强生成 (RAG) 语境下)，这里概述了三种检索方法的优缺点：

**优点**

+ **简单高效: **此方法使用相同的文本块进行嵌入和合成，简化了检索过程，非常直接和高效。
+ **数据处理的一致性:** 它保持了检索和合成阶段所使用数据的的一致性。

**缺点**

+ **有限的上下文理解: **大型语言模型可能需要更大的窗口来生成更好的响应，而这种方法可能无法充分提供。
+ **生成欠佳响应的可能性: **由于上下文有限，大型语言模型可能没有足够的信息来生成最相关和准确的响应。

### 句子窗口检索 / 小到大的切分
+ 句子窗口方法将文档分解成更小的单元，例如句子或一小段句子。
+ 这种方法将用于检索任务的嵌入 (存储在向量数据库中的较小块) 与用于生成任务的上下文分离。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1714028581785-ec05f00c-ead4-48e6-821c-a484dfd27879.png)

+ 在检索过程中，我们通过相似性搜索检索与查询最相关的句子，并用完整的周围上下文替换句子 (使用围绕上下文的静态句子窗口，通过检索最初检索到的句子周围的句子来实现)，如下图所示 。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1714028602303-aa2721eb-24f0-48f8-a059-d34ac7f76565.png)

**优点**

+ **检索特异性增强：**通过将文档分解成更小的单元，可以更精确地检索与查询直接相关的片段。
+ **丰富的上下文合成：**它在检索到的块周围重新引入上下文以进行合成，为大型语言模型提供更广泛的理解以生成更加合适的响应。
+ **平衡的方法：**这种方法在聚焦检索和上下文丰富性之间取得平衡，有可能提高响应质量。

**缺点**

+ **增加复杂性：**为检索和合成管理单独的流程会增加流程的复杂性。
+ **潜在的上下文差距：**如果添加回的周围信息不够全面，则存在漏掉更广泛上下文的风险。

### 自动合并检索 / 层次检索
+ 下图展示了自动合并检索如何运作，它不会像朴素方法那样检索到一堆零散的片段。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1714028719304-3fefc9f2-6ad3-495a-bd64-3b390c323087.png)

+ 正如下图所示，朴素方法在较小的片段尺寸下会产生更严重的片段化问题。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1714028739352-cb58636a-5928-4571-8735-593a29ce2844.png)

+ 自动合并检索旨在通过组合 (或合并) 来自多个来源或文本片段的信息，为查询创建更全面且与上下文相关的响应。这种方法在没有单个文档或片段能完全回答查询，而是需要来自多个来源的信息组合时尤其有用。
+ 它允许将较小的片段合并成更大的父片段。 它通过以下步骤完成此操作：
    1. 定义链接到父片段的较小子片段的层次结构。 
    2. 如果链接到父片段的较小子片段集超过某个阈值 (例如，余弦相似度)，则将较小的片段“合并”到更大的父片段中。
+  该方法最终将检索父片段以获得更好的上下文。

**优点**

1. **综合上下文响应：**通过合并来自多个来源的信息，它可以创建更全面且与上下文相关的响应。
2.  **减少片段化：**这种方法解决了朴素方法中常见的信息检索片段化问题，尤其是在较小的片段尺寸下。 
3. **动态内容集成：**它将较小的片段动态组合成更大、更具信息的片段，从而增强提供给大型语言模型的信息丰富性。

**缺点**

1. **层次结构和阈值管理的复杂性：**定义层次结构和设置适当的合并阈值的过程既复杂又关键，关系到该方法的有效运行。 
2. **过度泛化的风险：**存在合并太多或无关信息的可能性，从而导致响应过于宽泛或跑题。
3. **计算强度：**由于合并和管理文本片段的层次结构需要额外的步骤，因此这种方法的计算强度可能会更高。

### 寻找最佳片段大小
+ 在构建 RAG 应用时，经常会遇到许多检索参数/策略需要抉择（例如，从片段大小到向量搜索 vs. 关键词搜索 vs. 混合搜索）。让我们仔细看看片段大小这一方面。
+ 构建 RAG 系统涉及为检索器组件处理的文档确定最佳片段大小。理想的片段大小取决于几个因素：
    1. **数据特征**：数据的性质至关重要。对于文本文档，请考虑段落或章节的平均长度。如果文档结构良好且具有 distinct 的部分，那么这些自然划分可能作为分块的良好基础。
    2. **检索器限制**：您选择的检索器模型（例如 BM25、TF-IDF 或类似 DPR 的神经网络检索器）可能对输入长度有限制。确保片段与这些限制兼容至关重要。
    3. **内存和计算资源**：较大的片段大小可能导致更高的内存使用率和计算开销。平衡片段大小和可用资源以确保高效处理。
    4. **任务需求**：任务的性质（例如，问题解答、文档摘要）会影响理想的片段大小。对于细节任务，较小的片段可能更有效地捕获特定细节，而更广泛的任务可能受益于较大的片段来捕获更多上下文。
    5. **实验**：通常，确定理想片段大小的最佳方式是通过实证测试。使用不同的片段大小运行实验，并评估验证集上的性能，以找到粒度和上下文之间的最佳平衡。
    6. **重叠考虑**：有时，在片段之间留一些重叠是有益的，以确保不会在边界处遗漏重要信息。根据任务和数据特征确定合适的重叠大小。
+ 确定 RAG 系统的理想片段大小是一个权衡的过程，需要考虑数据特征、检索器模型的限制、可用资源、任务的特定需求和实证实验。这是一个可能需要迭代和微调才能取得最佳结果的过程。

## 检索集成和重新排序
+ 思路： 假设我们可以同时尝试多种分块大小，然后让重新排序器来剪枝结果会怎么样？
+ 这样做可以达到两个目的：
    - **性能提升（尽管代价更高）的检索结果**：通过汇总来自多个分块大小的结果来实现，前提是重新排序器具有相当的性能水平。
    - 互相比较不同检索策略的一种方法（相对于重新排序器）。
+ 过程如下：
    1. 使用一堆不同的方式将同一个文档进行分块，例如分块大小：128、256、512 和 1024。
    2. 在检索过程中，我们从每个检索器中获取相关的块，从而将它们一起集成用于检索。
    3. 使用重新排序器对结果进行排序/剪枝。
+ 下图描述了这一过程。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1714028998385-8c969854-aafd-49f5-82ed-c098e71bfa2e.png)

+ 根据 LlamaIndex 的评估结果，集成方法的忠实度指标略有上升，表明检索到的结果稍微更相关。但是成对比较会导致对两种方法的偏好相同，这使得集成是否更好仍然值得怀疑。
+ 注意： 集成策略还可以应用于 RAG 管道的其他方面，不仅仅是分块大小，例如向量搜索 vs. 关键词搜索 vs. 混合搜索等。

### 使用近似最近邻检索
+ 下一步是考虑从索引中选择哪个近似最近邻 (ANN) 库。选择最佳选项的一种方法是查看[这里](https://ann-benchmarks.com/)的排行榜。
+ 有关 ANN 的更多信息，请参阅 [ANN ](https://aman.ai/primers/ai/concepts/ann-similarity-search)基础知识。

### RAG 中的重新排序
+ RAG 中的重新排序是指根据检索到的文档或信息片段与给定查询或任务的相关性，对其进行评估和排序的过程。
+ RAG 中使用了几种不同的重新排序技术：
    - 词汇重新排序: 这涉及基于查询和检索到的文档之间词汇相似性的重新排序。BM25 或使用 TF-IDF 向量进行的余弦相似性等方法很常见。
    - 语义重新排序: 这种类型的重新排序利用语义理解来判断文档的相关性。它经常涉及像 BERT 或其他基于 Transformer 的模型，以理解超越单纯单词重叠的上下文和含义。
    - 学习排序 (LTR) 方法: 这些方法涉及训练一个模型，专门用于根据从查询和文档中提取的特征对文档进行排序（逐点、成对和列表式）。这可以包含词汇、语义和其他特征的混合。
    - 混合方法: 这些方法结合词汇和语义方法，可能还结合用户反馈或领域特定特征等其他信号，来改善重新排序效果。
+ 由于候选集限制在几十个样本，因此此阶段最常用神经网络 LTR 方法。一些用于重新排序的常见神经网络模型包括：
    - 多阶段文档排序 BERT (monoBERT 和 duoBERT)
    - 用于文本排序的预训练 Transformer 模型 - BERT 及以后
    - ListT5
    - ListBERT

## 响应生成/合成（Response Generation / Synthesis）
+ 响应生成/合成是 RAG 管道的最后一步，模型会在此步骤中综合其预训练知识和检索到的信息，生成连贯且符合上下文语境的回复。这一过程涉及整合来自各个来源的洞见，确保准确性和相关性，并精心设计一个回复，使其不仅信息丰富，而且与用户最初的查询保持一致，并采用自然对话的语气。
+ 需要注意的是，在为大型语言模型 (LLM) 创建扩展提示（包含检索到的前 K 个片段）以生成知情回复时，将重要信息策略性地放置在输入序列的开头或结尾，可以提高 RAG 系统的有效性，从而使系统性能更高。下面这篇论文对此进行了总结。

### 论文：[Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/pdf/2307.03172)
+ 尽管最近的语言模型能够以长文本作为输入，但我们对于语言模型如何有效利用较长上下文的信息知之甚少。这篇由刘等人 (Liu et al.) 撰写的论文（来自斯坦福大学、加州大学伯克利分校的 Percy Liang 实验室和 Samaya AI）分析了语言模型在两个任务上的性能，这两个任务都需要在输入上下文中识别相关信息：多文档问答和键值检索。简而言之，他们分析并评估了大型语言模型如何通过识别相关信息来利用上下文。
+ 他们测试了开源模型 (MPT-30B-Instruct、LongChat-13B) 和闭源模型 (OpenAI 的 GPT-3.5-Turbo 和 Anthropic 的 Claude 1.3)。他们使用多文档问答，其中上下文包含多个检索到的文档和一个正确答案（其位置被打乱）。键值对检索用于分析较长的上下文是否会影响性能。
+ 他们发现，当相关信息出现在输入上下文的开头或结尾时，性能往往最高，而当模型必须访问位于长上下文中间的相关信息时，性能会显著下降。换句话说，他们的发现基本上表明，当要回答查询的相关信息出现在上下文窗口的中间，并且强烈偏向开头和结尾时，检索增强 (RAG) 的性能会下降。
+ 他们学习结果的总结如下：
    - 相关信息位于开头时性能最佳。
    - 随着上下文长度的增加，性能下降。
    - 检索到的文档过多会损害性能。
    - 通过加入排序环节来改进检索和提示创建步骤可能会将性能提高高达 20%。
    - 如果提示适合原始上下文，扩展上下文模型 (GPT-3.5-Turbo vs. GPT-3.5-Turbo (16K)) 并没有更好表现。
+ 考虑到 RAG 从外部数据库检索信息，该数据库通常包含更长的文本，这些文本会被分割成块。即使经过分割，上下文窗口也会很快变得非常大，至少比“正常”问题或指令大得多。此外，即使对于明确的扩展上下文模型，随着输入上下文变长，性能也会大幅下降。他们的分析让我们更好地理解了语言模型如何使用其输入上下文，并为未来的长上下文模型提供了新的评估协议。
+ “基于 Transformer 的大型语言模型架构中没有特定的归纳偏置可以解释为什么文档中间的文本检索性能会更差。我怀疑这完全是因为训练数据和人类的写作方式：最重要的信息通常位于开头或结尾（想想论文的摘要和结论部分），然后大型语言模型在训练过程中会以此方式参数化注意力权重。” 
+ 换句话说，人类的文本内容通常会以开头和结尾部分最重要的方式进行构建，这可能是这项工作观察到的特征的潜在解释。
+ 您也可以通过以下图形用两种常见的人类认知偏差（首位-末位效应）来理解这一点。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1714029287365-f313d527-7af3-4d33-b19d-cfffee776445.png)

+ 最终的结论是，在 RAG 中将检索与排序结合起来（就像推荐系统一样）应该可以为问答提供最佳性能。
+ 下图展示了论文提出的观点的概述：“大型语言模型更擅长使用输入上下文开头或结尾的信息”。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1714029295434-d4fe039d-0110-483c-ab0a-e96ac09afd79.png)

+ 论文中的下图说明了改变相关信息位置（包含答案的文档）对多文档问答性能的影响。较低的位置更接近输入上下文的开头。当相关信息位于上下文的最开头或结尾时，性能通常最高；当模型必须推理位于其输入上下文中间的信息时，性能会迅速下降。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1714029300769-dd6a6764-8adc-443c-817b-b13538910694.png)

### 大海捞针测试（The “Needle in a Haystack” Test）
+ 对理解长上下文语言模型 (LLM) 在提示的不同部分进行上下文检索能力，我们可以进行一个简单的“大海捞针”测试。这种方法涉及在较大的、更复杂的文本（“干草堆”）中嵌入特定的目标信息（“针”）。测试的目的是评估 LLM 在大量其他数据中识别和利用这一特定信息的的能力。
+ 实际操作中，分析可以涉及将一个独特的事实或数据点插入到一篇冗长、看似无关的文本中。然后，LLM 将被赋予一些任务或查询，要求它回忆或应用嵌入的信息。这种设置模拟了现实世界中的场景，在这种场景中，重要细节经常埋藏在大量内容中，而检索这些细节的能力至关重要。
+ 实验可以设计成评估 LLM 性能的各个方面。例如，“针”的放置位置可以改变——文本的开头、中间或结尾——以查看模型的检索能力是否会根据信息的位置而改变。此外，周围“干草堆”的复杂性也可以进行修改，以测试 LLM 在不同程度的上下文难度下的性能。通过分析 LLM 在这些场景中的表现，我们可以深入了解其上下文检索能力以及潜在的改进领域。
+ 这可以通过使用 “大海捞针” 库来实现。下图显示了 OpenAI 的 GPT-4-128K 在不同上下文长度下的性能（顶部）和比较（底部）。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1714029417010-56107d8c-fcc9-48ef-80d9-242468188122.png)

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1714029423245-cd1a377c-efdf-4b8a-aadd-158870108401.png)

+ 下图显示了 Claude 2.1 的长上下文问题回答错误，这些错误基于提示上下文长度的区域。与 Claude 2 相比，Claude 2.1 的错误答案平均减少了 30%。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1714029427518-0061373c-a6cc-42f5-89c4-2fa8d0d8aafc.png)

+ 然而，在他们关于 Claude 2.1 的“长上下文提示”的博客中，Anthropic 指出，在 Claude 的回答开头添加“这是上下文中最相关的一句话”将原始评估中的得分从 27% 提高到了 98%! 下面来自博客的图表显示了 Claude 2.1 在其完整的 200K 个标记上下文窗口中检索单个句子时的性能。此实验使用了上述提示技术来指导 Claude 回忆最相关的句子。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1714029432618-4c27d71c-707c-47ee-ab97-864b2f13f103.png)

## 组件式评估 RAG 系统中的大型语言模型 (LLMs)
+ 组件式评估涉及单独评估 RAG 系统的各个组件。这种方法通常会检查检索组件的性能，该组件从数据库或语料库中获取相关信息，以及生成组件的性能，该组件根据检索到的数据合成响应。通过单独评估这些组件，研究人员可以识别整体 RAG 系统中需要改进的特定领域，从而使大型语言模型的的信息检索和响应生成更加高效和准确。
+ 虽然诸如上下文精度、上下文召回和上下文相关性等指标可以深入了解 RAG 系统检索组件的性能，但扎实性和答案相关性则可以揭示生成的响应质量。
+ 具体来说：
    1. **用于评估检索的指标：** 上下文相关性、上下文召回和上下文精度，它们共同评估响应用户查询时检索到的信息的相关性、完整性和准确性。上下文精度侧重于系统将相关条目排名更高的能力，上下文召回评估系统检索上下文所有相关部分的性能，而上下文相关性则衡量检索到的信息与用户查询的一致性。这些指标确保检索系统在为生成准确响应提供最相关和最完整的上下文方面发挥效用。
    2. **用于评估生成的指标：** 忠实性和答案相关性，分别衡量生成的答案与给定上下文的语义一致性以及其与原始问题的相关性。忠实性侧重于答案的事实准确性，确保所有做出的陈述都可以从给定的上下文中推断出来。答案相关性评估答案解决原始问题的能力，并惩罚不完整或冗余的响应。这些指标确保生成组件生成符合上下文且语义相关的答案。
+ 这四个方面的调和平均数会给你一个整体得分（也称为 ragas 分数），它是衡量 RAG 系统在所有重要方面性能的单一指标。
+ 大多数测量不需要任何标记数据，这使用户更容易运行评估，而无需担心首先构建人工注释的测试数据集。为了运行 ragas，您只需要几个问题，如果您使用 context_recall，则还需要一个参考答案。
+ 总而言之，这些指标提供了 RAG 系统检索性能的综合视图，可以使用诸如 Ragas 或 TruLens 的评估 RAG 管道的库来实现，并提供有关您的 RAG 管道的性能的详细见解，重点关注响应用户查询时检索和生成内容的上下文和事实一致性。具体来说，Ragas 提供了专门用于孤立评估 RAG 管道的每个组件的指标。这种方法补充了更广泛的系统级端到端评估（在“端到端评估”中详细说明），可以让您更深入地了解 RAG 系统在现实世界场景中的性能，在这些场景中，上下文和事实准确的细化至关重要。下图显示了 Ragas 提供的指标，这些指标专门用于孤立评估 RAG 管道的每个组件（检索、生成）。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1714029564252-83538550-b14d-4222-bc82-530f1d3c90ba.png)

+ [下图](https://learn.deeplearning.ai/building-evaluating-advanced-rag/lesson/2/advanced-rag-pipeline)显示了用于评估 RAG 的“三元组”指标：扎实性（也称为忠实性）、答案相关性和上下文相关性。请注意，上下文精度和上下文召回也很重要，并且在较新版本的 Ragas 中被引入。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1714029577420-fcef6606-1a8d-4d4c-9348-74066a02ad1a.png)

## 检索评估指标
+ 在大型语言模型 (LLMs) 的背景下，评价 RAG 的检索组件涉及评估系统检索相关信息以支持生成准确和符合上下文响应的有效性。

### 上下文准确率
+ 类别：上下文对齐和准确率
+ 关注点：评估 RAG 系统在结果中将来自上下文的 ground-truth 相关条目排名更高的准确性。它对于确定响应查询时是否将最相关的的信息块优先放在顶层排名至关重要。
+ 测量方法：使用一个公式，该公式考虑了前 K 个结果中真阳性 (正确排名较高的相关条目) 和假阳性 (错误排名较高的非相关条目) 的存在。这通常使用问题及其上下文信息进行评估。
+ 评估方法：该指标的计算首先是识别前 K 个上下文块中的真阳性和假阳性，然后计算 K 位准确率。上下文准确率的公式如下：

Context Precision@k = ∑ precision@k / 总的 K 个 top 结果中的相关条目数

Precision@k = 真阳性@k / (真阳性@k + 假阳性@k)

    - 其中 k 表示上下文中考虑的总块数。上下文准确率的值介于 0 和 1 之间，得分越高表示上下文与查询的相关条目更精确地对齐。

### 上下文召回率
+ 类别：上下文对齐和召回率
+ 关注点：评估 RAG 系统检索到的上下文与被视为 ground-truth 的标注答案对齐的程度。它具体衡量系统检索与 ground-truth 答案直接相关的上下文的所有相关部分的能力。
+ 测量方法：上下文召回率通过分析 ground-truth 答案中的句子和检索到的上下文之间的对应关系来计算。可以使用评估 ground-truth 句子与检索到的上下文归因的方法进行测量。
+ 评估方法：该过程涉及识别 ground-truth 答案中的每个句子并确定其是否包含在检索到的上下文中。计算上下文召回率的公式如下：

Context Recall = 可归因于上下文的 GT 句子数 / GT 中的句子数

    - 示例
        * **问题：** 法国在哪里？它的首都是什么？
        * **Ground truth (标注答案)：** 法国位于西欧，其首都巴黎。
        * **高上下文召回率示例：** 上下文包含法国位于西欧的信息，并提到巴黎是其首都。
        * **低上下文召回率示例：** 上下文讨论了法国的地理特征和历史，但没有提及首都。
    - 在这个指标中，值介于 0 和 1 之间，值越高表示检索到的上下文与 ground-truth 答案对齐的性能越好。

### 上下文相关性
+ 类别：上下文对齐和相关性
+ 关注点：
    - “返回的段落是否与给定查询的答案相关？”
    - 衡量 RAG 系统检索到的上下文或内容与用户查询的匹配程度。它具体评估检索到的信息是否与给定查询相关和适当，确保仅包含基本信息以有效地解决查询。
+ 测量方法：可以使用小型 BERT 风格的模型、嵌入距离或 LLM 进行测量。该方法涉及通过识别检索到的上下文中直接与回答给定问题相关的句子来估计上下文相关性的值。
+ 评估方法：涉及两步程序：第一步，使用语义相似度测量识别相关句子，为每个句子生成相关性得分。然后量化整体上下文相关性，最终得分使用以下公式计算：

Context Relevance = 检索到的上下文中与查询相关的句子数 / 检索到的上下文中的句子总数

    - 示例
        * **高上下文相关性示例：** 对于像“法国的首都是什么？”这样的问题，非常相关的上下文示例是：“法国位于西欧，拥有中世纪城市、阿尔卑斯山村庄和地中海海滩。其首都巴黎以时尚之家、包括卢浮宫在内的古典艺术博物馆以及埃菲尔铁塔等地标而闻名。”
        * **低上下文相关性示例：** 对于同一个问题，较少相关的上下文示例可能包含一些额外的、无关的信息，例如：“该国还以其美酒和精致的菜肴而闻名。拉斯科的史前洞穴壁画、里昂的罗马剧院和宏伟的凡尔赛宫都证明了其丰富的历史。”
    - 这个指标确保 RAG 系统提供简洁和直接相关的的信息，从而提高对特定查询的响应的效率和准确性。

## 生成器指标 (Generation Metrics)
+ 在大型语言模型 (LLMs) 的背景下评估 RAG 的生成部分涉及评估系统将检索到的信息无缝集成到连贯、语境相关且语言准确的响应中的能力，确保检索数据和生成语言技能的和谐融合。简而言之，这些指标共同提供了一种细致和多维的方式来评估 RAG 系统，不仅强调信息的检索，还强调其上下文相关性、事实准确性和与用户查询的语义一致性。

### 忠实度 (Groundedness (a.k.a. Faithfulness))
+ 类别：事实一致性和语义相似性
+ 关注点：
    - “生成的答案是否忠实于检索到的段落？ 还是包含了段落之外的幻觉或推断陈述？”
    - 此指标评估模型响应与检索到的文档之间的事实一致性和语义相似性。它确保生成的响应在上下文上是适当的，并且在事实层面基于检索到的信息。具体而言，它评估模型答案中提出的所有陈述是否可以直接从给定的上下文中推断出来。
+ 测量方法：忠实度可以使用自然语言推理 (NLI) 模型、大型语言模型 (LLMs) 以及自动系统和人类判断的组合来测量。忠实度得分是通过将生成的答案中提出的陈述与给定上下文中提出的陈述进行比较来计算的，使用以下公式：

忠实度 = 可以从给定上下文中推断的陈述数量 / 生成答案中陈述的总数 

+ 评估方法：利用思维链 (CoT) 提示来模拟推理过程，通过结合自动系统（用于语义匹配和事实核查）和人类判断的混合方法对齐进行评分 (0 或 1 分)。评估过程涉及从生成的答案中识别一系列陈述，并用给定的上下文交叉检查这些陈述中的每一个，以确定它们的 factual consistency（事实一致性）。
    - 例子：
        * 高忠实度示例：对于问题“爱因斯坦出生在哪里？什么时候出生？”，上下文为“阿尔伯特·爱因斯坦（1879 年 3 月 14 日出生）是一位德国理论物理学家”，高忠实度的答案将是“爱因斯坦于 1879 年 3 月 14 日出生于德国”。
        * 低忠实度示例：对于相同的问题和上下文，低忠实度的答案将是“爱因斯坦于 1879 年 3 月 20 日出生于德国”。
    - 此指标对于确保 RAG 系统生成响应的可靠性和可信度至关重要，因为它直接关系到响应查询时提供的信息的准确性和事实一致性。

### 答案相关性 (Answer Relevance)
+ 类别：响应质量和语义相关性 (Response Quality and Semantic Relevance)
+ 关注点：
    - “生成的答案是否与查询和检索到的段落相关？” (Is the generated answer relevant given the query and retrieved passage?”)
    - 此指标评估模型生成的答案与用户查询的相关性。它评估生成的答案与给定提示的关联程度，并惩罚不完整或包含冗余信息的答案。它不考虑事实性，而是关注响应对原始问题的直接性和适当性。
+ 测量方法：使用 BERT 风格的模型、嵌入距离或 LLM 进行量化。该指标采用无参考的方式计算，使用从答案生成的多个问题和原始问题之间的平均余弦相似度。该测量的公式为：

答案相关性 = 1 / n * Σ (i=1 到 n) sim(q, q_i)

    - 其中 q 表示原始问题，q_i 表示从答案生成的多个问题，sim 表示它们嵌入之间的余弦相似度。
+ 评估过程：涉及提示 LLM 多次生成一个适合生成答案的问题，然后测量这些生成的问题与原始问题之间的平均余弦相似度。基本思想是，如果生成的答案准确地解决了初始问题，那么 LLM 应该能够从答案中生成与原始问题紧密相关的后续问题。
    - 例子：
        * 低相关性答案示例：对于问题“法国在哪里？它的首都是什么？”，低相关性答案将是“法国位于西欧”。
        * 高相关性答案示例：对于相同的问题，高相关性答案将是“法国位于西欧，巴黎是其首都”。
    - 此指标对于确保 RAG 系统提供的答案不仅准确，而且完整且直接地回答用户查询，而不包含不必要的细节，至关重要。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1714030144061-086b71ef-2eb2-4a2a-a862-51057c464a34.png)

## 端到端评价(End-to-End Evaluation)
+ 评估管道 (Pipeline) 的端到端性能也至关重要，因为它直接影响用户体验。Ragas 提供了可用于评估管道整体性能的指标，以确保全面评估。

### 答案语义相似度 (Answer Semantic Similarity)
+ 类别：答案质量和语义一致性 关注点：评估 RAG 系统生成的答案与真实答案之间的语义相似程度。此指标专门评估生成的答案的含义与真实答案的含义的接近程度。
+ 测量方法：使用交叉编码器模型来测量此指标，该模型旨在计算语义相似度得分。这些模型分析生成答案和真实答案的语义内容。
+ 评估方法：该方法涉及将生成答案与真实答案进行比较，以确定语义重叠的程度。语义相似度以 0 到 1 的比例进行量化，得分越高表示生成答案与真实答案的一致性越强。答案语义相似度的公式隐式地基于语义重叠的评估，而不是直接的公式。
+ 例子： 
    - 真实答案：阿尔伯特爱因斯坦的相对论彻底改变了我们对宇宙的理解。 
    - 高相似度答案：爱因斯坦的突破性相对论改变了我们对宇宙的理解。 
    - 低相似度答案：艾萨克牛顿的运动定律极大地影响了经典物理学。
+ 在这个指标中，较高的分数反映了生成的响应在语义上接近真实答案的质量，表明答案更加准确和符合上下文。

### 答案正确率 (Answer Correctness)
+ 类别：答案准确性和正确性 关注点：此指标评估 RAG 系统生成的答案与真实答案相比的准确性。它不仅强调语义相似性，还强调生成答案相对于真实答案的事实正确性。
+ 测量方法：答案正确率的评估涉及评估语义相似性和事实相似性的结合。这些方面使用加权方案进行集成，该方案可以包括使用交叉编码器模型或其他用于语义分析的复杂方法。用户还可以应用阈值以二进制方式解释分数。
+ 评估方法：该方法需要将生成答案与真实答案进行比较，以评估语义和事实一致性。这两种方面的综合评估会得到答案正确率得分，范围为 0 到 1，得分越高表示与真实答案的准确性和一致性更高。
+ 例子： 
    - 真实答案：爱因斯坦于 1879 年出生于德国。 
    - 高答案正确率示例：1879 年，爱因斯坦出生于德国。 
    - 低答案正确率示例：爱因斯坦于 1879 年出生于西班牙。
+ 此指标不仅强调理解用户查询的上下文和内容（例如上下文相关性评估），还确保提供的答案在事实和语义上与既定事实保持一致，从而确保 RAG 系统提供高质量的响应。多模态 RAG

## 多模态 RAG
+  许多文档包含多种类型的内容,包括文本和图像。然而,在大多数 RAG 应用中,图像中捕获的信息会丢失。随着多模态 LLM 的出现,如 GPT-4V,值得考虑如何在 RAG 中利用图像。 
+  以下是在 LangChain 中使用 RAG 的三种方式: 
    -  **选项 1:** 
        *  使用多模态嵌入(如 CLIP)嵌入图像和文本。 
        *  使用相似性搜索检索两者。 
        *  将原始图像和文本块传递给多模态 LLM 进行答案合成。 
    -  **选项 2:** 
        *  使用多模态 LLM(如 GPT-4V、LLaVA 或 FUYU-8b)从图像生成文本摘要。 
        *  嵌入和检索文本。 
        *  将文本块传递给 LLM 进行答案合成。 
    -  **选项 3:** 
        *  使用多模态 LLM(如 GPT-4V、LLaVA 或 FUYU-8b)从图像生成文本摘要。 
        *  嵌入和检索带有原始图像引用的图像摘要。您可以使用 Chroma 等向量数据库的[多向量检索器](https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector)来存储原始文本和图像及其摘要,以便检索。 
        *  将原始图像和文本块传递给多模态 LLM 进行答案合成。 
+  当不能使用多模态 LLM 进行答案合成时(例如,成本等),选项 2 是合适的。 
+  下图概述了上述所有三个选项。 

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1714030566397-906478f9-0f26-4a6c-ad0f-96df8e7f06c1.png)

+  LangChain 提供[选项 1](https://github.com/langchain-ai/langchain/blob/master/cookbook/multi_modal_RAG_chroma.ipynb) 和[选项 3](https://github.com/langchain-ai/langchain/blob/master/cookbook/Multi_modal_RAG.ipynb?ref=blog.langchain.dev) 的 cookbooks。 
+  下面的信息图也提供了多模态 RAG 的顶层概述。 

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1714030591818-19ef714c-61cb-48ea-85e8-d023e19e7565.png)

## 改进 RAG 系统
+ 为了增强和改进 RAG 系统,请考虑以下三种结构化方法,每种方法都有全面的指南和实际实现: 
    1. **重新排序检索结果**:一种基本而有效的方法是使用重新排序模型来完善通过初始检索获得的结果。这种方法优先考虑更相关的结果,从而提高生成内容的整体质量。MonoT5、MonoBERT、DuoBERT 等都是可用作重新排序器的深度模型的例子。要详细探索这种技术,请参阅 Mahesh Deshwal 提供的指南和代码示例。
    2. **FLARE 技术**:在重新排序之后,应该探索 FLARE 方法。每当生成内容的某个部分的置信度低于指定阈值时,该技术会动态查询互联网(也可以是本地知识库)。这克服了传统 RAG 系统的一个重大限制,传统系统通常只在开始时查询知识库,然后生成最终输出。Akash Desai 的指南和代码演练提供了对该技术的深入理解和实际应用。更多关于 FLARE 技术的信息请参见主动检索增强生成部分。
    3. **HyDE 方法**:最后,HyDE 技术引入了一个创新概念,即生成一个假设文档来响应查询。然后将该文档转换为嵌入向量。这种方法的独特之处在于使用该向量在语料库嵌入空间中识别相似的邻域,从而根据向量相似性检索类似的真实文档。要深入了解这种方法,请参阅 Akash Desai 的指南和代码实现。更多关于 HyDE 技术的信息请参见无相关性标签的精确零样本密集检索部分。
+ 这些方法中的每一种都提供了一种独特的方法来改进 RAG 系统,有助于获得更准确和上下文相关的结果。

## RAG 2.0
+ 由 Contextual AI 推出的 RAG 2.0 代表了企业使用的稳健 AI 系统的重大进步,与其前身不同,它对整个系统进行了端到端的优化。这一新一代引入了上下文语言模型(CLM),不仅超越了原始 RAG 基准,而且在各种行业基准测试中胜过了基于 GPT-4 的最强可用模型,在开放领域问答和真相验证等专业任务中表现出色。
+ RAG 2.0 的引入标志着不再使用现成的模型和分散的组件,这些模型和组件使以前的系统变得脆弱,不适合生产环境。相反,RAG 2.0 将语言模型和检索器作为一个系统进行端到端优化。
+ 部署 RAG 2.0 CLM 的实际应用中,关键改进显而易见。使用 Google Cloud 最新的 ML 基础设施,这些模型显示出显著的准确性提高,特别是在金融和法律等领域,凸显了它们在专业领域的潜力。
+ 进一步的比较显示,RAG 2.0 显著优于传统的长上下文模型,以更少的计算需求提供更高的准确性。这使得 RAG 2.0 特别适合在生产环境中扩展。
+ 总的来说,RAG 2.0 的创新方法不仅推动了生成式 AI 在生产环境中的发展,而且通过广泛的基准测试和实际部署证明了其优越性,邀请企业参与其持续开发和应用。

## 



2023年,大型语言模型(LLM)吸引了所有人的注意力,它可以通过提示来执行通用任务,例如翻译或编码。这自然导致人们将模型作为AI应用开发的主要成分而密切关注,所有人都在想新的LLM将带来什么能力。然而,随着越来越多的开发者开始使用LLM构建,我们认为这种关注正在迅速改变:最先进的AI结果越来越多地来自具有多个组件的复合系统,而不仅仅是单一的模型。  
例如,谷歌的AlphaCode 2通过精心设计的系统设置了编程的最新状态,该系统使用LLM为一个任务生成多达100万个可能的解决方案,然后过滤和评分。同样,AlphaGeometry将LLM与传统的符号求解器相结合,以解决奥林匹克问题。在企业中,我们在Databricks的同事发现,60%的LLM应用使用某种形式的检索增强生成(RAG),30%使用多步链。即使研究传统语言模型任务的研究人员,以前只报告单个LLM调用的结果,现在也开始报告越来越复杂的推理策略的结果:微软写了一种链接策略,在医学考试中超过GPT-4的准确率9%,谷歌发布Gemini时使用新的CoT@32推理策略在MMLU基准测试中调用模型32次,这引发了与单次调用GPT-4进行比较的问题。向复合系统的转变开启了许多有趣的设计问题,但这也令人兴奋,因为这意味着领先的AI结果可以通过巧妙的工程实现,而不仅仅是训练规模的扩大。  
在这篇文章中,我们分析了复合AI系统的趋势及其对AI开发者的意义。为什么开发人员要构建复合系统?随着模型的改进,这个范式是否会持续下去?又有哪些新兴的工具可以开发和优化这样的系统——这是一个比模型训练研究要少得多的领域?我们认为,复合AI系统在未来可能是最大化AI结果的最佳方式,并且可能是2024年AI中最重要的趋势之一。![](https://cdn.nlark.com/yuque/0/2024/png/406504/1708594927806-57d91a8c-d3c5-4d11-908b-9f4a80df0bce.png)

## 为什么需要复合系统？
我们将复合AI系统定义为使用多个交互组件来处理AI任务的系统,包括对模型、检索器或外部工具的多次调用。相比之下,AI模型仅仅是一个统计模型,例如用于预测文本中下一个标记的Transformer。  
尽管AI模型在不断改进,其规模也看不到尽头,但越来越多的最先进结果是使用复合系统获得的。为什么会这样?我们看到了几个不同的原因:

1.  通过系统设计改进某些任务更加容易。虽然LLM似乎遵循显著的缩放定律,可以通过更多的计算可靠地产生更好的结果,但在许多应用中,构建复合系统的回报率高于简单扩大训练规模。例如,假设当前最好的LLM可以解决编程竞赛问题的30%左右,将其训练预算提高3倍可以将这个比例提高到35%;这对于赢得编程竞赛来说仍然不够可靠!相比之下,通过多次对模型进行采样、测试每个样本等方式进行工程设计的系统,可能会使用目前的模型将性能提高到80%,正如AlphaCode等工作所示。更重要的是,迭代系统设计的速度往往比等待训练运行要快得多。我们认为,在任何高价值的应用中,开发人员都会想要使用所有可用的工具来最大限度地提高AI质量,因此他们会除了扩大规模外还会使用系统设计的想法。我们经常在LLM用户身上看到这一点,一个好的LLM会创建一个引人注目但令人沮丧的不可靠的首次演示,然后工程团队会系统地提高质量。 
2.  系统可以是动态的。机器学习模型本质上是有限的,因为它们是在静态数据集上训练的,所以它们的“知识”是固定的。因此,开发人员需要将模型与其他组件(如搜索和检索)相结合,以纳入及时的数据。此外,训练使模型可以“看到”整个训练集,因此需要更复杂的系统来构建具有访问控制的AI应用程序(例如,仅根据用户有权访问的文件来回答用户的问题)。 
3.  通过系统提高控制和信任更容易。仅使用神经网络模型很难进行控制:尽管训练会对它们产生影响,但几乎不可能保证模型会避免某些行为。相反,使用AI系统而不是模型可以帮助开发人员更严格地控制行为,例如通过过滤模型输出。同样,即使是最好的LLM仍然会幻想,但将LLM与检索相结合的系统可以通过提供引文或自动验证事实来增加用户信任。 
4.  性能目标差异很大。每个AI模型都有固定的质量水平和成本,但应用程序通常需要改变这些参数。在一些应用中,例如内联代码建议,最好的AI模型成本太高,因此像Github Copilot这样的工具使用经过精心调优的较小模型和各种搜索启发式来提供结果。在其他应用中,即使是最大的模型,像GPT-4,也太便宜了!许多用户愿意为一个正确的法律意见支付几美元,而不仅仅是让GPT-4消耗几美分,但开发人员需要设计一个AI系统来利用这个更大的预算。 

在生成式AI中的复合系统趋势也与其他AI领域的行业趋势相匹配,例如自动驾驶汽车:大多数最先进的实现都是具有多个专用组件的系统(更多讨论)。出于这些原因,我们相信复合AI系统在模型改进的同时仍将是领先的范式。

## 复合系统的发展
虽然复合AI系统可以提供明显的好处,但设计、优化和操作它们的艺术仍在萌芽阶段。表面上,AI系统是传统软件和AI模型的组合,但存在许多有趣的设计问题。例如,整体的“控制逻辑”应该用传统代码编写(例如,调用LLM的Python代码),还是应该由AI模型驱动(例如,调用外部工具的LLM代理)?同样,在复合系统中,开发者应该在哪里投入资源——例如,在带检索的RAG管道中,是更好地在检索器还是LLM上花费更多的FLOPS,或者甚至多次调用LLM?最后,我们如何对具有离散组件(如搜索引擎或代码解释器)的AI系统进行端到端优化,以最大限度地提高指标,就像我们可以训练神经网络一样?在这一节中,我们详细说明了几个AI系统示例,然后讨论了这些挑战和最近对它们的研究。

## AI系统设计空间
下面是一些最近的复合AI系统示例,以展示设计选择的广度:

AlphaCode 2:

+ 精调过的LLM用于对程序进行采样和评分
+ 代码执行模块
+ 聚类模型
+ 为编程问题生成多达100万个解决方案然后过滤和评分
+ 在编程比赛中匹配人类85%的水平

AlphaGeometry:

+ 精调过的LLM
+ 符号数学引擎
+ 通过LLM迭代地在几何问题中建议构造,并检查由符号引擎产生的演绎事实
+ 在国际数学奥林匹克竞赛的计时测试中位于银牌和金牌得主之间

MedPrompt:

+ GPT-4 LLM
+ 正确示例数据库中的最近邻搜索
+ LLM生成的思维链示例
+ 多次采样和集成
+ 通过搜索类似示例来构造少量提示,为每个示例添加模型生成的思维链,并生成和评估多达11个解决方案,来回答医学问题
+ 优于像Med-PaLM这样的专用医学模型和更简单的提示策略

Gemini对MMLU的评估:

+ Gemini LLM
+ 自定义推理逻辑
+ Gemini对MMLU基准测试的CoT@32推理策略对模型进行32次链式思考采样,如果大多数采样结果一致则返回顶部选择,如果不一致则使用不带链式思考的生成
+ 在MMLU上达到90.04%的准确率,相比之下,GPT-4带5个示范的提示达到86.4%,Gemini带5个示范的提示达到83.7%

ChatGPT Plus:

+ LLM
+ 检索即时内容的网页浏览器插件
+ 执行Python的代码解释器插件
+ DALL-E图像生成器
+ ChatGPT Plus可以调用网络浏览等工具来回答问题;LLM确定何时以及如何调用每个工具以响应
+ 拥有数百万付费订阅用户的流行消费者AI产品

RAG、ORQA、Bing、 [Baleen](https://arxiv.org/pdf/2101.00436.pdf)等:

+ LLM(有时被多次调用)
+ 检索系统
+ 以各种方式组合LLM和检索系统,例如,让LLM生成搜索查询,或直接搜索当前上下文
+ 在搜索引擎和企业应用中被广泛使用的技术

## 符合系统的关键性挑战
与AI模型相比,复合AI系统在设计、优化和操作方面带来了新的挑战。

### 设计空间
针对给定任务的可能系统设计范围是巨大的。例如,即使在只有检索器和语言模型的简单RAG情况下,也存在:(i)许多可选择的检索和语言模型,(ii)提高检索质量的其他技术,如查询扩展或排序模型,以及(iii)改进LLM生成输出的技术(例如,运行另一个LLM来检查输出是否与检索到的段落相关)。开发人员必须探索这个巨大的空间来找到好的设计。

此外,开发人员需要在系统组件之间分配有限的资源,如延迟和成本预算。例如,如果要在100毫秒内回答RAG问题,是应该将20毫秒的预算分配给检索器,80毫秒分配给LLM,还是反过来?

### 优化
通常在机器学习中,最大化复合系统的质量需要组件之间的协同优化。例如,考虑一个简单的RAG应用程序,其中LLM看到用户的问题,生成发送给检索器的搜索查询,然后生成答案。理想情况下,LLM应该针对特定检索器调优以生成适合的查询,检索器应该调优以优先返回适合该LLM的答案。

在PyTorch等单模型开发中,用户可以轻松对整个可微模型进行端到端优化。然而,复合AI系统包含不可微分的组件,如搜索引擎或代码解释器,因此需要新的优化方法。优化这些复合AI系统仍是一个新的研究领域;例如,DSPy提供了一个针对LLM调用和其他组件管道的通用优化器,而其他系统(如LaMDA、Toolformer和AlphaGeometry)在模型训练期间使用工具调用来优化针对这些工具的模型。

### 操作
对于复合AI系统,机器学习操作(MLOps)变得更具挑战性。例如,对于传统的机器学习模型(如垃圾邮件分类器),跟踪成功率很简单,但开发人员应该如何跟踪和调试同一任务的LLM代理的性能,它可能会使用不同数量的“反思”步骤或外部API调用来分类消息?我们认为将开发一代新MLOps工具来解决这些问题。有趣的问题包括:

+  监控:开发人员如何最高效地记录、分析和调试复杂AI系统的跟踪? 
+  数据运维:由于许多AI系统涉及数据服务组件(如向量数据库),并且它们的行为取决于所提供的数据质量,因此关注这些系统的运维应该额外跨越数据管道。 
+  安全:研究表明,与单个模型相比,复合AI系统(如具有内容过滤器的LLM聊天机器人)可能会产生意想不到的安全风险。需要新的工具来保护这些系统。 

## 新型范式
为了解决构建复合AI系统的挑战,正在产业和研究中出现许多新方法。我们重点介绍其中一些最广泛使用的方法和我们对这些挑战研究的示例。  
设计AI系统: 组合框架和策略。现在许多开发人员使用“语言模型编程”框架,可以通过对AI模型和其他组件的多次调用来构建应用程序。这些包括开发人员从传统程序中调用的组件库,如LangChain和LlamaIndex,允许LLM驱动应用程序的代理框架,如AutoGPT和BabyAGI,以及用于控制LM输出的工具,如Guardrails、Outlines、LMQL和SGLang。与此同时,研究人员正在开发许多新的推理策略,以通过对模型和工具的调用来生成更好的输出,比如思维链、自洽性、WikiChat、RAG等。  
自动优化质量:DSPy。来自学术界的DSPy是第一个旨在优化由LLM调用和其他工具组成的系统以最大限度地提高目标指标的框架。用户可以通过对LLM和其他工具的调用编写应用程序,并提供目标指标,例如在验证集上的准确性,然后DSPy会通过为每个模块创建提示说明、少量示例等参数选择来自动调整管道,以最大限度地提高端到端性能。其效果类似于在PyTorch中对多层神经网络进行端到端优化,只是DSPy中的模块不总是可微分层。为此,DSPy巧妙地利用了LLM的语言能力:为指定每个模块,用户编写自然语言签名,如user_question -> search_query,其中输入和输出字段的名称有意义,DSPy会自动将其转化为适当的提示与说明、少量示例,甚至对底层语言模型的权重更新。  
优化成本:FrugalGPT和AI网关。大量可用的AI模型和服务使得为应用程序选择合适的模型变得具有挑战性。此外,不同的模型在不同的输入上表现可能更好。FrugalGPT是一个框架,可以自动将输入路由到不同的AI模型级联,在给定预算约束下最大化质量。根据少量示例,它可以学习一个路由策略,在相同成本下最多可比最好的LLM服务提高4%的质量,或在匹配其质量的同时将成本降低高达90%。FrugalGPT是一个更广泛的新兴概念的例子,即AI网关或路由器,在Databricks AI Gateway、OpenRouter和Martian等软件中实现,以优化AI应用程序每个组件的性能。当AI任务被分解成较小的模块化步骤在复合系统中时,这些系统的效果会更好,网关可以针对每个步骤单独优化路由。

操作:LLMOps和DataOps。 AI应用程序一直需要仔细监控模型输出和数据管道才能可靠运行。但是,对于复合AI系统,系统对每个输入的行为可能要复杂得多,所以重要的是跟踪应用程序采取的所有步骤和中间输出。LangSmith、Phoenix Traces和Databricks Inference Tables等软件可以在细粒度跟踪、可视化和评估这些输出,在某些情况下还可以将其与数据流水线质量和下游指标相关联。在研究领域,DSPy Assertions旨在利用来自监控检查的反馈直接在AI系统中改进输出,而基于AI的质量评估方法(如MT-Bench、FAVA和ARES)旨在自动化质量监控。

## 总结
生成AI使每个开发者都可以通过自然语言提示解锁广泛的功能,这非常令人激动。但是,随着开发人员希望走出演示阶段并最大限度地提高AI应用程序的质量,他们越来越多地转向复合AI系统,作为一种自然的方法来控制和增强LLM的功能。确定开发复合AI系统的最佳实践仍是一个开放的问题,但已经有令人兴奋的方法来辅助设计、端到端优化和操作。我们认为,复合AI系统将是在未来最大化AI应用程序质量和可靠性的最佳方式,可能是2024年AI中最重要的趋势之一。

> 原文：[https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/](https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/)
>


RAG的应用不再局限于问答系统；其影响正在扩展到更多的领域。现在，推荐系统、信息提取和报告生成等多种任务开始受益于RAG技术的应用。

与此同时，RAG技术堆栈正在经历繁荣。除了Langchain和LlamaIndex等知名工具之外，市场上还出现了更有针对性的RAG工具，例如：那些为特定用例定制的工具，以满足更集中的场景需求；那些为进一步降低进入门槛而简化的工具；以及那些专门从事功能的工具，逐渐瞄准生产环境。

![来源：复旦RAG综述](https://cdn.nlark.com/yuque/0/2024/png/406504/1706575468999-e309894f-fbc5-4520-b95c-166aa14ffd4e.png)

## RAG Application
![](https://cdn.nlark.com/yuque/0/2024/png/406504/1706593447424-05d6da5c-b36c-406e-87c0-a3c7ade8e45b.png)

## RAG Paradigm
![](https://cdn.nlark.com/yuque/0/2024/png/406504/1706584381914-7bbff468-8d25-4803-84c0-039b4c93a6cb.png)

## Key Issues of RAG
![](https://cdn.nlark.com/yuque/0/2024/png/406504/1706584302133-a9d2f894-52eb-4897-945b-7336c7597161.png)

## Better RAG
### 1、数据索引优化
+ small2big：关键思想是将用于检索的块与用于合成的块分开，使用较小的块可以提高检索的准确率，而较大的块可以提供更多的上下文信息。
+ sliding window：一个简单方法是使用重叠chunks通过滑动窗口，语义转换得到增强。然而，存在局限性，包括对上下文大小的不精确控制、截断单词或句子的风险以及缺乏语义考虑。
+ summary：它类似于从小到大的概念，首先生成大块的摘要，然后对摘要进行检索。随后，可以对较大的块进行二次检索。
+ adding metadata：块可以用元数据信息丰富，如页码、文件名、作者、时间戳、摘要或块可以回答的问题。随后，可以根据该元数据过滤检索，限制搜索范围

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1706575982228-e58dceb2-0671-446f-a2ad-14e6168fd5cc.png)

### 2、结构化语料
+ Hierarchical Index：在文档的层次结构中，节点以父子关系排列，块链接到它们。数据摘要存储在每个节点，有助于数据的快速遍历，并帮助RAG系统确定要提取哪些块。这种方法还可以减轻块提取问题造成的幻觉。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1706575946468-1c5657f2-7b67-4fc9-a17a-727d89996e2c.png)

### 3、将知识图谱作为数据源
+ 利用知识图（KG）构建文档的层次结构有助于保持一致性。它描绘了不同概念和实体之间的联系，显着减少了产生错觉的可能性。优点是将信息检索过程转化为大模型可以理解的指令，从而提高知识检索的准确率，并使大模型能够生成上下文连贯的响应，从而提高RAG系统的整体效率.

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1706577281338-a5859c91-5498-4e2b-81ce-2fa90ca24396.png)

### 4、query优化
+ 将单个查询扩展为多个查询丰富了查询的内容，提供了进一步的上下文来解决任何缺乏特定细微差别的问题，从而确保生成答案的最佳相关性。包括：
    - multi-query：通过使用提示工程通过LLMs扩展查询，这些查询可以并行执行。查询的扩展不是随机的，而是精心设计的。这种设计的两个关键标准是查询的多样性和覆盖率。使用多个查询的挑战之一是用户原始意图的潜在稀释，为了缓解这种情况，我们可以指示模型在提示工程中为原始查询分配更大的权重。
    - sub-query：子问题规划的过程代表了生成必要的子问题，以使上下文化，并在组合时充分回答原始问题。这种添加相关上下文的过程原则上类似于查询扩展。具体来说，将一个复杂的问题分解成一系列更简单的子问题。
    - 重写：原始查询并不总是最适合大模型检索，尤其是在现实世界的场景中。因此，我们可以提示大模型重写查询。除了使用大模型进行查询重写之外，还可以利用专门的较小语言模型
    - HyDE：在响应查询时，大模型构造假设文档（假设答案），而不是直接在向量数据库中搜索查询及其计算向量，它侧重于从答案到答案的嵌入相似性，而不是为问题或查询寻求嵌入相似性，此外，它还包括Reverse HyDE，它侧重于从查询到查询的检索。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1706577759187-fc86584b-f672-429c-9b21-9e123ec5513a.png)

    - step-back：利用Google DeepMind提出的[Step-back Prompting](https://arxiv.org/abs/2310.06117)方法，对原始查询进行抽象，生成高级概念问题（step-back问题），在RAG系统中，同时使用step-back问题和原始查询进行检索，并将两者的结果作为语言模型答案生成的基础。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1706577358206-9eade359-3a8c-46d3-989b-de4ddbb59520.png)

### 5、Embedding优化
+ 选择一个更适合的Embedding提供方
+ 对Embedding大模型进行微调

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1706578213893-d21e08dc-a368-4048-b9cf-e59e3a614c64.png)

### 6、检索过程优化
+ 迭代检索：通过迭代的方式不断获取相关信息
+ 自适应检索：由大模型动态决定，是否需要检索以及检索的范围
    - Self-RAG：它旨在通过结合检索和自我反思的方法提升模型生成的质量和事实准确性。这种框架允许模型在需要时自主决定是否引入检索模块，选择哪个检索文档，并评判生成回复的质量。为了训练一个Self-RAG模型，论文作者首先在一个由GPT-4生成的合成数据集上训练一个Critic模型，该模型展示了如何将反思标记插入LLM的正常回复中。然后，使用这个Critic模型以离线方式为Self-RAG生成训练数据，通过向LLM的回复中添加Critic标记。接下来，可以使用一个类似LLaMA的模型作为起点，对这些数据进行端到端的训练，通过下一个标记的预测来学习必要的retrieve和critic技巧。
    - ![](https://cdn.nlark.com/yuque/0/2024/png/406504/1706579210547-bde57e4f-14ce-4619-a2de-6d81efa44a42.png)

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1706578341312-c0399ca9-538d-4c03-b2fe-393ea180a32d.png)

### 7、微调
+ 微调检索器
+ 微调生成器
+ 微调全部

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1706579567136-a62d9680-fef7-4f86-bfc8-17c95f41d3d5.png)

## Evaluate RAG
### 1、评估方法
+ 单独评估：评估检索、生成效果
+ 端到端评估：通过人工或自动的方式，自动评估整体的效果

### 2、评估能力
+ 答案
+ 上下文
+ query

### 3、评估框架
![](https://cdn.nlark.com/yuque/0/2024/png/406504/1706579641318-ea9b3b80-b85e-4aac-8700-5d7e4ee00771.png)

# Modular RAG Technical Map
定义模块和运算符后，它们可以帮助我们从流程的角度查看各种RAG方法。每个RAG都可以用一组运算符排列。

<font style="color:rgb(36, 36, 36);">那么，在模块化RAG的范式下，我们应该如何设计我们的RAG系统呢？</font>

![](https://cdn.nlark.com/yuque/0/2024/jpeg/406504/1706575494198-ff2d5a95-3a1a-40c3-ba4a-a00b5e72422e.jpeg)

## 典型的RAG流模式和实现
首先，让我们探索RAG流的突出模式，以及每个模板下的特定流，说明不同的模块和运算符是如何编排的。

在RAG Flow的上下文中，我们将为微调阶段描绘三个不同的流，为推理阶段描绘四个流。

### Tuning Stage
#### RetrieverFT
在RAG Flow中，微调检索器的常用方法包括：

+ 检索器的直接微调。构建用于检索和微调密集检索器的专用数据集。例如，使用开源检索数据集或基于特定领域的数据构建一个。
+ 添加可训练的适配器模块。有时，直接微调基于API的嵌入模型（例如OpenAIAda-002和Cohere）是不可行的。合并适配器模块可以增强数据的表示。此外，适配器模块有助于更好地与下游任务对齐，无论是针对特定任务（例如[PCRA](https://arxiv.org/pdf/2310.18347.pdf)）还是通用目的（例如[AAR](https://arxiv.org/abs/2305.17331)）。
+ LM-监督检索（LSR）。根据大模型生成的结果微调检索器。
+ 大模型奖励RL：仍然使用大模型输出结果作为监督信号。采用强化学习将检索器与生成器对齐。整个检索过程以生成马尔科夫链的形式拆解。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1706582361696-c967f803-36ec-404d-a1ad-0d443b30dfe8.png)

#### <font style="color:rgb(36, 36, 36);">GeneratorFT</font>
微调生成器的主要方法包括：

+ 直接微调。通过外部数据集进行微调可以为生成器补充额外的知识。另一个好处是能够自定义输入和输出格式。通过设置问答格式，大模型可以理解特定的数据格式并根据指令输出。
+ GPT-4蒸馏。在使用开源模型的本地部署时，一个简单有效的方法是使用GPT-4批量构建微调数据，以增强开源模型的能力。
+ 从大模型/人类反馈中强化学习。基于最终生成答案的反馈的强化学习。除了使用人类评估，GPT-4还可以作为评估法官。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1706582518351-c6891c9f-147d-41a6-89dc-2000c9210e2f.png)

#### <font style="color:rgb(36, 36, 36);">Dual FT</font>
在RAG系统中，同时微调检索器和发生器是RAG系统的一个独特特征。需要注意的是，系统微调的重点是检索器和发生器之间的协调。单独微调检索器和发生器属于前两者的组合，而不是Dual FT的一部分。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1706582560010-273ca486-8164-452e-bb64-24b942b033eb.png)

一个示例性实现是[RA-DIT](https://arxiv.org/abs/2310.01352)，它对大模型和检索器进行微调。LM-ft组件更新大模型以最大化给定检索增强指令的正确答案的似然，而R-ft组件更新检索器以最小化检索器分数分布和大模型偏好之间的KL-Diver度。

该框架采用本地Llama作为生成器，采用最先进的基于双编码器的密集检索器DRAGON+作为检索器。

在[REPLUG](https://arxiv.org/abs/2301.12652)之后，RA-DIT根据语言模型提示检索相关文本块。每个检索到的块都被添加到提示中，并且来自多个块的预测被并行计算并通过加权可能性集成以产生最终输出。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1706582620836-5a0f4047-29f3-42ce-a891-cbe40bc72cfb.png)

### <font style="color:rgb(36, 36, 36);">Inference Stage</font>
#### <font style="color:rgb(36, 36, 36);">Sequential</font>
RAG流的顺序结构将RAG的模块和运算符组织在线性的流水线中，如下图所示。如果它包括预检索和后检索模块类型，它代表典型的高级RAG范式；否则，它体现了典型的朴素RAG范式。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1706582732093-cc668416-468b-40e1-aa1e-d5f1dbe3af46.png)

目前使用最广泛的RAG管道是Sequential，它通常包括检索前的查询重写或HyDE和检索后的Rerank运算符，例如在[Qany](https://github.com/netease-youdao/QAnything)的情况下。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1706582751214-2cd3b646-c077-4238-89d4-0692dde10a8b.png)

[Rewrite-Retrieve-Read](https://arxiv.org/pdf/2305.14283.pdf)（RRR）也是典型的序列结构。查询重写模块是一个较小的可训练语言模型，在强化学习的背景下，重写器的最优化被形式化为马尔可夫决策过程，大模型的最终输出作为奖励。检索器利用稀疏编码模型BM25。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1706582761773-1cad76ee-9456-4be2-8acb-1e7967d0467d.png)

#### Conditional
具有条件结构的RAG Flow涉及根据不同的条件选择不同的RAG路径。通常，这是通过基于查询关键字或语义学确定路由的路由模块来完成的。

根据问题类型选择不同的路由，针对特定场景定向到不同的流。例如，当用户询问严肃的问题、政治事务或娱乐话题时，大型模型的答案容差各不相同。不同的路由分支通常在检索源、检索过程、配置、模型和提示方面有所不同。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1706582803690-417c32c4-cee5-4233-859c-68e8b44cfcec.png)

条件RAG的一个经典实现是[Semantic Router](https://github.com/aurelio-labs/semantic-router)。

#### <font style="color:rgb(36, 36, 36);">Branching</font>
具有分支结构的RAG Flow与条件方法的不同之处在于它涉及多个并行分支，而不是在条件方法中从多个选项中选择一个分支。在结构上，它可以分为两种类型：

+ 预检索分支（Multi-查询、并行检索）。这涉及扩展原始查询以获得多个子查询，然后对每个子查询进行单独检索。检索后，该方法允许基于子问题和相应的检索内容立即生成答案。或者，它可能涉及仅使用扩展的检索内容并将其合并到统一的上下文中进行生成。
+ 后检索分支（单查询，并行生成）。这种方法维护原始查询并检索多个文档块。随后，它同时使用原始查询和每个文档块进行生成，最后将生成的结果合并在一起。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1706583010306-2914ae62-3684-499b-bfa5-f37f3fb7c73c.png)

REPLUG体现了经典的后检索分支结构，其中对每个分支预测每个令牌的概率，通过加权可能性集成，聚合不同的分支，最终生成结果用于通过反馈微调检索器，称为Contriever。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1706583024590-1ce96d97-4f6f-4e3e-a2bc-205d1c9c2ece.png)

#### <font style="color:rgb(36, 36, 36);">Loop</font>
具有循环结构的RAG流是模块化RAG的一个重要特征，它涉及相互依赖的检索和推理步骤。它通常包括一个用于流控制的判断模块。这可以进一步分为迭代、递归和自适应（主动）检索方法。

+ Iterative Retrieval：有时，单一的检索和生成可能无法有效地解决需要广泛知识的复杂问题。因此，可以在RAG中使用迭代方法，通常涉及固定数量的检索迭代。迭代检索的一个示例性案例是[ITER-RETGEN](https://arxiv.org/abs/2305.15294)，它迭代检索-增强生成和generation-augmented检索，检索-增强生成基于所有检索到的知识输出对任务输入的响应，在每个迭代中，ITER-RETGEN利用前一个迭代的模型输出作为特定上下文来帮助检索更相关的知识，循环的终止由预定义的迭代次数决定。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1706583331850-89a4bdb8-043b-46b4-a3e3-b4994fa0cba2.png)

+ Recursive Retrieval：递归检索相对于迭代检索的特征是对上一步的明确依赖和不断深化检索，通常有一个终止机制作为递归检索的退出条件，在RAG系统中，递归检索通常涉及查询变换，依赖于每次检索新重写的查询。递归检索的典型实现，例如ToC，涉及递归执行RAC（递归增强澄清），以从初始歧义问题（AQ）逐渐插入子节点到澄清树中。在每个扩展步骤中，基于当前查询执行段落重排序以生成歧义问题（DQ）。树的探索在达到最大有效节点数或最大深度时结束。一旦澄清树被构造，ToC收集所有有效节点并生成一个全面的长文本答案来解决AQ。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1706583338646-bfb12eee-4b63-4a21-9243-b0f77338b995.png)

+ Adaptive(Active) Retrieval：随着RAG的发展，从被动检索逐渐转向自适应检索，也称为主动检索，这在一定程度上归功于大模型的强大能力。这与大模型代理有一个核心概念。RAG系统可以主动决定检索的时间，并决定何时结束整个过程并产生最终结果。根据判断标准，这可以进一步分为基于提示和基于调整的方法。
    - 基于提示的方法涉及使用提示工程来控制流程以指导大模型。一个典型的实现示例是FLARE。它的核心概念是语言模型应该只在缺乏基本知识时进行检索，以避免在增强的LM中进行不必要或不适当的检索。FLARE迭代生成下一个临时句子，并检查是否存在低概率tokens。如果找到，系统检索相关文档并重新生成句子。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1706583392388-84841d4b-1bfa-4009-8e36-4e6a94302a31.png)

    - Tuning-base。基于Tuning-base的方法涉及微调大模型以生成特殊tokens，从而触发检索或生成。这个概念可以追溯到Toolaker，其中特定内容的生成有助于调用工具。在RAG系统中，这种方法用于控制检索和生成步骤。一个典型的案例是Self-RAG。具体来说：
        * 给定一个输入提示和上一代结果，首先预测特殊令牌“检索”是否有助于通过段落检索增强继续生成。
        * 如果需要检索，模型生成：一个批评令牌来评估检索到的文章的相关性，下一个响应片段，以及一个批评令牌来评估响应片段中的信息是否得到文章的支持。
        * 最后，批评令牌评估响应的整体效用，并选择最佳结果作为最终输出。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1706583422567-962141d3-5be8-4c49-b975-65660735c958.png)

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1706583132584-9a216977-f12d-47c9-a472-d18852b23eb1.png)

# <font style="color:rgb(36, 36, 36);">Best Industry Case</font>
## OpenAI
在努力提升RAG成功的过程中，OpenAI团队从45%的准确率开始，试验了各种方法，确定了最终采用哪些方法进行生产。他们探索了假设文档嵌入（HyDE）、微调嵌入和其他方法，但结果并不令人满意。通过试验不同大小的信息块和嵌入不同的内容部分，他们能够将准确率提高到65%。通过重新排序和量身定制的方法来处理不同类型的问题，他们进一步将准确率提高到85%。最终，通过结合提示工程、查询扩展和其他方法，他们实现了98%的准确率。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1706583657269-d04b10ad-ab95-4fe8-9ee1-9e3475387306.png)

该团队强调了模型微调和RAG集成的强大潜力，特别是在不使用复杂技术的情况下，仅通过简单的模型微调和提示工程接近行业领先水平。

## [Baichuan](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw%3D%3D&mid=2650901201&idx=1&sn=3a9bd61403fb4b024ec5d8c128990495&scene=21#wechat_redirect)
百川从Meta的CoVe中汲取灵感，设计了一种将复杂提示解构为多个独立且可并行检索的搜索友好查询的方法。这使得大型模型能够对每个子查询进行有针对性的知识库搜索，从而提供更准确、更详细的答案并减少虚假输出。

此外，他们还利用专有的TSF（Think-Step Advanced）推断和挖掘用户输入背后更深层次的潜在问题，从而更精确、更全面地理解用户意图。虽然TSF的技术细节尚未披露，但据推测它是对Step-back提示方法的增强。

在检索步骤中，百川智能开发了Baichuan-Text-Embedding向量模型，在包含1.50万亿tokens的高质量中文数据上进行预训练。他们通过专有损失函数解决了对比学习中的批量依赖问题。这个向量模型已经超越了C-MTEB。

此外，他们还引入了稀疏检索和重新排序模型（未公开。），形成了一种混合检索方法，将向量检索与稀疏检索并行结合起来，显着将查全率/召回率提高到95%。

此外，他们还引入了自我批判，使大型模型能够根据提示、相关性和实用性对检索到的内容进行内省，并进行二次审查以选择最匹配和高质量的候选内容。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1706583710594-b5f5cb6e-3b7b-4283-8c7b-bb72ac222fa4.png)

鉴于整个百川RAG Flow中的众多分支以及缺乏具体披露，可以合理地推测，重新排序和选择需要重新排序和筛选所有材料，无论是从其他分支检索还是生成。

## Databricks
Database ricks作为大数据领域领先的服务商，在RAG设计方面一直保持着鲜明的特色和优势。

当用户输入问题时，系统从预处理的文本向量索引中检索相关信息，并结合提示工程来生成响应。上半部分，非结构化数据管道，遵循主流RAG方法，没有表现出任何特殊的唯一性。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1706583742803-bfe0c645-cb52-4513-bade-aafd30fd6c15.png)

下半部分是结构化数据管道，代表了Database ricks的特征工程流程，也是Database ricks 的RAG实施中最重要的方面。Database ricks利用其在大数据方面的专业知识，从其高度准确的数据存储中进行额外的检索，充分利用其在实时数据服务方面的优势。很明显，Database ricks在GenAI时代的战略是赋予RAG应用广泛的市场需求，将其强大的数据处理能力与生成人工智能技术相结合，构建一个集成的解决方案，并向客户推广这种统一的服务。






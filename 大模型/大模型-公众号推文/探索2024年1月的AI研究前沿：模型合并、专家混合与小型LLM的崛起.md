<font style="color:rgb(5, 7, 59);">2023年见证了大型语言模型（LLMs）在潜力和复杂性方面的迅猛发展。展望2024年，开源社区和研究领域的持续进步似乎预示着一个令人振奋的新阶段即将到来——我们有望在不扩大模型规模的前提下，实现模型性能的显著提升（即模型更加精简高效）。</font>

<font style="color:rgb(5, 7, 59);">在本月的文章中，着重推荐了四篇与这一主题紧密相连的论文。这些论文为我们提供了深入了解和探讨该领域的新视角和思路。</font>

+ **WARM: On the Benefits of Weight Averaged Reward Models：**<font style="color:rgb(5, 7, 59);">通过权重平均和模型合并，我们可以有效地将多个大型语言模型（LLMs）融为一体，形成一个性能更佳的模型。这种方法巧妙地避免了传统集成方法所带来的资源需求激增等典型缺陷，为模型的优化和提升提供了新的可能。</font>
+ **Tuning Language Models by Proxy：**<font style="color:rgb(5, 7, 59);">通过使用两个较小规模的大型语言模型（LLMs）进行代理调整，我们可以在不改变现有大型LLM权重的情况下，提升其性能表现。这种方法巧妙地利用了小型模型的调整和优化，为大型模型的性能提升提供了新的思路和途径。通过代理调整，我们能够实现更高效、更灵活的模型性能提升，为语言模型的应用和发展带来更大的潜力。</font>
+ **Mixtral of Experts：**<font style="color:rgb(5, 7, 59);">通过巧妙地结合多个较小的模块，我们可以构建出混合专家模型。这些模型在效率和效果上展现出了令人瞩目的表现，足以与它们的大型对应模型相媲美，甚至在某些情况下还能超越它们。这种方法的优势在于，它允许我们以更灵活、更高效的方式利用计算资源，同时实现了模型性能的提升。这为大型语言模型（LLMs）的发展和应用开辟了新的道路。</font>
+ **TinyLlama: An Open-Source Small Language Model：**<font style="color:rgb(5, 7, 59);">预训练一个拥有1.1亿参数的小型LLM不仅可以显著降低开发和运营成本，而且还能够为教育和研究应用提供全新的可能性。这样的模型在保持较高性能的同时，减少了对计算资源的需求，使得更多的机构和个人能够接触到先进的语言模型技术，推动相关领域的发展和创新。</font>

# 1. WARM: On the Benefits of Weight Averaged Reward Models
在[WARM: On the Benefits of Weight Averaged Reward Models (Jan 22)](https://arxiv.org/abs/2401.12187)论文中，研究人员提出了一种用于大模型奖励模型的权重平_均_方法。（奖励模型是指RLHF：[reinforcement learning with human feedback](https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives)）

什么是权重平均？由于LLMs的权重平均和模型合并似乎是2024年最有趣的主题，我想在进一步深入WARM论文之前简要介绍一下这个主题。

## **了解模型合并和权重平均**
<font style="color:rgb(5, 7, 59);">模型合并与权重平均，虽然在技术领域中并非新概念，但目前在开源大型模型排行榜上占据着主导地位。接下来，我们将简要探讨这两种技术的核心优势。</font>

<font style="color:rgb(5, 7, 59);">无论是模型合并还是权重平均，它们都致力于将多个模型或检查点融合成一个统一的实体。这种融合带来的优势显而易见：类似于模型集成的概念，通过将多个模型的力量汇聚在一起，不仅能够加速训练收敛过程，还可以显著提升模型的整体性能，并增强其稳健性。</font>

<font style="color:rgb(5, 7, 59);">然而，值得一提的是，与传统的集成方法相比，模型合并与权重平均存在一个显著的区别。传统集成方法通常需要维护多个独立的模型，而模型合并与权重平均则旨在生成一个单一的、更为强大的模型。这种差异使得模型合并与权重平均在实际应用中更具吸引力，因为它们能够在保持性能提升的同时，简化模型的维护和管理过程。</font>

<font style="color:rgb(5, 7, 59);">下图将为我们更直观地展示这一概念，帮助我们深入理解模型合并与权重平均的工作原理及其所带来的优势。</font>

![权重平均和模型合并（左）与绝对多数投票（右）等传统集成方法的比较。](https://cdn.nlark.com/yuque/0/2024/jpeg/406504/1707356402227-91889bf2-c7ea-4d87-8641-8148d9887a2b.jpeg)

<font style="color:rgb(5, 7, 59);">传统上，权重平均涉及在模型训练过程的不同时间点对单个模型的权重（参数）进行平均。通常，这是在训练接近尾声，模型几乎收敛时进行的。这种技术的一种常见形式是随机权重平均（SWA），在该方法中，我们衰减一个初始较大的学习率，并在衰减期间（但仍然相对较高）的学习率的几个迭代中对权重进行平均。</font>

<font style="color:rgb(5, 7, 59);">通过这种方法，模型能够利用训练过程中的多个权重状态，从而有可能获得更好的泛化性能和更平坦的优化空间。SWA已被证明在多种任务中都能提高模型的性能，尤其是在深度学习和神经网络领域。</font>

<font style="color:rgb(5, 7, 59);">权重平均是一种有效的技术，它结合了模型训练过程中的多个状态，以产生更稳健和性能更佳的模型。而随机权重平均（SWA）则是这一技术的一个具体实现，它通过在学习率衰减期间对权重进行平均来进一步提升模型性能。</font>

![随机权重平均（SWA）在训练周期结束时平均模型的权重](https://cdn.nlark.com/yuque/0/2024/png/406504/1707356402612-5334d1b5-363e-41f8-bc58-192bd482270c.png)

<font style="color:rgb(5, 7, 59);background-color:rgb(253, 253, 254);">由于模型在训练过程中的轨迹可能呈现不均匀性，因此，一种常见的策略是在学习率较低（尤其是当使用学习率调度程序时）且训练接近收敛的情况下，对模型进行平均。这种做法通常发生在训练的尾声阶段。另一种方法是指数移动平均（EMA），它通过指数递减旧状态的权重来计算权重的平滑版本，从而进一步稳定模型的性能。</font>

<font style="color:rgb(5, 7, 59);background-color:rgb(253, 253, 254);">近年来，权重平均技术取得了新的进展。2022年提出的最新权重平均（LaWA）方法表明，通过平均最近k个检查点的权重，且每个检查点在一个训练周期结束时进行，可以在损失和准确率方面显著加快训练速度，甚至可以减少几个训练周期的时间。这种方法在ResNet视觉模型和RoBERTa语言模型上均被证实有效。</font>

<font style="color:rgb(5, 7, 59);background-color:rgb(253, 253, 254);">随后，在2023年，针对大型模型预训练的高学习率问题，研究人员探索了LaWA的改进版本。这一新版本采用了更高的学习率，并在训练过程中更早地开始平均检查点。研究结果显示，这种方法在性能上明显优于标准的随机权重平均（SWA）和指数移动平均（EMA）技术。这一发现为大型模型的训练提供了新的思路和方法，有望进一步推动深度学习领域的发展。</font>

![早期加权平均的改进LaWA方法满足大模型预训练论文（https://arxiv.org/abs/2306.03241）的高学习率](https://cdn.nlark.com/yuque/0/2024/jpeg/406504/1707356403458-be066984-6066-4247-ac2a-51a8ca0d4905.jpeg)__

权重平均将同一模型的多个检查点组合成一个模型，而模型合并涉及将多个不同的训练模型组合成一个模型。这些模型中的每一个都可能是独立训练的，可能在不同的数据集上或任务上。

模型合并可以追溯到很久以前，但与LLMs相关的最新和最有影响力的论文可能是[Model Ratatouille: Recycling Diverse Models for Out-of-Distribution Generalization](https://arxiv.org/abs/2212.10445)。

这篇论文背后的想法是在各种不同的辅助任务中重用相同基本模型的多个微调迭代，如下图所示。

![](https://cdn.nlark.com/yuque/0/2024/jpeg/406504/1707356402849-1fc9bb1c-846b-4f1d-8a4f-ee774e0ee36f.jpeg)

为了提供更多细节，这个方法可以总结如下图所示。

![](https://cdn.nlark.com/yuque/0/2024/jpeg/406504/1707356403437-194aaa85-4dfd-45c8-8289-1b76040d6ad3.jpeg)

请注意，这种整体合并思想也可以应用于LoRA适配器，如[LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition](https://arxiv.org/abs/2307.13269)所示。

## **权重平均的奖励模型**
<font style="color:rgb(5, 7, 59);">在深入探讨了权重平均和模型合并的概念之后，我们简要回顾一下1月22日新发表的论文《</font>[<font style="color:rgb(5, 7, 59);">WARM： On the Obits of Weight Average Reward Models</font>](https://arxiv.org/abs/2401.12187)<font style="color:rgb(5, 7, 59);">》。这篇研究的核心目标是优化大型语言模型（LLM）的RLHF对齐步骤。</font>

<font style="color:rgb(5, 7, 59);">具体来说，研究人员试图通过平均微调奖励模型的权重来缓解LLM中的黑客奖励行为。这一方法有望为LLM的训练提供更稳定、更可靠的奖励信号，从而进一步提高模型的性能和对齐程度。</font>

> <font style="color:rgb(5, 7, 59);">黑客奖励： </font>当一个大模型学会操纵或利用其奖励系统的缺陷来获得高分或奖励，而没有真正完成预期任务或实现基本目标时，就会发生黑客奖励攻击。
>

![](https://cdn.nlark.com/yuque/0/2024/jpeg/406504/1707356403769-df0bcd94-3bda-4ead-b21f-6721c101fef5.jpeg)

为了解决黑客奖励（reward hacking），研究人员建议通过权重平均来组合大模型奖励模型。由这个过程产生的合并奖励模型比单个奖励模型获得了79.4%的胜率。

WARM是如何运作的？该方法相当简单：类似于随机权重平均，WARM平均多个模型（在本例中为奖励模型）的权重，如下图所示。

![](https://cdn.nlark.com/yuque/0/2024/jpeg/406504/1707356404194-02227a37-03b2-49d4-87ca-f82c334d1d1e.jpeg)

之前，我们讨论了几种权重平均方法。WARM如何准确地平均权重以获得奖励模型？在这里，他们使用简单线性的平均，就像随机权重平均一样。然而，不同之处在于模型不是从相同的轨迹中采样的，而是从预训练模型中独立创建的，就像在[Model Ratatouille](https://arxiv.org/abs/2212.10445)中一样。或者，WARM也有一个所谓的Baklava过程来沿着微调轨迹采样。下图中比较了差异。

![](https://cdn.nlark.com/yuque/0/2024/jpeg/406504/1707356405116-a6fbccd5-a7ef-4f0d-8951-142a470bb688.jpeg)

按照上述WARM程序并平均10个奖励模型，研究人员发现，RL策略WARM相对于具有单一奖励模型的策略具有79.4%的胜率，如下图所示。

![](https://cdn.nlark.com/yuque/0/2024/jpeg/406504/1707356405242-b34cf86b-e941-496b-8ec5-5f614377659c.jpeg)

## **结论**
<font style="color:rgb(5, 7, 59);">模型合并在机器学习领域并非新技术，但在大型语言模型（LLMs）的背景下，它展现出了特别的潜力和希望。这主要是因为LLMs的训练成本极高，且对资源的需求也非常密集。因此，任何能够有效利用训练过程中产生的多个LLMs的方法，都无需额外的工作，自然具有极大的吸引力。</font>

<font style="color:rgb(5, 7, 59);">模型合并正是这样一种方法，它能够将多个训练好的LLMs组合成一个更强大的模型，从而提高性能并降低对单个模型的依赖。与传统的集成方法相比，模型合并具有显著的优势。传统的集成方法需要同时运行多个模型，这不仅增加了计算成本，还使得推理过程变得复杂和耗时。而权重平均模型则相对较轻，它在推理期间的成本不会超过单个模型，因此更加高效和实用。</font>

# 2. Tuning Language Models by Proxy
<font style="color:rgb(5, 7, 59);">论文《Tuning Language Models by Proxy》介绍了一种颇具前景的大型语言模型（LLMs）优化技术，即代理调优。这种独特的方法能够在不改变模型权重的情况下，对LLMs进行微调。</font>

<font style="color:rgb(5, 7, 59);">代理调优的工作原理非常直接，它在解码阶段通过调整目标LLM的logits来实现。具体来说，它首先计算一个较小基础模型与经过微调的模型之间logits的差异。然后，将这个差异添加到目标模型的logits上。（Logits是模型最后一层生成的原始输出值。在通过softmax等函数转换为概率之前，这些logits代表了LLMs词汇表中每个可能输出标记的未归一化得分。）通过这种方法，代理调优能够有效地提升大型语言模型的性能，为其带来更广泛的应用前景。</font>

![](https://cdn.nlark.com/yuque/0/2024/jpeg/406504/1707356405426-8dc5d1a0-fc68-4f6a-9d45-af4e655492c8.jpeg)

<font style="color:rgb(5, 7, 59);">为了更清晰地阐述这一概念，我们假设要优化一个大型目标模型M1（例如Llama 2 70B）。这一过程中涉及两个较小的模型：一个是小的基础模型M2（如Llama 2 7B），另一个是基础模型的微调版本M3（如Llama 2 7B Chat）。</font>

<font style="color:rgb(5, 7, 59);">优化过程的关键在于，我们将这两个较小模型预测结果的差异（即对数几率）应用于目标模型M1。由此，改进后的目标模型M1</font>_<font style="color:rgb(5, 7, 59);">的输出对数几率计算公式为：M1</font>_<font style="color:rgb(5, 7, 59);">(x) = M1(x) + [M3(x) - M2(x)]。在得到这些输出对数几率后，我们通过softmax函数将其转换为概率值。最终，这些概率值将用于通过核抽样或top-k解码等方式，生成最终的文本输出。</font>

<font style="color:rgb(5, 7, 59);">通过这样的优化过程，我们能够在不改变原模型权重的情况下，有效地提升大型语言模型的性能表现。</font>

## **代理调优在实践中的效果如何？**
<font style="color:rgb(5, 7, 59);">实验结果令人印象深刻。研究人员在三种不同的场景中应用了他们的方法：</font>

1. <font style="color:rgb(5, 7, 59);">指令调整：改进了70B大小的Llama 2基础模型，以使其性能与Llama 2 70B聊天模型相匹配。通过这一调整，基础模型在理解和执行指令方面取得了显著进步。</font>
2. <font style="color:rgb(5, 7, 59);">领域适配：在编码任务中对70B大小的Llama 2基础模型进行了升级，旨在达到CodeLlama 70B的性能水平。经过适配，该模型在编程领域的表现得到了大幅提升。</font>
3. <font style="color:rgb(5, 7, 59);">任务特定微调：针对特定任务，如TriviaQA或数学问题，对70B大小的Llama 2基础模型进行了改进。通过微调，模型在这些专门任务上的性能得到了显著优化。</font>

<font style="color:rgb(5, 7, 59);">在每个场景中，与原始基础模型相比，都观察到了显著的改进。为了简洁起见，下表主要关注Llama 70B基础模型和聊天模型之间的比较。然而，本文还为CodeLlama提供了额外的性能基准，以全面评估其在不同场景下的表现。这些结果表明，研究人员的方法在多种场景下都具有广泛的应用潜力和实用价值。</font>

![](https://cdn.nlark.com/yuque/0/2024/jpeg/406504/1707356406031-d27ef913-3c3e-4500-a8ac-e12398bb6d4f.jpeg)

可以看出，基于上图所示的基准测试，代理调整的70B Llama 2模型比70B基础模型性能好得多，几乎与直接调整的Llama 70B聊天模型一样好。

## **实际考虑**
<font style="color:rgb(5, 7, 59);">采用这种方法的动机主要是为了提高研发效率。具体来说，开发人员可以首先在小模型上测试新的训练策略或模型增强方法，以降低成本和风险。一旦验证有效，这些方法便可以无缝扩展到大型基础模型上，无需从头开始训练大型模型。这种策略能够显著缩短开发周期，提高研发效率。</font>

<font style="color:rgb(5, 7, 59);">然而，要在实际环境中实施这种方法，我们需要考虑使用三种不同的模型：</font>

+ <font style="color:rgb(5, 7, 59);">首先是大型通用基础模型，它提供了广泛的预训练能力；</font>
+ <font style="color:rgb(5, 7, 59);">其次是较小的通用模型，用于在更小的规模上快速测试和验证新的方法；</font>
+ <font style="color:rgb(5, 7, 59);">最后是多个针对特定用例或客户需求定制的小型专用模型，它们能够确保在新的应用场景中取得最佳性能。</font>

<font style="color:rgb(5, 7, 59);">那么，为什么我们要选择这种方法，而不是采用LoRA（低秩自适应）这样的技术呢？LoRA技术不需要较小的通用模型，并且可以用一组小型的LoRA矩阵来替代多个小型专用模型。尽管如此，代理调优方法仍然具有两个潜在的优势：</font>

1. <font style="color:rgb(5, 7, 59);">首先，在某些特定场景下，代理调优方法可能会比LoRA表现出更好的性能。虽然目前还没有直接的比较数据可用，但代理调优方法通过在小模型上进行测试和优化，有可能在大模型上实现更高的性能提升。</font>
2. <font style="color:rgb(5, 7, 59);">其次，当大型基础模型是一个“黑匣子”，并且其内部权重无法访问时，代理调优方法将变得非常有用。在这种情况下，我们无法直接对大型基础模型进行修改或优化，但可以通过调整小模型的输出来间接地影响大模型的性能。这为那些无法直接访问大型模型内部权重的开发人员提供了一个有效的解决方案。</font>

<font style="color:rgb(5, 7, 59);">然而，需要注意的是，使用这种方法时较小的模型必须与较大的目标模型共享相同的词汇表。这意味着在开发过程中需要确保所有模型都使用一致的数据预处理和词汇编码方式。这在理论上为我们提供了一个有趣的可能性：如果我们能够知道GPT-4等先进模型的词汇表，并且能够访问其输出对数几率（logits），那么我们就有可能使用这种方法创建出专门针对这些先进模型进行优化的定制模型。</font>

# 3. Mixtral of Experts
<font style="color:rgb(5, 7, 59);">Mixtral 8x7B论文重磅发布！这款引领风潮的模型终于面世。Mixtral 8x7B是一种卓越的稀疏混合专家（Sparse Mixture of Experts，简称Sparse MoE）模型，目前在公开可用的大型语言模型（Large Language Models，简称LLMs）中脱颖而出，以出色的性能和广泛的应用前景备受瞩目。该模型存储库以Apache 2许可证发布，这意味着无论是学术研究还是商业应用，您都可以免费使用这一强大的工具。</font>

<font style="color:rgb(5, 7, 59);">那么，什么是MoE呢？MoE，即混合专家模型，是一种集成模型，它巧妙地融合了多个较小的“专家”子网络。每个子网络都擅长处理特定类型的任务或更具体的标记（tokens）。通过采用多个小型子网络而非单一大型网络的设计，MoE能够更高效地分配计算资源，从而实现更出色的扩展性，并在更广泛的任务范围内潜在地获得卓越性能。</font>

<font style="color:rgb(5, 7, 59);">在Mixtral的专家论文中，作者们深入探讨了Mixtral 8x7B的构建过程。这款模型在与规模更大的Llama 2 70B模型的比较中表现出色，充分展现了其卓越的性能和潜力。</font>我将在下面讨论，作者讨论了他们如何构建Mixtral 8x7B。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1707356406297-b7096ffe-0085-4912-846f-11fa250c74fd.png)

## **Mixtral Architecture**
Mixtral 8x7B的关键思想是用8个专家层替换Transformer架构中的每个**feed-forward**模块，如下图所示。

![](https://cdn.nlark.com/yuque/0/2024/jpeg/406504/1707356406620-ac492d53-bfce-40f1-af3e-fa953a54ed27.jpeg)

feed-forward模块本质上是一个多层感知器。在PyTorch机器学习库的伪代码中，它本质上看起来像这样：

```python
class FeedForward(torch.nn.Module):

    def __init__(self, embed_dim, coef):

        super().__init__()

        self.layers = nn.Sequential(

            torch.nn.Linear(embed_dim, coef*embed_dim),

            torch.nn.ReLU(),

            torch.nn.Linear(coef*n_embed, embed_dim),

            torch.nn.Dropout(dropout)

        )

    def forward(self, x):

        return self.layers(x)
```

此外，还有一个路由模块（也称为门控网络），它将每个令牌重定向到8个专家联邦学习模块。然后将这8个专家feed-forward层的输出相加，如下图所示。

![](https://cdn.nlark.com/yuque/0/2024/jpeg/406504/1707356406987-e0fd5475-6141-49f6-89c9-bfbf801a223f.jpeg)

将上述转化为数学公式，可以表述为下面形式：

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1707359787543-ca013248-009c-4eb7-8ad8-2fc818f775fd.png)

这里，G代表路由器，Ei是专家模块的输出，基于上面的等式，MoE层计算专家输出Ei的加权和，其中权重由选通网络G(x)提供。

乍一看，Mixtral似乎只是通过这些专家模块向大模型添加额外的参数，以表示一种加权集成方法。然而，还有一个额外的调整： Mixtral是一个稀疏MoE，这意味着每个输入只使用专家的子集：

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1707359866823-6cc982ad-0925-4fda-a505-853ea3cb80f3.png)

在Mixtral 8x7B的具体情况下，作者指定TopK=2，这意味着一次只使用2个专家。因此，基于上述等式，_G（x）_的输出可能如下所示：[0, 0, 0.63, 0, 0, 0.37, 0, 0]。这表明第三个专家对输出贡献了63%，第六个专家贡献了37%。

## **Model Size**
<font style="color:rgb(5, 7, 59);">Mixtral 8x7B这一名称的来源，以及其作为稀疏MoE模型的实际大小，都是基于其独特的结构和设计。名称中的“8x”指的是模型使用了8个专家子网络，而“7B”则表明它融合了Mistral 7B模块。但需要澄清的是，Mixtral 8x7B的大小并不是简单地通过8乘以7B来计算的，即它并不等于56B。实际上，这里的70亿参数是指整个Mistral 7B模型的规模。然而，在Mixtral 8x7B中，只有前馈层被专家层所替代。</font>

<font style="color:rgb(5, 7, 59);">具体来说，Mixtral 8x7B总共包含470亿个参数。这意味着，如果从一个Mistral 7B模型中减去前馈层的参数，剩下的非前馈层参数大约有90亿个。这一发现相当有趣，因为它揭示了在大型语言模型（LLM）中，大部分参数实际上都集中在前馈模块，而不是注意力机制中。</font>

<font style="color:rgb(5, 7, 59);">尽管拥有470亿个参数，Mixtral 8x7B在规模上仍然显著小于某些其他模型，例如Llama 2 70B模型。此外，由于在每个时间步上只有2个专家是活跃的，因此该模型在处理每个输入标记时实际上只使用了约130亿个参数。这使得Mixtral 8x7B在效率上远胜于常规的、非MoE结构的470亿参数模型。</font>

![](https://cdn.nlark.com/yuque/0/2024/jpeg/406504/1707356407064-5af16d8a-82f0-47eb-b3a3-eae592823ca6.jpeg)

## **Expert Specialization**
<font style="color:rgb(5, 7, 59);">一个有趣的问题是，这些专家是否展现出针对特定任务或令牌的模式。然而，遗憾的是，作者们未能按照主题（如GitHub、Arxiv、数学、维基百科等数据集）观察到这种专业化分工。</font>

<font style="color:rgb(5, 7, 59);">尽管如此，作者们还是发现了一个有趣的现象：在文本数据集中，连续的令牌往往被分配给相同的专家。此外，在Python代码中，缩进令牌也经常被分配给同一位专家，如下图所示。这一现象或许揭示了模型在处理文本数据时的一种内在机制或策略。</font>

![](https://cdn.nlark.com/yuque/0/2024/jpeg/406504/1707356407306-fb0bd304-6506-4343-bbbc-480fa6623d40.jpeg)

## **Conclusion**
<font style="color:rgb(5, 7, 59);">Mixtral 8x7B模型拥有多项显著优势：它公开可用，性能与Llama 2 70B等大型模型相当甚至更优，并且采用了一种相对新颖（尽管并非全新）的方式来构建大型语言模型（LLM），即稀疏MoE模块。</font>

<font style="color:rgb(5, 7, 59);">该模型强大的性能、高效的参数利用以及处理长达32k上下文窗口的能力，使其在可预见的未来（至少在未来几个月内）成为备受瞩目的模型。我相信，在2024年，MoE模型将成为大多数开源项目的主要关注领域之一，因此Mixtral of Experts值得持续关注。</font>

<font style="color:rgb(5, 7, 59);">然而，有一点小遗憾是，作者并未分享关于训练数据集的任何信息（这可能是为了避免版权争议，可以理解）。尽管如此，我们仍然期待看到更多关于该模型的研究和应用。</font>

<font style="color:rgb(5, 7, 59);">此外，尽管相关研究成本高昂，但我们仍然对Mixtral 8x70B与在同一数据集上训练的Llama 2 70B模型之间的性能比较充满好奇。同时，我也希望在未来能够看到Mixtral 8x70B与以下两个假设模型的比较结果，以便更直接地评估MoE和非MoE方法的性能差异：</font>

1. <font style="color:rgb(5, 7, 59);">Mistral 56B（一个更大的非MoE模型）</font>
2. <font style="color:rgb(5, 7, 59);">Mistral 47B（一个与Mixtral 8x70B参数数量相同的非MoE模型）</font>

<font style="color:rgb(5, 7, 59);">值得一提的是，Brave浏览器现已将Mixtral 8x7B作为其Leo助手功能的默认大型语言模型。这一事实进一步证明了该模型的实用性和广泛应用前景。</font>

# 4. TinyLlama: An Open-Source Small Language Model
<font style="color:rgb(5, 7, 59);">继微软phi-2在去年12月掀起热潮之后，TinyLlama作为“小型”大型语言模型（LLM）家族的新成员闪亮登场。TinyLlama不仅规模小巧，仅包含11亿参数，还实现了完全开源。这里的“开源”意味着，其训练代码和模型检查点均可通过无限制的开源库轻松获取。感兴趣的开发者可访问GitHub存储库进行探索：</font>[https://github.com/jzhang38/TinyLlama。](https://github.com/jzhang38/TinyLlama%E3%80%82)

<font style="color:rgb(5, 7, 59);">那么，究竟小型LLM（亦称SLM，即小型语言模型）有何魅力，能吸引如此多的关注呢？它们的优势在于：</font>

1. <font style="color:rgb(5, 7, 59);">便捷性与经济性：小型LLM可在资源有限的环境（如笔记本电脑或小型GPU）中顺畅运行（推理模式），无需高昂的硬件成本。</font>
2. <font style="color:rgb(5, 7, 59);">开发与预训练成本低：相较于大型模型，小型LLM所需的GPU数量大幅减少，从而降低了开发与预训练的门槛。</font>
3. <font style="color:rgb(5, 7, 59);">易于针对特定任务定制：小型模型通常可在单个GPU上进行微调，更便于满足特定应用场景的需求。</font>
4. <font style="color:rgb(5, 7, 59);">节能环保：鉴于大规模AI模型在训练和运行过程中对环境的影响，小型LLM的节能特性显得尤为重要。此外，在智能手机等便携式设备上部署时，电池续航能力也是一大考量因素。</font>
5. <font style="color:rgb(5, 7, 59);">教育价值突出：由于其规模较小、更易于管理，因此小型LLM更便于学生和研究人员理解、修改和学习。</font>

## **TinyLlama Performance**
除了小型和开源之外，与其他类似规模的开源模型相比，TinyLlama在常识推理和问题解决基准上的表现也相对较好。

![](https://cdn.nlark.com/yuque/0/2024/jpeg/406504/1707356407422-0caecaab-5e1d-4a87-bd49-26dc3ea5dc4c.jpeg)

当然，TinyLlama在这些基准测试中无法与更大的模型相媲美，但由于所有代码都是开源的，它为进一步研究和微调提供了有趣的机会。

## **TinyLlama Learnings**
<font style="color:rgb(5, 7, 59);">例如，作者在训练过程中有一个有趣的教育性发现：对包含1万亿tokens的模型进行3个epoch的训练实际上是有益的。尽管这与Chinchilla </font>scaling laws<font style="color:rgb(5, 7, 59);">矛盾，因为根据这些定律，对于这样大小的模型，建议使用的数据集应该要小得多。然而，这一发现表明，在训练大型语言模型时，可能需要根据具体情况灵活调整训练策略，而不是严格遵循既定的</font>scaling laws<font style="color:rgb(5, 7, 59);">。</font>

![](https://cdn.nlark.com/yuque/0/2024/jpeg/406504/1707356408083-4c3df864-a8f9-48ca-973e-e11f4bf5b491.jpeg)

例如，如下图所示，即使数据通过多个时期的训练重复，模型仍在不断改进。

![](https://cdn.nlark.com/yuque/0/2024/jpeg/406504/1707356408217-e6966b3b-96ea-42d0-9eaa-4fe9371d257b.jpeg)

<font style="color:rgb(5, 7, 59);">对于大型模型而言，在超大数据集上进行训练绝非易事。然而，尽管如此，依然对未来TinyLlama的微调实验结果充满期待。尽管初步实验显示，TinyLlama目前相较于规模虽小但参数多三倍的phi-2模型仍有所不足，但我相信未来仍有改进和突破的空间。</font>


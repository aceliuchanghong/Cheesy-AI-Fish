### <font style="color:rgb(63, 63, 63);">定义一个新的NLP游乐场</font>
### <font style="color:rgb(63, 63, 63);">李沙 1, 吴智涵 1, 余朋飞 1, 卡尔·爱德华兹 1, 李漫玲 2, 王星遥 1, 冯毅仁 1, 于学 1, 特雷沃尔 3, 哈弗 4, 蒋恒 1</font>
<font style="color:rgb(29, 33, 41);">伊利诺伊大学厄巴纳-香槟分校 2 西北大学</font>

<font style="color:rgb(29, 33, 41);">3 Dataminr，Inc. 4墨尔本大学</font>

<font style="color:rgb(29, 33, 41);">{shal2, chihan3, pengfei4, cne2, xingyao6, yifung2, ctyu2, hengji}@illinois.edu</font>

[manling.li@northwestern.edu](mailto:manling.li@northwestern.edu)<font style="color:rgb(29, 33, 41);">,</font><font style="color:rgb(29, 33, 41);"> </font>[jtetreault@dataminr.com](mailto:jtetreault@dataminr.com)<font style="color:rgb(29, 33, 41);">,</font><font style="color:rgb(29, 33, 41);"> </font>[eduard.hovy@unimelb.edu.au](mailto:eduard.hovy@unimelb.edu.au)

### <font style="color:rgb(63, 63, 63);">摘要</font>
<font style="color:rgb(29, 33, 41);">最近大型语言模型（LLMs）性能的爆炸式增长，比自然语言处理（NLP）领域80年历史上的任何其他转变都更突然、更地震般地改变了该领域。这导致人们担心该领域会变得同质化和资源密集型。这种新的常态使许多学术研究人员，尤其是博士生处于不利地位。本文旨在通过提出20多个论文水平的研究方向来定义一个新的NLP游乐场，涵盖理论分析、新问题和挑战、学习范式以及跨学科应用。</font>

### <font style="color:rgb(63, 63, 63);">1. 简介</font>
<font style="color:rgb(29, 33, 41);">这是最好的时代。 这是最坏的时代。 我们生活在一个令人难以置信的令人兴奋但又陌生的自然语言处理 (NLP) 研究时代，由于最近在各种数据模态上大型语言模型 (LLMs) 的发展，从自然语言（Brown等人，2020）和编程语言（Chen等人，2021；王等人，2023a）到视觉（Radford等人，2021；李等人，2022a；王等人，2022b）以及分子（Edwards等人，2022；曾等人，2022；苏等人，2022）。</font>

<font style="color:rgb(29, 33, 41);">从根本上讲，LLMs通过根据上下文计算条件概率来逐字生成文本序列。在足够大的规模下，它们可以回答问题、生成论点、写诗、扮演角色、协商合同，并在一系列标准NLP任务中取得竞争性结果，包括实体类型识别、情感分析和文本蕴含，展示了诸如基于上下文的学习（Wei等人，2022）等“涌现行为”。</font>

<font style="color:rgb(29, 33, 41);">然而，这一“突破时刻”在自然语言处理研究社区中引发了两极分化：虽然一些人对这一进展表示欢迎，</font>

<font style="color:rgb(29, 33, 41);">其他人感到困惑。为什么NLP对单一进步如此敏感？</font>

<font style="color:rgb(29, 33, 41);">回过头来看，当自然语言处理在 20 世纪 90 年代采用机器学习范式时，它开始了一段导致更大同质性的旅程。主导的方法成为：（1）确定一个挑战性问题或任务；（2）创建一组期望的输入输出示例；（3）选择或定义一个或多个评估指标；以及（4）开发、应用和改进机器学习模型和算法以提高性能。</font>

<font style="color:rgb(29, 33, 41);">如果挑战不支持数据集（例如，不同职业的人们的文本风格）或指标（例如，小说或电影的摘要）的创建，或者更糟糕的是，它不适合机器学习解决方案，那么主流NLP 简直就是没有触及。很长一段时间以来，NLG 都处于这种境地，因为它的起点——语义表示法——既不是标准化的，也不是容易大规模产生的，也不是可以直接评估的。没有数据集，没有指标——很少关注。然而，从深度语义输入开始，针对不同的受众定制输出的多句NLG 可能是NLP 中最复杂的任务，因为它涉及语言交际的许多方面。因此，它理应得到过去的NLP 对MT、语音识别、QA 和其他主要挑战所付出的巨大努力。</font>

<font style="color:rgb(29, 33, 41);">突然之间，几个月内，形势发生了变化。NLP 遇到了一个似乎可以做该领域几十年来一直在做的所有事情的引擎。NLP 的许多子任务似乎在一夜之间变得无关紧要：解析哪个语法形式？哪种修辞结构和焦点控制模型适用于多句话的一致性？哪种神经架构最适合信息提取或摘要？如果神奇的引擎能够无缝地完成端到端的语言到语言任务（Sanh等人，2022；OpenAI，2023），那么这些都不重要。</font>

**<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);">第</font>****<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);"> </font>****<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);">2</font>****<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);"> </font>****<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);">页</font>**

<font style="color:rgb(29, 33, 41);">数十篇博士论文因其观点被视为过程中的一个微小步骤而失去了意义，这个步骤似乎不再需要。主导范式也受到挑战：人们开始发现这些模型的新功能，而不是先建立基准，然后相应地开发模型（Bubeck等人，2023年）。（谁知道LLMs可以使用TikZ画独角兽？）</font>

<font style="color:rgb(29, 33, 41);">一个重要的限制是目标的实用性。这一代新的语言模型超出了除少数自然语言处理研究人员以外的所有人的实际使用范围。除非某个正在构建语言模型的组织为研究提供免费访问——考虑到运行一个模型每月估计六位数的费用，这种情况不太可能发生——或者开发出一种廉价的方法来构建大学规模的语言模型，学术界的自然语言处理社区必须在识别原则上无法生成的语料库或可以不重新训练就构建的应用程序方面具有很强的创造力，同时这些应用程序在实践中也是重要且可行的。</font>

<font style="color:rgb(29, 33, 41);">受一群博士生（Ignat等人，2023年）的工作启发，我们认为定义新的研究路线图将是一项有价值的练习。我们相信，虽然 LLMs 在表面上关闭了研究途径，但它们也打开了新的途径。当前的 LLMs 在某种程度上仍然是相当单一、昂贵、健忘、虚幻、缺乏创造力、静态、专横和有偏见的黑匣子。它们在获取某些类型的知识（Wang 等人，2023 f）、知识推理和预测方面仍然存在令人惊讶的缺陷（近似随机性能）。在这篇论文中，我们的目标是通过提出一系列博士论文水平的研究方向来重新民主化自然语言处理 (NLP) 研究。具体而言，我们在 LLM 理论（第 2 部分）、具有挑战性的新任务（第 3 部分）、重要但尚未得到充分研究的学习范例（第 4 部分）、适当的评估（第 5 部分）以及跨学科应用（第 6 部分）的角度提供了观察和建议。</font>

### <font style="color:rgb(63, 63, 63);">2. LLM 的理论分析</font>
<font style="color:rgb(29, 33, 41);">通过理论分析来打开机器学习模型的黑匣子变得越来越必要。在这一部分，我们提倡数学（通过数学分析）和实验（从大量的观察中推导规则和定律，例如Ghorbanial 等人 (2021)，Hoffmann 等人 (2022)）理论。</font>

<font style="color:rgb(29, 33, 41);">LLM.</font>

### <font style="color:rgb(63, 63, 63);">2.1 崭新能力背后的机制</font>
<font style="color:rgb(29, 33, 41);">语言模型已经展示了令人印象深刻的新兴能力，如指令遵循、链式思维推理和上下文学习（Brown等人，2020；Wei等人，2022；Min等人，2022；Wei等人；Logan IV等人，2022；Wei等人，2021）。例如，指令遵循的能力使模型能够遵循新指令。为了在启发式之外提供提示指导，我们需要全面了解指令的工作原理。一些初步理论表明，可以通过贝叶斯推断进行解释（Jiang，2023），这依赖于强烈的假设而没有实际见解。在这里，我们主张关于约束或测量模型偏离指令可行性的理论。多人设置也很重要，在多人设置中，一个用户的消息会与另一个玩家的消息（比如OpenAI的隐藏元指令）组合在一起，然后再输入到语言模型中，这样第一个用户可能会出现额外的安全问题。</font>

<font style="color:rgb(29, 33, 41);">连锁思维（Chain-of-thought，CoT）推理是指语言模型通过按顺序、逐步生成解决方案来解决复杂任务。理论上来讲，CoT可以增强基于transformer的语言模型解决超过O(n^2)复杂度问题的计算能力。虽然有人提出了有建设性的解释（Feng等人，2023a），但它们尚未得到充分验证作为潜在机制。重要的是，值得研究链式推理的可证性问题（是否可以将CoT视为有效的逻辑链）以及其校准（LLM是否为任意结论制定特定的CoT）。</font>

<font style="color:rgb(29, 33, 41);">在上下文学习 (ICL) 中，LLMs 在不更新参数的情况下从上下文示例中学习，这得到了基于梯度下降（Akyürek et al.，2022；von Oswald et al.，2022）、核回归（Han et al.，2023a）或贝叶斯推断（Xie et al.；Jiang，2023；Wang et al.，2023d）的解释。仍然存在重要的挑战，并需要更全面的解释，例如对示例顺序的敏感性和对扰动输入输出映射的鲁棒性。我们假设深入了解如何平衡算法解决方案与隐式语言推理可以帮助澄清这些问题，这可能可以通过探索如何解耦语义和功能信息来实现。</font>

**<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);">第</font>****<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);"> </font>****<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);">3</font>****<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);"> </font>****<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);">页</font>**

<font style="color:rgb(29, 33, 41);">模型特定与模型无关是在解释中普遍存在的差距，这引发了这样一个问题：新兴能力是否依赖于变压器架构还是仅仅适应预训练数据。随着一些最近的研究表明其他架构在某些领域取得了相当的表现（彭等人，2023；翟等人，2021），这个问题对于在模型设计（包括其他架构）、工程和简单地仔细收集更大的数据集之间进行优先排序非常重要。为了弥合这一差距，我们还提倡超越HMM（混合）的理论框架来更好地建模语言数据属性。</font>

### <font style="color:rgb(63, 63, 63);">2.2 理论鲁棒性和透明度</font>
<font style="color:rgb(29, 33, 41);">鲁棒性是指确保模型中不会轻易实现后门设计或对抗使用。虽然从定义上来说，这不是一个新问题，但在 LLM 时代，这个问题具有新的含义和形式化。在大多数用户无法访问预训练和模型编辑细节的情况下，我们呼吁对任意给定的 LLM 进行鲁棒性诊断研究。尽管有证据表明，在某些条件下防止对抗提示可能几乎是不可能的（Wolf 等人，2023 年），但我们持乐观态度，并希望在更现实的条件下，例如搜索对抗提示时计算复杂度高，这可能会被潜在地推翻。</font>

<font style="color:rgb(29, 33, 41);">在 LLM 中，透明性关注模型的自我解释与内部计算推理之间的对齐。由于实证研究表明，LLM 并不一定总是能够准确地表达其“想法”（Turpin 等人，2023），因此有必要进行计算建模来理解 LLM 的意图。追求透明度对于防止 LLM 生成误导性的理由以欺骗人类至关重要。我们主张在不同条件下建立关于如何对抗虚假理由的正反定理，并检查特定架构中“忠诚”模式与神经元活动之间的关联。</font>

### <font style="color:rgb(63, 63, 63);">3 新任务，新挑战</font>
### <font style="color:rgb(63, 63, 63);">3.1 知识获取与推理</font>
<font style="color:rgb(29, 33, 41);">LLM中的知识 LLM的黑匣子特性在评估内部隐含的知识时带来了重大挑战。</font>

<font style="color:rgb(29, 33, 41);">模型。初步研究已经进行了识别（Cohen等人，2023；Shin等人，2020；Petroni等人，2019年、2020年；Fung等人，2023年；Gudibande等人，2023年；Li等人，2023c）以及本地化编辑知识（Dai等人，2021；Meng等人，2022a、b；Zhu等人，2020；Mitchell等人，2022a；De Cao等人，2021；Hase等人，2023；Meng等人，2022a；Mitchell等人，2022b）。然而，我们对语言模型中知识组织的理解仍然有限（知识存储在哪里以及如何存储），而且目前尚不确定是否能够完全理解。此外，现有研究主要关注事实或常识性知识，而忽视了更复杂的推理规则等知识（Boolos等人，2002）。</font>

<font style="color:rgb(29, 33, 41);">大规模知识推理 LLM 在适当的提示下，在各种推理任务（Dua 等人，2019 年；Miao 等人，2020 年；Cobbe 等人，2021 年；Yu 等人，2020 年；Bhagavatula 等人，2020 年；Talmor 等人，2019 年）中表现出有希望的表现，例如通过使用连锁思维和改进的连锁思维（Wei等人；Chowdhery 等人，2022 年；Xue 等人，2023 年；Diao 等人，2023 年；Wang 等人，2023e；Paul 等人，2023 年）或程序思维（Chen 等人，2022 年）。然而，当前的推理基准测试（Cobbe 等人，2021 年；Ling 等人，2017 年；Patel 等人，2021 年；Hosseini 等人，2014 年；Miao 等人，2020 年；Koncel-Kedziorski 等人，2016 年；Talmor 等人，2019 年；Geva 等人，2021 年）侧重于处理小规模语境，通常由数百个单词组成。这种推理水平在应对复杂任务时捉襟见肘，例如科学研究，它需要从大量相关文献和特定领域的知识库中获取知识。检索增强（Guu 等人，2020 年；Khandelwal 等人，2020 年；Borgeaud 等人，2022 年；Izacard 等人，2022 年；Lai 等人，2023b）是一种强大的工具，用于将大规模上下文知识整合到语言模型中。然而，目前的检索方法主要依赖于语义相似性，而人类则具有适应学习（Illeris，2018 年）的能力，可以从语义上不相似的知识中汲取灵感，并将其转移到目标任务中。要做到这一点，我们不仅需要延长输入语境的长度，还需要了解模型如何组织知识并开发更有效的知识表示和评估指标（第 5 节）。</font>

**<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);">第</font>****<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);"> </font>****<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);">4</font>****<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);"> </font>****<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);">页</font>**

<font style="color:rgb(29, 33, 41);">保证生成输出的真实性需要最优地利用模型内的知识和外部的知识，包括输入上下文、知识库和开放网络资源。访问外部知识通常依赖于信息检索的成功（Lewis等人，2020；何等人，2023；余等人，2023c，b），信息提取（Wen等人，2021；黄等人，2023），基于语境的生成（Li等人，2021,2022b；Gao等人，2023a；韦勒等人，2023；Lai等人，2023a）和知识增强型生成（Petroni等人，2020；Geva等人，2023）。内部知识涉及存储在模型中的隐式参数化知识，其校正和修正仅限于推断阶段（Lee等人，2022；孟等人，2022a，b；陈等人，2023a）。为了有效地减少幻觉并纠正事实错误，不仅要破解知识如何通过模型参数模式进行解释，还要了解模型是如何组合知识并在生成过程中控制底层逻辑的。知识引导生成的一个重要挑战是定义一个适当的知识表示方法来支持复杂结构和分布式表示。我们认为这种表示应该结合符号推理的优势以最小化不合理的推论，并具有分布式表示的灵活性以编码任何语义粒度。从虚假信息检测和知识比较推理系统中获得的见解也可以是有用的信号维度，用于提高忠诚度和真实性（刘等人，2021a； fung等人，2021；吴等人，2022, 2023）。</font>

### <font style="color:rgb(63, 63, 63);">3.2 创意生成</font>
<font style="color:rgb(29, 33, 41);">尽管人们长期以来一直设想使用模型进行创意写作，但直到最近，当语言生成模型能够可靠地产生连贯文本时，这才成为现实。与以前的部分不同，其中生成的文本充当知识载体，创意用例更关注语言的风格或形式，并鼓励开放式输出。[1]</font>

<font style="color:rgb(29, 33, 41);">创意写作帮助 由于语言模型具有生成能力，因此可以为——</font>

<font style="color:rgb(29, 33, 41);">盒子，它们已经被创意行业中的许多人采用，用于头脑风暴或研究工具（Kato 和 Goto，2023；Gero 等人，2023；Halperin 和 Lukin，2023）。此类工具面临的一个关键挑战是促进创造性生成，而不是生成最有可能的延续，这是语言模型经过训练后要做的。目前的 LLM 被作家观察到过度依赖群体或套路，并产生过于道德化和可预测的结局（Chakrabarty 等人，2024 年）。虽然情节应该出乎意料，但故事中的细节不应违背常识（除非它是背景的一部分），并保持故事的一致性。这需要一个能够控制其输出的创造力水平的模型。我们需要训练一个更具创造性的模型吗，还是我们可以在推断阶段解决这个问题？另一方面，通过强化学习与人类反馈相结合来净化 LLM 的关注可能会导致该模型在导航更深入和道德上更具挑战性的主题方面不称职。探索的另一个方向是如何构建更好的写作工具，这些工具可以与人类协同工作。已经尝试让用户通过说明进行交互（Chakrabarty 等人，2022 年）或使用编辑序列来提高写作质量（Schick 等人，2022 年）。这些可以作为朝着开发支持不同类型输入、可以通过互动改进和个性化的模型这一目标的关键构建块。此外，模型还可以帮助写作的不同阶段，例如世界建设和审阅草稿。有待探索的是，模型在哪里最有效，哪里需要人类作者做出决定。</font>

<font style="color:rgb(29, 33, 41);">交互式体验 文本生成模型不仅可以作为编写静态脚本的助手，还可以通过基于用户输入的条件来为用户提供动态且个性化的体验。这些交互式体验可以用于教育、治疗、游戏设计或电影制作。最近，人们试图将对话模型与语音识别、文本转语音和音频到面部渲染等其他组件连接起来，以创建一个端到端的沉浸式体验，让用户能够与非玩家角色进行互动。[23] 另一个相关的探索领域是创造</font>

**<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);">第</font>****<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);"> </font>****<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);">5</font>****<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);"> </font>****<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);">页</font>**

<font style="color:rgb(29, 33, 41);">以情感为导向的体验，这是讲故事的关键目标之一（Lugmayr等人，2017）。我们应该考虑根据期望的情感反应和读者的反馈来构建叙述（Brahman和Chaturvedi，2020；Ziems等人，2022；Mori等人，2022）。</font>

### <font style="color:rgb(63, 63, 63);">4. 新颖且富有挑战性的学习范式</font>
### <font style="color:rgb(63, 63, 63);">4.1 多模态学习</font>
<font style="color:rgb(29, 33, 41);">鉴于语言世界取得的巨大进步，我们现在正准备进入以前无法考虑的多种模式。一些学习信号来自于阅读静态数据，如图像、视频、语音等，这将在这一部分讨论；而其他信号需要与物理世界交互，具体请参见第 4.2.2 节。</font>

<font style="color:rgb(29, 33, 41);">多模态编码的核心是学习各种模态之间的“对应关系”或“对齐”，这总是面临着不同模态之间粒度差异的挑战。这是一个新的、不断增长的领域，已经提出了几种跨越模态对齐的方法：（1）硬对齐，使粒度感知融合成为可能（Tan 和 Bansal，2020；Li 等人，2022a；Momeni 等人，2023；Wang 等人，2022c, 2023f）；（2）软对齐以将文本空间与视觉空间投影到一起（Zhou 等人，2023；Li 等人，2023b；Zhu 等人，2023；Lin 等人，2023）。除了这些语义对齐的挑战之外，在非语义抽象方面还存在进一步的困难：</font>

<font style="color:rgb(29, 33, 41);">几何推理：识别空间关系，如“左”、“右”、“旁边”、“上方”或“后面”，需要全面的空间心理模拟，而现有模型一直存在错误（Kamath 等人，2023）。无论位置、旋转还是缩放，保持变换不变性仍然是一个核心挑战。此外，目前主要基于二维图像训练的模型本质上忽略了三维空间配置的复杂性，阻碍了根据距离理解深度和相对物体大小的理解。为了解决这些挑战，现有的工作通过增强现有大型模型来推断空间布局，并预测可能的导航路线，使用视觉和文本线索（Liu等人，2022；Berrios等人，2023；Feng等人，2023b）。然而，我们认为几何推理的根本挑战在于缺乏目标。现有的</font>

<font style="color:rgb(29, 33, 41);">预训练范式主要关注图像/视频语言对之间的语义对齐，而特征（例如低级边缘、线条）在编码的图像表示中被大量忽略。</font>

<font style="color:rgb(29, 33, 41);">语境歧义：准确理解应考虑时间动态、社会动态、情感动力等广泛背景。时间维度为理解视觉和语言带来了独特的挑战。现有方法只关注时序顺序（Zellers等人，2021年，2022年）和前后生成（Seo等人，2022年；Yang等人，2023a；Cheng等人，2023）。然而，时间动力学要复杂得多。例如，视频手势（如点头）可能对应于稍后在演讲中的肯定（Li等人，2019）。这种模糊性需要对各种约束下的更广泛的上下文进行推理。情绪是另一个尚未充分探索的抽象维度，它通过语音的音调、音高和速度以及视觉表达或身体语言来传达。此外，社交规范的理解也很困难，因为相同的单词或面部表情可以根据上下文传达不同的情绪。因此，潜在的解决方案需要考虑各种上下文，包括之前的对话或事件，并进行闲聊推理。</font>

<font style="color:rgb(29, 33, 41);">层次化感知：人类认知本质上是分层的。在处理视觉信号时，我们的注意力并不均匀地分布在每个像素上，而是集中在包含最多信息的显著区域，使我们能够快速识别关键特征并理解周围环境（Hochstein 和 Ahissar，2002；Eickenberg 等人，2017）。然而，现有的模型忽视了这种注意力层次，并且在询问视觉细节时往往会失去焦点（Gao 等人，2023 年 b）。为了解决这一挑战，解释自然场景需要从广义上下文到详细属性抽象的层次化识别。此外，将视觉层次与语言结构对齐很重要。此外，它还需要具备对细节进行抽象的能力，在抽象的场景理解和复杂的识别之间取得平衡是一项持续的挑战。</font>

### <font style="color:rgb(63, 63, 63);">4.2 在线学习</font>
<font style="color:rgb(29, 33, 41);">由于是在静态语料库上进行训练，现有的模型无法保持更新，也无法从交互历史中学习新信息。</font>

**<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);">第</font>****<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);"> </font>****<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);">6</font>****<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);"> </font>****<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);">页</font>**

<font style="color:rgb(29, 33, 41);">自我提升。为了解决这些问题，这一部分讨论了下一代模型需要在线学习的需求。</font>

### <font style="color:rgb(63, 63, 63);">4.2.1 在模型中更新信息</font>
<font style="color:rgb(29, 33, 41);">更新模型的一种直观方法是在新数据上继续训练。然而，这不是一个有效的解决方案，因为我们只关心占数据一小部分的新信息，并且也不是很有效，因为在新数据上的微调可能会干扰模型中学习到的信息。为了实现高效的更新，我们希望模型能够自动识别新数据中的显著信息（余、吉，2023），而不是像知识编辑任务那样依赖于繁重的人工选择或预处理（戴等，2021；孟等，2022a，b；朱等，2020；De Cao等，2021；Hase等，2023；米切尔等，2022b）。有效地更新模型需要克服对（余、吉，2023；韦等，2023）的偏见以及避免对先前学习的信息进行灾难性遗忘（McCloskey和Cohen，1989；Ratcliff，1990）。这可以通过改变训练范式来实现，随着时间的推移增加模型容量（例如渐进式训练（龚等，2019）、Mixture of Experts（沈等，2023））或者更好地理解模型内的知识组织（如第3.1节所述），以便在最小化干扰的情况下进行编辑。</font>

<font style="color:rgb(29, 33, 41);">4.2.2从连续交互中学习 交互在人类学习中起着至关重要的作用（Jarvis，2006）。通过与环境互动，人们学会了如何最好地处理不同的任务，并且他们通过与其他人的互动来学习社会规范。此外，这种交互具有多轮特性，使人们能够迭代地改进手头的任务并不断改善其心理模型在未来执行类似任务的能力。</font>

<font style="color:rgb(29, 33, 41);">与环境的交互 我们认为环境是一类广泛的系统，它会对行为提供反馈。我们所生活的世界可以被视为典型的环境：物理定律决定了世界的改变，并为行为者提供了传感器刺激（例如，Ahn等人，2022）。训练一个模型（即嵌入式人工智能），通过多模态输入与物理世界进行交互（Driess等人，2023；Jiang等人，2023）会带来多模态学习相关的问题（第4.1节），以及独特的挑战——</font>

<font style="color:rgb(29, 33, 41);">由于长期规划需求和动态环境，需要应对挑战。 环境的概念也适用于人类设计的环境（例如，编程语言解释器（Wang等人，2023 年 b） ，身体模拟器（Shridhar 等人，2020 年）），这些环境通过规则为任何输入提供自动反馈。 这种人工环境可以轻松收集自动反馈，这有助于模型在物理世界中部署。</font>

<font style="color:rgb(29, 33, 41);">除了从对通用人类偏好学习来构建一般性代理 (Ouyang等人，2022)，现实世界的应用通常需要高效地创建可定制的解决方案（例如个性化代理）。我们主张一种新的学习范式，其中模型可以通过与人进行（多模态）交互来进行教学，包括自然语言反馈（Padmakumar等人，2022；Wang等人，2023c）和物理演示（Lee，2017）。这种复杂的问题性质也可能涉及从大量专用模型中定制检索和有效行动规划（Qin等人，2023；Yuan等人，2023）。</font>

### <font style="color:rgb(63, 63, 63);">5. 评价</font>
<font style="color:rgb(29, 33, 41);">随着模型变得越来越强大、多功能，其评估已成为推进NLP发展的瓶颈。我们首先讨论“应该评估什么”的问题，然后讨论“我们应该如何衡量性能”。</font>

### <font style="color:rgb(63, 63, 63);">5.1 基准测试</font>
<font style="color:rgb(29, 33, 41);">众所周知，语言模型是多任务学习者，新一代的 LLM 在少量样本甚至零样本条件下都能取得令人印象深刻的表现。这导致了诸如 GLUE (Wang 等，2018)，SuperGLUE (Wang 等，2019)，MMLU (Hendrycks 等，2021)，Super-NaturalInstructions (Wang 等，2022a)，HELM (Liang 等，2022)，AGIEval (Zhong 等，2023)等许多通用基准的创建。虽然全面建立基准很有用，但当前的基准仍然存在以下限制：(1) 缺乏对现实世界应用至关重要的多样化和困难的任务；(2) 只包含静态数据集，对于需要多轮上下文相关输入的应用程序（如情景驱动型对话）来说并不足够；(3) 健壮性不足；(4) 缺乏性能分析支持。</font>

**<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);">第</font>****<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);"> </font>****<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);">7</font>****<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);"> </font>****<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);">页</font>**

<font style="color:rgb(29, 33, 41);">尽管一些基准测试涵盖了数千项自然语言处理任务，但大多数都是句子级别的任务变体，而忽略了更困难的任务，如结构化预测和跨文档推理。例如，Li等人（2023a）报告称，在信息提取任务中，基于更小模型的 LLM 方法比最先进的方法表现低了25.2%至68.5%。任务设计还应旨在帮助人类用户完成日常任务，例如最受欢迎的任务与 ChatGPT 用户在 ShareGPT 4 中的计划和寻求建议有关。另一个问题是，由于新模型的发展，基准测试很快就会饱和，因此可以随着时间更新的“实时”基准测试（Kiela等人，2021年）可能值得追求。</font>

<font style="color:rgb(29, 33, 41);">为了超越静态数据，我们认为大规模多人游戏环境等模拟环境可以提供一种有效的解决方案。游戏被用作衡量强化学习算法进展的标准（Silver 等人，2018；Guss 等人，2021），也被用于在自然语言处理中收集静态数据集（Urbanek 等人，2019；Bará 等人，2021；Lai 等人，2022）。游戏世界为探索不同的环境和情况提供了廉价的方式，这是基于地面的语言学习和通过互动学习所必需的。人类可以与在游戏中扮演角色的模型进行交互以评估其性能，或者我们可以让模型相互交互(朴等人，2023)，并评估它们的整体交互行为。</font>

<font style="color:rgb(29, 33, 41);">最后，我们提倡超越目前通过人工检查的脆弱案例研究范式之外的模型诊断工作：帮助识别模型在哪些输入部分表现不佳的方法（Liu等人，2021年b），模型的行为模式是什么以及性能可以归因于什么数据（Ilyas等人，2022）。</font>

### <font style="color:rgb(63, 63, 63);">5.2度量标准</font>
<font style="color:rgb(29, 33, 41);">过去二十年来，自动评价指标一直是NLP发展的催化剂。基于启发式的方法（Papineni等人，2002；Lin，2004；Lavie和Agarwal，2007）发现与人类偏好相关性较弱（刘等人，2016）。因此，该领域已经转向模型驱动的度量标准，这些度量标准已显示出与人类判断更好的一致性（Lowe等人，2017；张</font>

<font style="color:rgb(29, 33, 41);">et al.，2020；Sellam et al.，2020；Yuan et al.，2021；Zhong et al.，2022）。然而，这些指标可能会导致捷径方法或在评分模型中嵌入偏见（Sun et al.，2022）。</font>

<font style="color:rgb(29, 33, 41);">自动度量标准在诸如对话和创造性写作等开放性自然语言生成问题上遇到困难，因为缺乏真实情况。 GPT模型为解决这个问题提供了机会（Zheng等人，2023；Fu等人，2023；Liu等人，2023年b），但也存在某些偏见，包括位置、冗长和自我增强偏见（模型更喜欢自己）用户应该注意。我们需要开发超越准确性的指标，并评估稳健性、偏差、一致性、信息丰富性、真实性以及效率等方面。</font>

<font style="color:rgb(29, 33, 41);">另一方面，人类评估传统上被认为是一种更值得信赖的评估方法，也是模型效用的一个更好指标。然而，随着模型的进步，众包工作者是否足以胜任评估者（或注释者）的角色是有疑问的，特别是在科学、医疗保健或法律等领域。注释者的偏见（Geva等人，2019；Sap等人，2022）和分歧（Fornaciari等人，2021）也应该被考虑在内。如果我们设计我们的模型成为“助手”，那么更有用的人类评估可能不是确定哪个输出更正确，而是哪个输出可以帮助人类更有效地完成任务。</font>

### <font style="color:rgb(63, 63, 63);">6. NLP+X 跨学科应用</font>
### <font style="color:rgb(63, 63, 63);">6.1 以人类为中心的自然语言处理</font>
<font style="color:rgb(29, 33, 41);">随着语言模型在研究和公共领域变得无处不在，缓解这些模型对社会群体的潜在危害（Blodgett等人，2020年）以及代表性和分配性（布洛德格特等人，2020年）都应成为核心考虑因素。社会偏见和刻板印象是语言模型表现内部缺陷的常见方式，因此消除这些模型中的偏见对于公平性和鲁棒性至关重要。此外，语言模型必须意识到遵守用户期望的社会文化规范的额外语境要求（ fung等人，2023），尤其是在直接与人类互动时作为聊天机器人使用。</font>

<font style="color:rgb(29, 33, 41);">后验校准和提高预训练语言模型的社会意识对于这一目标至关重要。尽管现代方法在民主化 LLM 训练方面取得了巨大进展，但大多数</font>

**<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);">第</font>****<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);"> </font>****<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);">8</font>****<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);"> </font>****<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);">页</font>**

<font style="color:rgb(29, 33, 41);">构建者不需要预先训练自己的 LLM，而是选择最多对其进行微调。研究人员已经讨论了在预训练后希望 LLM 不受偏见的用处，而不是对任何意外关联负责( Yu 等人，2023a；Omrani 等人，2023；Yang 等人，2023b)。相对较少探索的是增强 LLM 的意识和遵守社会规范的能力的要求。问题的核心在于如何训练模型识别其训练数据中哪些行为是社会规范的结果、发现这些规范何时以及为何应该遵循、以及这些规范是如何遵循的（即是否只能以特定方式遵循，还是可以在各种情况下概括为一种行为？）。</font>

<font style="color:rgb(29, 33, 41);">另一个重要的方向是基于用户个性化，尤其是针对聊天机器人。语言模型具有根据提示（第 2.1 节）中提供的语言上下文进行行为复用的惊人能力，但它们没有能力在文本推断之外考虑受众。这给个性化带来了问题，因为相同的语境或对话可能因受众的不同而具有不同的适当性（例如，一个人可能认为相对无害的东西对另一个人来说非常冒犯）。因此，我们必须提高语言模型独立于每个具体环境推断个人规范和适当行为的能力，并据此采取行动。这部分可能涉及弥合分享相似信仰的远程用户的鸿沟以解码潜在表示（孙等，2023 年）。同时，我们还可以为用户提供多维生成控件（韩等，2023 年），包括他们的情感、政治立场和道德价值观，以便他们直接影响模型的语言使用。</font>

### <font style="color:rgb(63, 63, 63);">6.2 科学中的自然语言处理</font>
<font style="color:rgb(29, 33, 41);">自然语言处理（NLP）对科学领域的影响潜力最大（Hope等人，2022；Zhang等人，2023）。尽管研究人员长期以来一直有兴趣从文献中提取可操作的信息（Hersh和Bhupatiraju，2003；Griffiths和Steyvers，2004；Li等人，2016；Wang等人，2021），但由于科学语言的多样性和复杂性，这一直是具有挑战性的。随着NLP技术能力的增长，加强了对科学文献的分析和理解的需求。这些需求包括但不限于：识别关键概念、发现新颖见解、生成摘要、检测抄袭和异常行为以及支持自动评估和反馈系统。在本文中，我们提供了关于如何使用 NLP 技术进行科学文献分析的详细说明。我们介绍了用于执行这些任务的不同方法，并讨论了各种技术的优缺点。我们还提供了有关如何设计实验以衡量不同方法之间性能差异的建议。此外，我们探讨了未来的研究方向，包括探索更复杂的文本理解和生成任务，以及开发更具解释性和透明度的方法。</font>

<font style="color:rgb(29, 33, 41);">现在值得做，因为这既有可能带来的影响，也有需要克服的挑战。</font>

<font style="color:rgb(29, 33, 41);">一个令人兴奋的新领域是在科学领域联合学习自然语言和其他数据模态（爱德华兹等人，2021；曾等人，2022；爱德华兹等人，2022；泰勒等人，2022），而当前 LLM 中最大的问题之一——幻想——已成为发现新分子（爱德华兹等人，2022）、蛋白质（刘等人，2023a）和材料（谢等人，2023）的优势。</font>

<font style="color:rgb(29, 33, 41);">另一个值得注意的应用是在医学中的自然语言处理。作为一个具体的激励例子，估计有 1033 种具有类药物特性的现实分子（波兰奇克等人，2013 年）。在这些药物中，有一些子结构赋予了药物有益的特性，而关于这些特性的知识被报道在数百万篇科学论文中。然而，现有的 LLM 只能从非结构化文本中进行预训练，并且无法捕获这种知识，部分原因是文献中的不一致之处。</font>

<font style="color:rgb(29, 33, 41);">最近为基于领域知识增强的 LLM 提供解决方案包括开发一个轻量级适配器框架，以选择并集成结构化领域知识到 LLM 中（Lai 等人，2023 年 b），数据扩充以从通用领域对科学领域的知识蒸馏（Wang 等人，2023 年 g）以及利用基础模型的工具学习框架来解决更复杂的序列问题（秦等人，2023；钱等人，2023）。总体而言，未来的研究可以探索定制架构、数据获取技术和训练方法，以理解科学中的各种模态、特定于域的知识和应用。</font>

### <font style="color:rgb(63, 63, 63);">6.3 教育中的NLP</font>
<font style="color:rgb(29, 33, 41);">语言模型可以轻松地捕获许多主题的知识，通过外部知识增强语言模型自然会提高提取这些知识以生成课程计划和材料的能力。然而，在教育中也有应用似乎与一般的NLP任务不同。特别是，使用语言模型个性化教育和学习体验将使教育者能够专注于更一般性的高级教学工作。然后，使用语言模型进行教育的好处不在于语言模型的“学习”能力，而在于它能够提供适当的内容。</font>

**<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);">第</font>****<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);"> </font>****<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);">9</font>****<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);"> </font>****<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);">页</font>**

<font style="color:rgb(29, 33, 41);">知识，而是其发现关联的能力。这个挑战的一个方面来自于识别并分析学生理解或学习中的空白。例如，除了在诸如流利性、句子结构等离散维度上评分外，还可以通过使用语言模型来确定自由形式提交中哪些部分存在空白，并将其与教师提供的学习目标相关联，而无需使用特定（且成本高昂）的黄金标记响应，以便学生能够获得可操作的反馈并进行自我改进。作为这项工作的一部分，我们需要准确地识别学生的回答中哪些部分是由学生撰写的，而不是从人工智能助手那里复制粘贴过来的。这将确保不会隐藏空白，但这需要对学生的技能有长期的了解。此外，我们必须能够确保语言模型的建议基于学生和文本的实际细节，而不是高先验的普遍预测或基于幻觉。此外，我们应该投资于使用语言模型生成或检索材料或支架，以帮助提高学生的学习速度，而不是简化原始课程材料。</font>

### <font style="color:rgb(63, 63, 63);">7 我们需要什么</font>
<font style="color:rgb(29, 33, 41);">我们的总体目标是打击把自然语言处理 (NLP) 简化为一个评价优化问题的做法，消除对大模型和生成式人工智能会扼杀这个领域的担忧。正如一句老话所说，“频繁搬家会使树木死亡，但人却繁荣”。正如 1980 年代的 NLP 研究人员不得不学习机器学习并将其作为该领域的一项核心技术一样，我们现在必须探索和接受大模型及其能力。机器学习并没有“解决”NLP 的挑战：它没有产生能够学习语言、翻译、回答问题、创作诗歌以及完成儿童可以完成的所有事情的引擎。有些人声称大模型可以做到这些，甚至更多。但我们刚刚开始接触它们，还没有时间发现它们的所有缺陷。</font>

<font style="color:rgb(29, 33, 41);">中央挑战在于规模。为了使用语言，没有一个孩子需要阅读或听到互联网上超过一半的英文文本。是什么推理</font>

<font style="color:rgb(29, 33, 41);">人们拥有哪些能力，这些能力是语言模型所缺乏的？自然语言处理研究如何发展来模拟并涵盖它们？我们需要全球基础设施来大规模扩展计算资源，因为开源模型仍然无法实现与 GPT 变体相当的性能（Gudibande 等人，2023 年）。但我们也迫切需要对我们领域的基础概念模型进行更深入的思考。</font>

<font style="color:rgb(29, 33, 41);">在自然语言处理研究人员对应该追求哪些研究问题感到不确定的这个独特时期，作为一个社区，我们需要共同努力系统地改变并改进我们的论文评审体系和学术成功衡量标准，以建立一个更具包容性的研究环境，并鼓励学者（尤其是初级学者）探索对于整个领域至关重要的长期、高风险主题。这些新的挑战也要求我们更加开放地与来自其他领域的研究人员进行紧密合作，包括社会科学、自然科学、计算机视觉、知识表示与推理以及人机交互。</font>

### <font style="color:rgb(63, 63, 63);">限制</font>
<font style="color:rgb(29, 33, 41);">在这篇论文中，我们描述了一些新的或尚未充分探索的 NLP 研究方向，这些研究方向仍然值得写进论文。我们提出了一个更广泛、更具吸引力的自然语言处理版本，它鼓励人们关注更广泛的一系列更具挑战性、难度更大的问题，这些问题对社会有益的可能性非常大。这些问题并不总是能够提供简单明了的数据集和纯粹的机器学习解决方案。我们的列表并不是详尽无遗的，我们选择这些方向作为例子。由 NLP 研究人员来揭示问题并开发新颖的解决方案。</font>

### <font style="color:rgb(63, 63, 63);">道德考量</font>
<font style="color:rgb(29, 33, 41);">本文件中列出的研究领域只是可以探索的主要领域之一；还有其他领域。我们无意强制执行我们的建议立场。我们鼓励对有价值的研 究领域进行更多样化、更深入的调查。在这些提出的 方向中，我们承认一些需要访问用户的个人信息（例如第 6.1 节中的聊天机器人个性化），而某些应用可能对用户有 高影响（例如使用模型来评估学生对知识的理解程度以 进行有针对性的教育）。</font>

**<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);">第</font>****<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);"> </font>****<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);">10</font>****<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);"> </font>****<font style="color:rgb(102, 110, 121);background-color:rgb(247, 247, 247);">页</font>**

<font style="color:rgb(29, 33, 41);">在第 6.3 节中。对创造性工作的使用也引发了人们对版权和监管规定的担忧，即人工智能是否可以被认定为作者。如果没有保障措施，我们不支持用 LLM 进行筛选或资源分配。即使对于风险较低的应用程序，我们也更倾向于研究系统的鲁棒性、透明性和公平性。最后，我们必须评估提示 LLM 的合规性法律法规。例如，在教育应用中，如果需要学生信息，就必须参考诸如 FERPA/DPA/GDPR 等法律，特别是在在线学习环境中。</font>


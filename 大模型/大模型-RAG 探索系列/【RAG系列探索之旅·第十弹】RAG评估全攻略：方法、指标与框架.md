在探索RAG的发展和优化过程中，有效评估其性能已成为一个核心问题。本章主要讨论评估方法、RAG的关键指标、它应具备的能力，以及一些主流的评估框架。

## <u><font style="color:#2F4BDA;">评估方法</font></u>
评估RAG有效性主要有两种方法：独立评估和端到端评估。

### <u><font style="color:#2F4BDA;">独立评估</font></u>
独立评估包括评估检索模块和生成（阅读/综合）模块。

1. 检索模块  
一套评估系统（如搜索引擎、推荐系统或信息检索系统）根据查询或任务对项目进行排名的有效性的指标通常用于评估RAG检索模块的性能。例如包括命中率、MRR、NDCG、准确度等。
2. 生成模块  
这里的生成模块是指通过将检索到的文档补充到查询中而形成的增强或综合输入，不同于最终答案/响应的生成，后者通常是端到端评估。生成模块的评估指标主要关注上下文相关性，衡量检索到的文档与查询问题的相关性。

### <u><font style="color:#2F4BDA;">端到端评估</font></u>
端到端评估评估RAG模型针对给定输入生成的最终响应，涉及模型生成答案与输入查询的相关性和一致性。从内容生成目标的角度来看，评估可以分为未标记内容和已标记内容。

未标记内容评估指标包括答案的忠实度、相关性、无害性等，而已标记内容评估指标包括准确性和EM。此外，从评估方法的角度来看，端到端评估可以分为人工评估和使用LLMs的自动评估。

以上总结了RAG的一般端到端评估情况。此外，根据RAG在特定领域的应用，采用特定的评估指标，如问答任务中的EM、摘要任务中的UniEval和E-F1，以及机器翻译中的BLEU。这些指标有助于理解RAG在各种特定应用场景中的性能。

## <u><font style="color:#2F4BDA;">关键指标和能力</font></u>
现有研究经常缺乏对检索增强生成（RAG）对不同大型语言模型（LLMs）影响的严格评估。在大多数情况下，评估RAG在各种下游任务中的应用以及与不同检索器的结合可能会产生不同的结果。然而，一些学术和工程实践专注于RAG的一般评估指标和其有效使用所需的能力。本节主要介绍评估RAG有效性的关键指标和评估其性能所必需的基本能力。

### <u><font style="color:#2F4BDA;">关键指标</font></u>
最近OpenAI的报告提到了优化大型语言模型（LLMs）的各种技术，包括RAG及其评估指标。此外，最新的评估框架，如RAGAS和ARES，也涉及RAG评估指标。综合这些工作，主要关注三个核心指标：答案的忠实度、答案相关性和上下文相关性。

#### 忠实度
该指标强调模型生成的答案必须保持对给定上下文的真实性，确保答案与上下文信息一致，不偏离或矛盾。这一评估方面对于解决大型模型中的幻觉问题至关重要。

#### 答案相关性
该指标强调生成的答案需要直接与提出的问题相关。

#### 上下文相关性
该指标要求检索到的上下文信息尽可能准确和有针对性，避免不相关的内容。毕竟，处理长文本对LLMs来说成本很高，太多不相关的信息会降低LLMs利用上下文的效率。

OpenAI的报告还提到了“上下文召回”作为一个补充指标，衡量模型检索回答问题所需的所有相关信息的能力。这个指标反映了RAG检索模块的搜索优化水平。低召回率表明可能需要优化搜索功能，例如引入重排序机制或微调嵌入，以确保检索到更多相关内容。

### <u><font style="color:#2F4BDA;">关键能力</font></u>
RGB的工作分析了不同大型语言模型（LLMs）在RAG所需的四项基本能力方面的表现，包括抗噪声能力、负面拒绝能力、信息整合能力和反事实鲁棒性，为检索增强生成建立了一个基准。RGB关注以下四种能力：

+ 抗噪声能力

这种能力衡量模型处理噪声文档的效率，噪声文档是指与问题相关但不包含有用信息的文档。

+ 负面拒绝

当模型检索到的文档缺乏回答问题所需的知识时，模型应正确地拒绝回应。在负面拒绝的测试设置中，外部文档只包含噪声。理想情况下，LLM应发出“缺乏信息”或类似的拒绝信号。

+ 信息整合

这种能力评估模型是否能整合多个文档的信息来回答更复杂的问题。

+ 反事实鲁棒性

这个测试旨在评估模型在接收到关于检索信息中可能风险的指令时，是否能识别并处理文档中已知的错误信息。反事实鲁棒性测试包括LLM可以直接回答的问题，但相关外部文档包含事实错误。

### <u><font style="color:#2F4BDA;">评估框架</font></u>
最近，大型语言模型（LLM）社区一直在探索使用“LLMs作为评判”进行自动评估，许多人利用强大的LLMs（如GPT-4）来评估自己的LLM应用输出。Databricks使用GPT-3.5和GPT-4作为LLM评判来评估其聊天机器人应用的实践表明，使用LLMs作为自动评估工具是有效的。他们认为这种方法也可以高效和经济地评估基于RAG的应用。

在RAG评估框架领域，RAGAS和ARES相对较新。这些评估的核心重点是三个主要指标：答案的忠实度、答案相关性和上下文相关性。此外，由行业提出的开源库TruLens也提供了类似的评估模式。这些框架都使用LLMs作为评估的裁判。由于TruLens与RAGAS相似，本章将专门介绍RAGAS和ARES。

#### RAGAS
这个框架考虑了检索系统识别相关和关键上下文段落的能力，LLM使用这些段落的忠实度，以及生成本身的质量。RAGAS是一个基于简单手写提示的评估框架，使用这些提示以完全自动化的方式衡量质量的三个方面 - 答案忠实度、答案相关性和上下文相关性。在实施和试验该框架时，所有提示都使用OpenAI API提供的gpt-3.5-turbo-16k模型进行评估。

**算法原理**

1. **评估答案忠实度：**使用LLM将答案分解为单独的陈述，并验证每个陈述是否与上下文一致。最终，通过比较支持的陈述数量与总陈述数量来计算“忠实度得分”。
2. **评估答案相关性：**使用LLM生成潜在问题，并计算这些问题与原始问题之间的相似性。通过计算所有生成问题与原始问题的平均相似性来得出答案相关性得分。
3. **评估上下文相关性：**使用LLM提取与问题直接相关的句子，并使用这些句子与上下文中总句子数量的比率作为上下文相关性得分。

#### ARES
ARES旨在从三个方面自动评估RAG系统的性能：上下文相关性、答案忠实度和答案相关性。这些评估指标与RAGAS中的类似。**然而，RAGAS作为一个基于简单手写提示的较新评估框架，对新的RAG评估设置的适应性有限，这是ARES工作的重要意义之一**。此外，如其评估所示，ARES的表现明显低于RAGAS。

ARES通过使用少量手工标注数据和合成数据来降低评估成本，并利用预测驱动推理（PDR）提供统计置信区间，提高评估的准确性。

**算法原理**

1. **生成合成数据集：**ARES最初使用语言模型从目标语料库的文档中生成合成问题和答案，以创建正面和负面样本。
2. **准备LLM裁判：**接下来，ARES使用合成数据集微调轻量级语言模型，训练它们评估上下文相关性、答案忠实度和答案相关性。
3. **使用置信区间对RAG系统进行排名：**最后，ARES应用这些裁判模型对RAG系统进行评分，并将其与手动标注的验证集结合使用PPI方法生成置信区间，可靠地估计RAG系统的性能。


本文主要从三个维度组织：增强阶段、增强数据源和增强过程，来阐述RAG发展中的关键技术。“RAG核心组件的分类”在图中进行了说明。

![](https://cdn.nlark.com/yuque/0/2023/png/406504/1703813233998-0f4e0441-8033-4469-a3e7-3c6f15be874a.png)

## <u><font style="color:#2F4BDA;">RAG增强</font></u>
作为一项知识密集型任务，RAG在语言模型训练的预训练、微调和推理阶段采用不同的技术方法。

### <u><font style="color:#2F4BDA;">预训练阶段</font></u>
自预训练模型出现以来，研究人员已经深入研究在预训练阶段通过检索方法增强预训练语言模型（PTMs）在开放域问答（QA）中的性能。识别并扩展预训练模型中的隐含知识可能具有挑战性。REALM引入了一种更模块化和可解释的知识嵌入方法。遵循掩蔽语言模型（MLM）范式，REALM将预训练和微调建模为一个检索-然后-预测过程，其中语言模型通过预测基于掩蔽句子x的掩蔽标记y来预训练，建模P(x|y)。

RETRO利用检索增强来预训练自回归语言模型，通过从大量标记数据中检索，实现从头开始的大规模预训练，并显著减少模型参数。RETRO与GPT模型共享骨干结构，并引入额外的RETRO编码器来编码从外部知识库检索到的相邻实体的特征。此外，RETRO在其解码器变换器结构中结合了块状交叉注意力层，有效地整合来自RETRO编码器的检索信息。RETRO实现了比标准GPT模型更低的困惑度。此外，它在更新语言模型中存储的知识时提供了灵活性，可以通过更新检索数据库而无需重新训练语言模型。

Atla采用了类似的方法，使用T5架构在预训练和微调阶段结合检索机制。在预训练之前，它使用预训练的T5初始化编码器-解码器LM骨干，使用预训练的Contriever初始化密集检索器。在预训练过程中，它每1000步刷新一次异步索引。

COG是一个文本生成模型，通过逐渐从现有文本集合中复制文本片段（如单词或短语）来形式化其生成过程。与传统的逐字选择的文本生成模型不同，COG利用高效的向量搜索工具来计算文本片段的有意义的上下文表示，并对其进行索引。因此，文本生成任务被分解为一系列复制和粘贴操作，在每个时间步骤中，从文本集合中寻找相关的文本片段，而不是从独立的词汇表中选择。COG在多个方面表现出优于RETRO的性能，包括问答、领域适应性和扩展短语索引。

另一方面，随着对规模定律的发现，模型参数迅速增加，使得自回归模型成为主流。研究人员也在探索是否可以使用RAG方法对更大的模型进行预训练。RETRO++是RETRO的扩展，增加了模型的参数规模。研究发现，在文本生成质量、事实准确性、低毒性和下游任务准确性方面均有一致的改进，特别是在知识密集型任务（如开放域问答）中。这些研究结果突出了结合检索对自回归语言模型进行预训练的有前景方向。

总的来说，增强预训练的优势和局限性是显而易见的。从积极方面来看，这种方法提供了一个更强大的基础模型，其困惑度、文本生成质量和下游任务性能均优于标准的GPT模型。此外，与纯预训练模型相比，它通过使用更少的参数实现了更高的效率。特别是在处理知识密集型任务方面表现出色，通过在特定领域语料库上的训练，允许创建特定领域的模型。

然而，也存在缺点，包括需要大量的预训练数据和更大的训练资源，以及更新速度较慢的问题。特别是随着模型规模的增加，检索增强训练的成本相对较高。尽管存在这些局限性，这种方法在模型鲁棒性方面表现出显著特点。一旦训练完成，基于纯预训练的检索增强模型就消除了对外部库依赖的需要，提高了生成速度和操作效率。

### <u><font style="color:#2F4BDA;">微调阶段</font></u>
在下游微调阶段，研究人员采用了各种方法来微调检索器和生成器，以改善信息检索，主要应用于开放域问答任务。关于检索器的微调，REPlUG将语言模型（LM）视为一个黑盒，并通过可调的检索模型对其进行增强。通过从黑盒语言模型获得监督信号的反馈，REPlUG改进了初始检索模型。另一方面，UPRISE通过在多样化任务集上的微调来创建一个轻量级和多功能的检索器。这个检索器可以自动为零样本任务提供检索提示，展示了其通用性和在不同任务和模型上的提升性能。

同时，微调生成器的方法包括Self-Mem，通过示例的内存池对生成器进行微调，以及Self-RAG，通过生成反射标记来满足主动检索需求。RADIT方法通过最大化给定检索增强指令的正确答案概率来微调生成器和检索器。它更新生成器和检索器，以最小化文档和查询之间的语义相似度，有效地利用相关背景知识。

此外，SUGRE引入了对比学习的概念。它对检索器和生成器进行端到端微调，确保高度详细的文本生成和检索子图。使用基于图神经网络（GNN）的上下文感知子图检索器，SURGE从与正在进行的对话相对应的知识图中提取相关知识。这确保了生成的响应忠实地反映了检索到的知识。为此，SURGE采用了一个不变的但高效的图编码器和图文对比学习目标。

总结来说，微调阶段的增强方法展现了几个特点：

1. 首先，同时微调LLM和检索器允许更好地适应特定任务，提供了单独或同时微调两者的灵活性，如RePlug和RA-DIT中所见。
2. 其次，这种微调的好处扩展到适应多样化的下游任务，如UPRISE所示，使模型更加通用。
3. 此外，微调使模型能够更好地适应各种语料库中不同的数据结构，特别是对于图结构语料库来说，这一点在SUGRE方法中得到了突出。

然而，微调阶段也存在限制，例如需要为RAG微调专门准备数据集，以及与推理阶段的RAG相比，需要更多的计算资源。总体来说，在微调阶段，研究人员可以根据特定需求和数据格式定制模型，与预训练阶段相比减少资源消耗，同时保留调整模型输出风格的能力。

### <u><font style="color:#2F4BDA;">推理阶段</font></u>
在推理阶段，将RAG方法与LLM整合已成为一个普遍的研究方向。值得注意的是，Naive RAG的研究范式依赖于在推理阶段整合检索内容。

为了克服Naive RAG的局限性，研究人员在推理阶段为RAG引入了更丰富的上下文。DSP框架依赖于一个复杂的流程，涉及在冻结的语言模型（LM）和检索模型（RM）之间传递自然语言文本，为模型提供更丰富的上下文以增强生成质量。PKG为LLMs配备了一个知识引导模块，允许在不改变LLMs参数的情况下访问相关知识，使模型能够执行更复杂的任务。此外，CREA-ICL利用同步检索跨语言知识来协助获取额外信息，而RECITE通过从LLMs中抽样一个或多个段落来形成上下文。

在推理阶段，优化RAG的过程可以有利于适应更具挑战性的任务。例如，ITRG通过迭代检索和寻找正确的推理路径，增强了适应需要多步推理任务的能力。ITERRETGEN采用迭代方法，将检索和生成融合，实现了“检索增强生成”和“生成增强检索”的交替过程。

另一方面，IRCOT融合了RAG和CoT的概念，采用交替的CoT引导检索，并使用检索结果来改善CoT。这种方法显著提高了GPT-3在各种QA任务上的性能，凸显了整合检索和生成的潜在优势。

总结来说，推理阶段的增强方法具有轻量级、成本效益高、无需额外训练、利用强大的预训练模型等优势。其主要优势在于在微调期间冻结LLMs的参数，专注于提供更符合需求的上下文，具有快速和低成本的特点。

然而，这种方法也有一些局限性，包括需要额外的数据处理和过程优化，同时受到基础模型能力的限制。通常，这种方法经常与逐步推理、迭代推理和自适应检索等过程优化技术相结合，以更好地满足不同任务的要求。

## <u><font style="color:#2F4BDA;">增强数据源</font></u>
数据源对RAG的有效性至关重要。各种数据源提供不同的知识粒度和维度，需要不同的处理方法。它们主要分为三类：非结构化数据、结构化数据和由LLMs生成的内容。

### <u><font style="color:#2F4BDA;">利用非结构化数据增强</font></u>
非结构化数据主要包括文本数据，通常源自纯文本语料库。此外，其他文本数据也可以作为检索源，如用于大模型微调的提示数据和跨语言数据。

**在文本粒度方面，**除了常见的文本块（包括句子），检索单元还可以是词汇（例如，kNN-LM）、短语（例如，NPM、COG）和文档段落。更细粒度的检索单元通常能更好地处理罕见模式和领域外情景，但会增加检索成本。

在词汇层面，FLARE采用主动检索策略，只在LM生成低概率词时进行检索。该方法涉及生成临时的下一句来检索相关文档，然后在检索到的文档条件下重新生成下一句来预测后续句子。

**在文本块层面，**RETRO使用前一个文本块来检索最近的相邻文本块，并将此信息与前一个文本块的上下文信息整合以指导下一个文本块的生成。RETRO通过从检索数据库中检索前一个块N(Ci−1)的最近邻居，然后通过交叉注意力将前面块的上下文信息（C1, ..., Ci−1）和N(Ci−1)的检索信息融合，以指导下一个块Ci的生成。为了保持因果性，第i块Ci的自回归生成只能使用前一块的最近邻居N(Ci−1)，而不能使用N(Ci)。

### <u><font style="color:#2F4BDA;">利用结构化数据增强</font></u>
像知识图谱（KG）这样的结构化数据源逐渐融入RAG的范式。经过验证的KG可以提供更高质量的上下文，减少模型幻觉的可能性。

RET-LLM通过从过去的对话中提取关系三元组来构建个性化知识图谱内存，以备将来使用。SUGRE使用图神经网络（GNN）嵌入从知识图谱中检索到的相关子图，以防止模型生成与上下文无关的回复。SUGRE采用一种图编码方法，将图结构反映到PTMs的表示空间，并利用图文模式之间的多模态对比学习目标，以确保检索到的事实与生成的文本之间的一致性。KnowledgeGPT以代码格式生成对知识库（KB）的搜索查询，并包含预定义的KB操作函数。除了检索外，KnowledgeGPT还提供了将知识存储在个性化知识库中的能力，以满足个别用户的需求。这些结构化数据源为RAG提供了更丰富的知识和上下文，有助于改善模型性能。

### <u><font style="color:#2F4BDA;">基于LLM生成内容的RAG</font></u>
观察到RAG回忆的辅助信息并非总是有效的，甚至可能产生负面效果，一些研究通过深入探索LLM的内部知识，扩展了RAG的范式。这种方法利用LLM自身生成的内容进行检索，旨在提高下游任务的性能。以下概述了此类别内的一些显著研究：

+ SKR采用一个标记的训练集，将模型可以直接回答的问题归类为已知问题，而需要检索增强的问题归类为未知问题。模型被训练用来识别问题是否为已知，仅对被识别为未知的输入应用检索增强，而直接回答其余问题。
+ GenRead用LLM生成器替代检索器。实验结果表明，生成的上下文文档包含正确答案的情况比Naive RAG检索到的更为普遍。生成的答案也表现出更高的质量。作者将此归因于生成文档级上下文的任务与因果语言模型的预训练目标之间的一致性，允许更好地利用存储在模型参数中的世界知识。
+ Selfmem使用检索增强的生成器迭代创建一个无限的内存池。采用一个内存选择器来选择输出作为后续生成的内存。这个输出作为原始问题的对偶问题。通过结合原始问题和对偶问题，检索增强的生成模型可以利用自己的输出来增强自身。

这些多样化的方法展示了RAG检索增强的创新策略，旨在提升模型性能和有效性。

## <u><font style="color:#2F4BDA;">增强过程</font></u>
大多数RAG研究通常只执行一次检索和生成过程。然而，单次检索可能包含冗余信息，导致“迷失在中间”的现象。这种冗余信息可能会掩盖关键信息，或包含与真实答案相反的信息，对生成效果产生负面影响。此外，单次检索获得的信息在需要多步推理的问题中是有限的。

优化检索过程的当前方法主要包括迭代检索和自适应检索。这些方法允许模型在检索过程中多次迭代或自适应调整检索过程，以更好地适应不同的任务和场景。

### <u><font style="color:#2F4BDA;">迭代检索</font></u>
定期根据原始查询和生成的文本收集文档可以为大型语言模型（LLM）提供额外材料。在多次迭代检索中提供额外参考资料以提高后续回答生成的鲁棒性。然而，这种方法可能在语义上不连贯，并可能导致收集噪声和无用信息，因为它主要依赖于一系列 n 个标记来区分生成和检索的文档。

针对特定数据场景使用递归检索和多跳检索。递归检索可以首先通过结构化索引处理数据，然后逐级检索。在检索层次丰富的文档时，可以为整个文档或长篇PDF的每个部分制作摘要。然后根据摘要进行检索。确定文档后，针对内部块进行第二次检索，从而实现递归检索。多跳检索常用于进一步挖掘图结构数据源中的信息。

一些方法迭代检索和生成步骤。ITER-RETGEN 协同利用“检索增强生成”和“生成增强检索”来完成需要重现信息的任务。即，模型使用完成任务所需的内容来响应输入任务，这些目标内容作为检索更相关知识的信息上下文。这有助于在另一次迭代中生成更好的响应。

IRCoT还探索了为每个生成的句子检索文档，引入了在思维链的每个步骤中进行检索。它使用CoT来指导检索，并使用检索结果来改善CoT，确保语义完整性。

### <u><font style="color:#2F4BDA;">自适应检索</font></u>
事实上，前两节中描述的 RAG 方法采用了被动方法，其中优先考虑检索。这种涉及根据上下文查询相关文档并将其输入LLM的方法，可能导致效率问题。例如，Flare和 Self-RAG引入的自适应检索方法优化了RAG检索过程，使LLM能够主动判断检索的时机和内容。这有助于提高检索信息的效率和相关性。

事实上，LLM主动使用工具和做出判断的方式并不是源自RAG，而已在大模型的代理中被广泛使用。Graph-Toolformer的检索步骤大致分为：LLM主动使用检索器，Self-Ask和DSP尝试使用少量示例提示触发LLM搜索查询。当LLM认为有必要时，它们可以决定搜索相关查询以收集必要的材料，类似于代理的工具调用。

WebGPT采用强化学习框架自动训练 GPT-3 模型使用搜索引擎进行文本生成。它使用特殊标记来执行动作，包括在搜索引擎上查询、滚动排名和引用参考资料。这使 GPT-3 能够利用搜索引擎进行文本生成。

另一方面，Flare自动化了检索的时机，并解决了基于生成文本概率的定期文档检索的成本问题。它使用概率作为LLM在生成过程中的信心指标。当一个术语的概率低于预定义阈值时，信息检索系统将检索参考资料并移除概率较低的术语。这种方法旨在处理LLM可能需要额外知识的情况。

Self-RAG引入了一个重要创新，称为反思标记。这些特殊标记生成以审视输出，并有两种类型：检索和批评。模型可以自主决定何时检索段落或使用设定阈值触发检索。需要检索时，生成器同时处理多个段落，执行片段级束搜索以获得最佳序列。使用批评分数更新每个子部分的得分，这些权重可以在推理过程中调整，以定制模型的行为。Self-RAG框架还允许LLM自主决定是否需要回忆，避免训练额外的分类器或依赖NLI模型。这增强了模型自主判断输入和生成准确答案的能力。


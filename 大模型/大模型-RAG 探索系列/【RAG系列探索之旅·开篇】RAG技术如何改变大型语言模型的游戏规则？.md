随着自然语言处理（NLP）领域的不断发展，大型语言模型（LLMs）如GPT系列和LLama系列模型已成为技术前沿的代表。这些模型在语言理解和知识掌握方面表现出了令人印象深刻的能力，甚至在多个评估基准上超越了人类的表现。然而，大型语言模型也存在明显的局限性，特别是在处理特定领域或需要最新信息的查询时，它们往往无法提供准确的答案。



传统上，通过对模型进行微调来适应特定领域的信息，这种方法被称为参数化知识。参数化知识是通过训练大型语言模型获得的，存储在模型的权重中。然而，这种方法需要大量的计算资源和专业技术，且难以适应快速变化的信息环境。与之相对的非参数化知识则存在于外部知识源，如向量数据库中，作为可更新的补充信息使用。



纯粹的参数化语言模型面临着保留所有训练语料库知识的困难，尤其是对于不常见和具体的信息。此外，由于模型参数不能动态更新，知识很容易过时。为了解决这些限制，出现了一种新的方法：检索增强生成（Retrieval-Augmented Generation，RAG）。



RAG首次由Lewis等人在2020年提出，结合了预训练的检索器和生成器，并进行端到端的微调。在大型模型出现之前，RAG主要集中在直接优化端到端模型上，使用诸如基于向量的密集通道检索（DPR）等密集检索方法，并在生成方面训练较小的模型。



随着像ChatGPT这样的大型语言模型的出现，LLMs在各种语言任务上展现出卓越性能。但它们仍面临着幻觉问题、知识更新和数据相关问题，这些问题影响了模型的可靠性。在知识密集型任务，如开放域问答和常识推理中，LLMs的表现还有待提高。



为了解决这些问题，研究者发现将RAG引入到大型模型的上下文学习中可以显著提高效果。RAG在推理过程中动态地从外部知识源检索信息，使用检索到的数据作为组织回答的参考，这大大提高了响应的准确性和相关性。

![](https://cdn.nlark.com/yuque/0/2023/png/406504/1703142305081-ec50e278-2e38-4d86-b6b2-f996aebf7d91.png)

当前，RAG技术的研究可以被分为三种主要范式：初级RAG、高级RAG和模块化RAG。这些范式分别涉及不同的增强策略和应用场景。初级RAG技术在ChatGPT发布后迅速普及，特别是在预训练和监督微调阶段的增强研究。而在推理阶段的增强研究则更多出现在LLM时代。这些研究旨在通过引入RAG模块来整合外部知识，以一种成本效益的方式增强模型生成。在增强数据的使用方面，早期的RAG主要关注于非结构化数据，尤其是在开放域问答中。随后，检索的知识来源范围扩大，使用高质量数据作为知识来源，有效解决了大型模型中的错误知识内化和幻觉问题。知识图谱是一个典型的结构化知识来源例子。最近，自我检索受到了更多的关注，它涉及到挖掘LLM自身的知识以增强其性能。



RAG技术为大型模型提供了一种有效的解决方案，它巧妙地结合了生成模型的强大能力和检索模块的灵活性，为纯参数化模型固有的知识局限性提供了补充。


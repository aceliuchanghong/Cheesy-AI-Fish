#### 主流的大模型开源模型体系有哪些？
目前主流的大模型开源模型体系包括以下几个：

1. GPT-4-Turbo：由OpenAI开发的GPT-4的轻量级版本，在多项评测中表现最佳，尤其在复杂推理方面虽有提升空间，但已明显领先于国内的商业模型和开源模型。
2. 智谱清言GLM-4：国内厂商智谱科技发布的大模型，紧随GPT-4-Turbo之后，在语言和知识等基础能力维度上可比肩GPT-4 Turbo。
3. 阿里巴巴Qwen-Max：阿里巴巴推出的大模型，同样在评测中表现优秀，尤其在中文场景下展现出优势。
4. 百度文心一言4.0：百度发布的大模型，也在评测中取得了优秀成绩，特别是在中文语言理解、中文知识和中文创作上具有竞争力。
5. Yi-34B-Chat、InternLM2-Chat-20B：开源社区的模型，以中轻量级的参数量、接近商业闭源模型的性能，在综合性对话体验上表现突出。
6. Gemma：由Google DeepMind开发的轻量化开源模型系列，采用与创建Gemini模型相同的研究和技术构建。
7. ERNIE：百度推出的面向用户场景的多任务大模型，具有11亿参数，在文本生成、情感分析、命名实体识别等多种任务中表现不错。
8. FlagOpen大模型技术开源体系：由智源研究院推出的开源体系，包括语言模型（如悟道·天鹰Aquila系列）、视觉与多模态模型、向量模型、大模型数据处理工具集、大模型算法框架等，旨在降低大模型开发和应用的门槛，提高开发效率。

#### Prefix Decoder 和 causal Decoder 和 Encoder-Decoder 区别是什么？
三种架构的主要区别在于解码器如何处理输入序列。

Encoder-Decoder

Encoder-Decoder 架构是第一个提出并广泛应用于自然语言处理任务的Transformer模型架构。该架构由两个部分组成：编码器和解码器。编码器负责将输入序列编码为一个潜在的表征，解码器则负责根据该表征生成目标序列。

在解码阶段，解码器会逐个生成目标序列的每个token。在生成每个token之前，解码器会首先对其之前生成的token进行自注意力操作，以获取上下文信息。然后，解码器会对编码器输出进行注意力操作，以获取输入序列的信息。最后，解码器会将这两种信息结合起来，生成下一个token。

Causal Decoder

Causal Decoder 架构是在 Encoder-Decoder 架构的基础上改进的。该架构的主要区别在于，解码器在生成每个token时，只会对其之前生成的token进行自注意力操作，而不会对编码器输出进行注意力操作。

这种改进的动机是，在许多自然语言处理任务中，目标序列与输入序列之间存在着因果关系。例如，在机器翻译任务中，目标句子的语义应该由源句子的语义决定。因此，在生成目标句子的每个token时，解码器应该只考虑源句子的信息，而不应该考虑目标句子的之前生成的token。

Prefix Decoder

Prefix Decoder 架构是近年来提出的新型Transformer模型架构。该架构的主要区别在于，解码器在生成每个token时，只会考虑输入序列的前缀部分，而不会考虑输入序列的完整部分。

这种改进的动机是，在许多自然语言处理任务中，目标序列与输入序列的前缀部分之间存在着强相关关系。例如，在文本摘要任务中，摘要的语义应该由文本的前几句话决定。因此，在生成摘要的每个token时，解码器应该只考虑文本的前几句话，而不应该考虑文本的完整部分。

三种架构的主要区别如下：

| 架构 | 解码器 | 关注范围 |
| --- | --- | --- |
| Encoder-Decoder | 自注意力 + 注意力 | 输入序列 |
| Causal Decoder | 自注意力 | 输入序列 |
| Prefix Decoder | 自注意力 | 输入序列前缀 |


#### 涌现能力是啥原因？
涌现能力是指大模型在训练过程中，出现了一些在小模型上没有出现的能力。这些能力往往是难以预测的，并且与模型的具体架构和训练目标无关。

目前，对于涌现能力的原因，还没有一个统一的解释。不过，有一些研究者认为，涌现能力可能与以下几个因素有关：

+ 模型规模：模型规模越大，模型的表达能力越强，就越有可能出现涌现能力。
+ 训练数据：训练数据的质量和数量也会影响涌现能力。高质量的训练数据可以帮助模型学习到更复杂的语言规律，从而提高涌现能力。
+ 训练方法：训练方法也会影响涌现能力。一些训练方法，例如半监督学习，可以帮助模型更好地学习到语言的统计规律，从而提高涌现能力。

以下是一些涌现能力的例子：

+ 零样本学习：大模型能够在没有见过任何训练样本的情况下，对新的类别进行分类。
+ 常识推理：大模型能够利用常识知识进行推理，回答一些没有明确答案的问题。
+ 创造性写作：大模型能够创作出具有原创性的文本，例如诗歌、小说等。

#### 为何现在的大模型大部分是Decoder only结构？
现在的大模型大部分是Decoder only结构，主要有以下几个原因：

+ 训练效率

Decoder only结构的训练效率更高。在Encoder-Decoder结构中，编码器和解码器需要分别训练，这需要双倍的训练时间和计算资源。而Decoder only结构只需要训练解码器，因此训练效率更高。

+ 推理效率

Decoder only结构的推理效率更高。在Encoder-Decoder结构中，在生成每个token时，解码器都需要对编码器输出进行注意力操作，这需要额外的计算成本。而Decoder only结构不需要进行注意力操作，因此推理效率更高。

+ 表达能力

Decoder only结构的表达能力与Encoder-Decoder结构相同。在Encoder-Decoder结构中，编码器负责将输入序列编码为一个潜在的表征，解码器则负责根据该表征生成目标序列。而Decoder only结构可以将输入序列的潜在表征作为输入，因此表达能力与Encoder-Decoder结构相同。

+ 低秩问题

在Encoder-Decoder结构中，编码器的双向注意力可能会存在低秩问题，这可能会削弱模型表达能力。而Decoder only结构不存在低秩问题，因此表达能力更强。

+ 参数量

Decoder only结构的参数量比Encoder-Decoder结构少一半。在同等参数量下，Decoder only结构的性能往往优于Encoder-Decoder结构。

+ 工程实现

Decoder only结构的工程实现更加简单。在Encoder-Decoder结构中，需要实现编码器和解码器两个部分，这增加了工程实现的复杂度。而Decoder only结构只需要实现解码器一个部分，因此工程实现更加简单。

#### 解释下Transformer中的Decoder模块
Transformer模型的Decoder模块由以下几个部分组成：

+ Self-Attention：Self-Attention是Transformer模型的核心部件，它可以捕捉输入序列的上下文信息。在Decoder模块中，Self-Attention用于计算目标序列每个token的注意力权重。
+ Masked Self-Attention：Masked Self-Attention是一种特殊的Self-Attention，它只允许解码器关注目标序列中之前生成的token。这可以防止解码器看到目标序列的未来信息，从而保证模型的预测能力。
+ Multi-Head Attention：Multi-Head Attention是多个Self-Attention层的组合，它可以从不同角度捕捉输入序列的上下文信息。
+ Feed Forward Network：Feed Forward Network是一个前馈神经网络，它可以对Self-Attention的输出进行非线性变换。
+ Positional Encoding：Positional Encoding可以为输入序列中的每个token添加位置信息，这可以帮助模型更好地理解输入序列的结构。

Decoder模块的工作流程如下：

1. 首先，Decoder模块会对输入序列进行Self-Attention操作，计算目标序列每个token的注意力权重。
2. 然后，Decoder模块会使用Masked Self-Attention来屏蔽目标序列的未来信息。
3. 接下来，Decoder模块会将Self-Attention的输出与Positional Encoding进行相加，得到最终的注意力权重。
4. 最后，Decoder模块会使用Multi-Head Attention来组合多个Self-Attention层的输出，并将其送入Feed Forward Network进行非线性变换，得到最终的预测结果。

Decoder模块的注意事项：

+ 在训练Decoder模块时，需要使用Masked Self-Attention来防止模型看到目标序列的未来信息。
+ Decoder模块的输出可以是各种类型的文本，例如词语、句子或篇章。

Decoder模块的应用：

+ 机器翻译
+ 文本摘要
+ 问答系统
+ 语音识别
+ 自然语言生成

#### 简单 介绍一下 大模型【LLMs】？
大模型 (LLMs) 是指参数量巨大的语言模型，通常由数十亿甚至万亿个参数组成。LLMs 通常使用大量的文本数据进行训练，能够学习语言的统计规律，并生成与人类语言相似的文本。

LLMs 的特点：

+ 强大的表达能力：LLMs 可以学习到语言的复杂规律，并生成具有丰富语义和表达力的文本。
+ 泛化能力强：LLMs 可以应用于各种自然语言处理任务，例如机器翻译、文本摘要、问答系统等。
+ 需要大量的训练数据和计算资源：LLMs 的训练需要大量的文本数据和计算资源，这使得它们的训练成本很高。

LLMs 的发展趋势：

+ 模型规模不断扩大：随着计算资源的不断增长，LLMs 的模型规模将会不断扩大，以提高其表达能力和泛化能力。
+ 训练方法不断改进：新的训练方法将被开发出来，以提高 LLMs 的训练效率和降低训练成本。
+ 应用领域不断拓展：LLMs 将被应用于更多的自然语言处理任务，发挥更大的作用。

#### 大模型【LLMs】后面跟的 175B、60B、540B等 指什么？
大模型【LLMs】后面跟的 175B、60B、540B 等数字指的是模型的参数量，单位是十亿。例如，175B 表示模型的参数量为 1750 亿个，60B 表示模型的参数量为 600 亿个，540B 表示模型的参数量为 5400 亿个。

模型的参数量是衡量模型规模的重要指标。一般来说，模型的参数量越大，模型的表达能力就越强，泛化能力就越强。以下是一些常见的大模型及其参数量：

| 模型名称 | 参数量 |
| --- | --- |
| GPT-3 | 175B |
| Bloom | 176B |
| WuDao 2.0 | 1.75T |
| LaMDA | 137B |
| Megatron-Turing NLG | 530B |


#### 什么是LLMs的复读机问题？
LLMs的复读机问题指的是LLMs在生成文本时出现的一种现象，即模型倾向于无限地复制输入的文本或者以过度频繁的方式重复相同的句子或短语。这种现象使得模型的输出缺乏多样性和创造性，给用户带来了不好的体验。

#### 为什么会出现LLMs会生成循环重复的内容？
LLMs（大型语言模型）出现“复读机”问题的原因多种多样，主要归结于模型的设计、训练过程以及它们处理信息的方式。下面是一些导致这一问题的关键因素：

1. **模型架构和训练目标**：LLMs通常通过最大化给定前文下一词的概率来训练，这意味着它们倾向于生成在训练数据中常见的、与前文最相似的响应。这种训练方式可能导致模型重复输入文本中的模式，而不是生成新颖的内容。
2. **数据偏差**：LLMs的训练数据通常来源于互联网，包含大量重复和模式化的文本。这些数据中的偏差和重复性可能会在模型的输出中得到放大，导致复读机问题。
3. **过拟合**：当模型在训练数据上学习得太好，以至于过度拟合这些数据的特定特征和噪声时，它们在生成文本时可能会重复训练集中的特定短语或句型，而不是学习到更泛化的语言生成能力。
4. **优化和损失函数**：模型的优化过程和使用的损失函数可能鼓励模型采取“保守”的策略，即复制输入或训练数据中的文本，因为这种策略在统计上更有可能减少预测错误，从而最小化损失函数。
5. **上下文理解的局限性**：尽管LLMs在处理大量文本时非常擅长，但它们可能无法完全理解上下文的深层含义和复杂性，这导致它们在尝试响应时可能回归到重复输入的“安全”行为。
6. **创新和风险的平衡**：在生成新颖内容时，LLMs需要在创新性和准确性/相关性之间取得平衡。过分依赖输入文本是避免生成完全不相关或离题内容的一种策略，但这也限制了创造性。

#### 如何缓解LLMs生成重复内容的问题
缓解LLMs（大型语言模型）的复读机问题涉及到多种策略和技术，旨在提高模型的创新性、多样性和对上下文的敏感度。以下是一些常见的方法：

1. **改进训练数据**：确保训练数据多样化且高质量，可以减少数据中的重复性和偏差，帮助模型学习到更广泛的语言表达和内容。数据清洗和去重也是重要的步骤。
2. **采用正则化技术**：正则化技术，如Dropout或权重衰减，可以减少模型的过拟合，帮助模型在训练数据上学到更泛化的模式，而不是过分依赖于特定的输入序列。
3. **控制生成过程**：在文本生成时，可以通过调整温度参数来控制输出的随机性和创新性。较高的温度值可以增加生成文本的多样性和创新性，但也可能增加文本不连贯的风险。
4. **使用不同的解码策略**：除了常见的贪婪解码和束搜索外，还可以尝试采用更多样化的解码策略，如随机采样、Top-k采样或核采样（Nucleus Sampling），这些方法可以增加输出文本的多样性。
5. **微调和域适应**：对LLMs进行微调，使其适应特定任务或领域的数据，可以改善模型在特定上下文中的性能，减少无关紧要的复读内容。
6. **促进创新性的损失函数**：在训练模型时，可以设计或添加损失函数，鼓励模型生成与训练数据不同但仍然相关和合理的输出，从而促进创新性。
7. **引入外部知识**：通过结合外部知识库或信息源，模型可以在生成文本时获得额外的上下文信息，这有助于生成更相关、更丰富的内容。
8. **交互式学习和人工反馈**：利用人工反馈来指导模型的学习，可以帮助模型更好地理解何种类型的生成被视为有创意和有价值，从而调整其生成策略。

这些方法可以单独使用或结合使用，以根据特定的应用需求和场景来缓解LLMs的复读机问题。

#### LLaMA输入句子长度理论上可以无限长吗？
LLaMA（Large Language Model at Meta AI）和大多数大型语言模型一样，实际上并不能处理理论上无限长的输入句子。这是因为模型的架构和内存限制决定了它可以处理的最大序列长度。在训练时，模型会被设计来处理固定长度的序列，这个长度通常由模型的注意力机制和可用的计算资源决定。

大多数现代的语言模型，包括LLaMA，使用的是变压器（Transformer）架构，它通过自注意力机制处理输入序列。这个架构在理论上可以处理任意长度的序列，但在实际应用中，由于计算复杂度（特别是内存使用）随序列长度呈二次增长，处理长序列会变得非常昂贵和不实际。因此，开发者会根据可用的计算资源和特定应用需求设定一个最大序列长度。

例如，一些模型可能被设计为处理最多512个或1024个标记（单词或字符）的序列，超过这个长度的输入可能需要被截断或以其他方式处理。对于需要处理更长文本的应用，可能会采用分段、汇总或其他策略来适应模型的限制。

因此，虽然理论上变压器模型可以处理很长的序列，但实际应用中的限制使得LLaMA和其他大型语言模型无法处理无限长的输入句子。

#### 如何看待当前大模型在垂类领域的发展情况？
是否需要为各个专业领域开发专用的大型模型，取决于几个关键因素，包括领域的特殊性、可用数据的性质和量级、以及特定应用的需求。以下是一些考虑点：

领域特殊性

+ **高度专业化的领域**：对于医学、法律、金融等高度专业化的领域，使用专门为这些领域定制的大模型可能更为合适。这些领域往往包含大量特定的术语、概念和知识体系，通用的大型模型可能无法充分理解和准确处理这些专业内容。
+ **通用性较强的领域**：对于通用性较强、术语和知识体系与日常语言相近的领域，通用的大型模型可能就足够应对大多数需求，特别是在进行基本的语言理解和生成任务时。

数据可用性和质量

+ **数据丰富的领域**：如果某个领域有大量高质量的训练数据可用，那么为这个领域开发一个专用的大模型可能更有意义，因为这样可以充分利用这些数据来训练模型，使其更好地理解和处理该领域的特定问题。
+ **数据稀缺的领域**：对于数据较为稀缺的领域，可能难以训练一个高效的专用大型模型，此时可能需要依赖更通用的模型，并通过技术如迁移学习和微调来适应特定的领域需求。

应用需求

+ **高准确度和可靠性要求**：在对准确度和可靠性有严格要求的应用中（例如医疗诊断、法律文件分析等），专用的大型模型可能更为必要，因为这些模型可以被定制和优化以满足这些领域的特殊要求。
+ **灵活性和适应性**：对于需要较高灵活性和适应性的应用，可能会优先选择通用模型，特别是在任务跨多个领域或需要模型具备一定的通用性时。

结论

虽然在某些情况下，为特定领域开发专用的大型模型是有益的，但这并不意味着每个领域都需要自己的大模型。这主要取决于领域的特定需求、可用数据的性质和量级，以及所面临的具体挑战。在某些情况下，通过微调和适应性技术，通用的大型模型可以被有效地适应用于特定领域的任务，而无需从头开始开发新模型。这种方法可以节省资源并利用现有模型的强大能力。

#### 如何缓解大模型上下文长度限制？
处理更长文本是大型语言模型（LLMs）面临的一个挑战，尤其是那些基于Transformer架构的模型，因为它们的内存需求随着输入长度的增加而显著增加。不过，有几种方法可以使这些模型能够处理更长的文本：

长序列Transformer变体  
研究人员已经开发了多种Transformer架构的变体，专门用于减少处理长序列时的计算和存储需求。例如：

+ 稀疏Transformer：通过修改自注意力机制，使其仅关注序列中的部分位置而非全部位置，减少了计算复杂度。
+ Longformer、BigBird等：这些模型通过使用稀疏性和局部性原则来改进注意力机制，能够有效处理更长的序列。

分段处理  
将长文本分成较小的片段，单独对每个片段进行处理。这种方法的关键在于如何在不丢失重要上下文信息的情况下分割文本，以及如何综合各个片段的输出以得到最终结果。

层次化注意力  
通过引入层次化的注意力机制，模型首先在较低层次上处理文本的局部信息，然后在更高层次上整合这些局部信息。这种方法可以减少对长距离依赖的直接计算，从而降低处理长序列的复杂度。 

窗口化注意力和全局标记  
使用固定大小的窗口来限制每个自注意力操作的范围，并引入全局标记来捕获跨窗口的信息。全局标记可以是特殊的标记或向量，它们在所有窗口中共享，以便整合全文信息。 

外部记忆体系  
引入外部记忆（如可微分的神经存储器），使模型能够在处理长文本时访问和更新这些存储器，以此来扩展其处理长序列的能力。 

压缩技术  
利用文本压缩技术，如摘要或信息提取，先将长文本压缩为更短的形式，然后再输入到模型中。这种方法牺牲了一些信息，但可以使模型处理更长的文本。 

微调和适应性技术  
对于特定的长文本处理任务，可以通过微调来训练模型，更好地处理分段的文本并维护跨段落的上下文关系。 

#### 如何计算全参微调下，大模型所需要的显存大小？
在某个模型基础上进行全参数微调所需的显存大小，主要取决于以下几个因素：

+ 模型参数量：模型参数量越大，所需的显存就越大。
+ 数据类型：使用浮点16位（fp16）精度训练，所需的显存约为使用浮点32位（fp32）精度训练的一半。
+ 优化器状态：Adam等优化器需要存储梯度和动量等状态信息，也会占用一定的显存。
+ 梯度累积：为了提高训练效率，可以将多个梯度累积起来再进行更新，这样可以减少所需的显存。

具体计算公式如下：

```plain
显存大小 = 模型参数量 * 数据类型大小 * (1 + 优化器状态系数) / 梯度累积步数
```

例如：

+ 一个参数量为10亿的模型，使用fp16精度训练，Adam优化器，梯度累积步数为1，则所需的显存大小约为：

```plain
显存大小 = 10亿 * 2字节 * (1 + 2) / 1 = 40GB
```

+ 如果使用fp32精度训练，则所需的显存大小为：

```plain
显存大小 = 10亿 * 4字节 * (1 + 2) / 1 = 80GB
```

以下是一些可以减少全参数微调所需显存的措施：

+ 使用fp16精度训练：fp16精度训练可以将所需的显存减少一半。
+ 使用梯度累积：梯度累积可以减少所需的显存，但会降低训练速度。
+ 使用混合精度训练：混合精度训练可以兼顾精度和效率。
+ 使用模型并行：模型并行可以将模型拆分到多个GPU上训练，从而减少单个GPU所需的显存。

#### 简要介绍下Transformer架构
Transformer架构基础

Transformer完全基于注意力机制，摒弃了以往RNN和CNN在序列处理中的使用。其核心组成部分包括：

+ **自注意力机制**（Self-Attention Mechanism）：允许模型在处理序列的每个元素时考虑序列中的所有其他元素，从而有效捕获长距离依赖关系。
+ **多头注意力**（Multi-Head Attention）：并行地执行多次自注意力，每个头捕获序列中不同的特征，然后将结果合并。
+ **位置编码**（Positional Encoding）：由于Transformer不像RNN那样递归处理序列，位置编码向模型提供了有关单词在序列中位置的信息。
+ **前馈网络**（Feed-Forward Networks）：在每个Transformer层中，经过多头注意力机制处理的数据会通过两层线性变换和一个非线性激活函数。
+ **标准化层**（Layer Normalization）和**残差连接**（Residual Connections）：有助于避免训练深层网络时出现的梯度消失或爆炸问题。

#### 如何进行模型的选型？
选择使用哪种大模型，如Bert、LLaMA或ChatGLM，取决于具体的应用场景和需求。下面是一些指导原则：

Bert模型：Bert是一种预训练的语言模型，适用于各种自然语言处理任务，如文本分类、命名实体识别、语义相似度计算等。如果你的任务是通用的文本处理任务，而不依赖于特定领域的知识或语言风格，Bert模型通常是一个不错的选择。Bert由一个Transformer编码器组成，更适合于NLU相关的任务。  
LLaMA模型：LLaMA（Large Language Model Meta AI）包含从 7B 到 65B 的参数范围，训练使用多达14,000亿tokens语料，具有常识推理、问答、数学推理、代码生成、语言理解等能力。Bert由一个Transformer解码器组成。训练预料主要为以英语为主的拉丁语系，不包含中日韩文。所以适合于英文文本生成的任务。  
ChatGLM模型：ChatGLM是一个面向对话生成的语言模型，适用于构建聊天机器人、智能客服等对话系统。如果你的应用场景需要模型能够生成连贯、流畅的对话回复，并且需要处理对话上下文、生成多轮对话等，ChatGLM模型可能是一个较好的选择。ChatGLM的架构为Prefix decoder，训练语料为中英双语，中英文比例为1:1。所以适合于中文和英文文本生成的任务。  
在选择模型时，还需要考虑以下因素：

数据可用性：不同模型可能需要不同类型和规模的数据进行训练。确保你有足够的数据来训练和微调所选择的模型。  
计算资源：大模型通常需要更多的计算资源和存储空间。确保你有足够的硬件资源来支持所选择的模型的训练和推理。  
预训练和微调：大模型通常需要进行预训练和微调才能适应特定任务和领域。了解所选择模型的预训练和微调过程，并确保你有相应的数据和时间来完成这些步骤。  
最佳选择取决于具体的应用需求和限制条件。在做出决策之前，建议先进行一些实验和评估，以确定哪种模型最适合你的应用场景。

#### 领域大模型LLM训练Trick
TODO

#### 介绍下大模型的发展历程、背景、和进展
这里谈到的大模型主要是指语言大模型，区分语言大模型主要是通过数据集的规模大小来区分的，通常我们会把训练数据集规模超过 10b（100 亿，billion 是单位，表示 10 亿）的模型称为大模型。  
大模型一共有四个主要的发展阶段。  
第一个阶段是 SLM，也就是统计语言模型。它是基于统计学的学习方法，通过建立单词预测模型来预测下一个单词。它存在数据稀疏性问题，对高阶语言模型的准确度很差。比较有代表的技术是文本分类、词性标注和句法分析等。  
第二个阶段是 NLM，也就是神经语言模型。它是使用神经网络描述单词序列的概率。NLM 通过词的分布式表示概念，并通过学习单词和句子的特征来改进任务的性能。比较有代表性的技术是各种编码器和生成对抗神经网络等。  
第三个阶段是 PLM，也就是预训练模型。这类模型在通过大规模无标签语料库的基础上进行预训练任务，学习通用的上下文感知的单词表示。虽然核心原理不变，但通过预训练和微调等方式，让 PLM 的性能有巨大的提升。比较有代表性的模型有 BERT 和 GPT 系列。  
第四个阶段是 LLM，也就是大语言模型。在 PLM 的基础上，增加模型规模和数据规模来大幅提升性能。LLM 可以解决更加复杂的任务，并让我们看到了 AGI，也就是通用人工智能的可能性。比较有代表性的是 GPT-3 和 GPT-4 等。  
我认为 LLM 的发展背景主要由两个因素推动的。第一是计算机硬件性能的提升，为 LLM 的训练提供了必备的基础能力。第二是互联网的蓬勃发展，为 LLM 提供了大量的语料数据。  
目前大模型存在几点问题：

+ 存在幻觉，有时候会给出看似合理实际上很荒谬的答案，无法保证生成内容的准确性和可信度。而且很多数
+ 更新迭代速度很快，大模型无法及时更新自身知识。
+ 存在数据安全隐患。企业内部的数据训练到公开的大模型中存在内部数据泄漏的风险。
+ 缺乏细分领域的行业深度。大模型很难和企业内部的业务进行有效结合，很难达到很高的专业性。

#### 介绍下注意力机制
注意力机制（Attention）是深度学习中一种非常重要的核心技术。它的作用是实现了高效的信息处理资源分配。原理是先关注场景中最重要的点，暂时性忽略掉不重要的点。也就是以高权重聚焦最重要的信息，以低权重忽略不相关信息。在这个过程中还会不断调整权重，在不同的情况下也可以选取重要的信息。这样可以收缩关注的信息范围，达到压缩信息的目的。  
目前注意力机制影响了深度学习的多个领域，包括图像处理、自然语言处理等。GPT 背后的 Transformer 模型，也是以注意力机制为基本单元设计的。  
随着注意力机制的发展，现在很多注意力机制都更加接近自注意力机制（Self Attention），ChatGPT 就是使用自注意力机制。自注意力机制的注意力是原信息本身，比如我们让 AI 执行一个句子填空的任务，就非常适合自注意力机制，它可以对句子的各个 token 进行划分，给每组 token 设置权重，优先关注权重高的 token，这样还可以跨 token 计算，相比 RNN 性能更好。

#### 介绍下Transformer
刚才介绍的注意力机制，是目前主流神经网络模型的基础机制，Transformer 是实现注意力机制的一种具体形式，我们也可以把它理解成一种模型结构组件。Transformer 的主要场景就是 LLM，比如 GPT 系列。  
Transformer 有两部分组件组成，一部分是 Encoder（编码器），一部分是 Decoder（解码器）。编解码器又都是由 6 个 Block（块）组成。  
Transformer 的工作流程是这样的：先从原始输入中拆分每个单词，并获取每个单词的表示向量。得到一个表示向量的矩阵，传入编码器，经过 6 个 Block 的处理后，得到一个维度相同的编码信息矩阵。然后再把编码信息矩阵传入解码器中，解码器会根据当前输入的矩阵进行处理，在处理过程中，会掩盖当前处理的 token 之后的 token。  
其中编码器的每个 block 会包含一个多头注意力（Multi-Head Attention），解码器的每个 block 会包含两个多头注意力（Multi-Head Attention）。多头注意力机制就是在模型中做多次注意力机制，让模型关注到不同部分的信息。  
而自注意力机制的输出是由三部分决定的，分别是 Q、K、V。Q 就是 Query，表示用户的输入，K 是 Key，表示模型数据中的标签，V 就是 Value，也就是上一个 Block 的输出。最后把 QKV 传入 softmax 公式中，得到最终输出 Z。多头注意力机制会包含多个注意力机制，也就是说有多个 Z，这时候会把多个 Z 进行拼接，传入 Linear 层进行变换，得到最终输出 Z。  
除了多头注意力机制之外，block 结构中还有另外两部分，add & norm 和 feed forward。  
add & norm 是由 add 和 norm 组成的，add 是残差链接，用来解决多层网络训练的问题，让网络只关注当前的差异部分。norm 是指 layer normalization，它可以把每层神经元的输入都转成一样的均值方差，可以加快收敛。  
feed forward 层是两层的全连接层，它也有一个公式，最终 feed forward 会得到一个和输入矩阵纬度相同的输出矩阵。

#### 对比一下 GPT 和 BERT 模型吗？
目前所有 LLM 都是在处理文字形式的内容，也就是自然语言处理。它们和我们人类一样，都属于一种智能体。但是在工作模式上有区别，人类处理自然语言，靠的是过往的经验、认知和自身的智慧，而 LLM 靠的是概率。在这一点上，GPT 和 BERT 的能力是相同的。  
GTP 和 BERT 在技术架构上是非常相似的，比如都是建立在 Transformer 架构上，都使用了无监督学习，都支持微调等。  
它们之间最大的区别在于结构的不同和工作模式的不同。GPT 是 OpenAI 研发的一个自回归的模型，它更考虑的是之前的上下文，BERT 是 Google 研发的一个双向模型，它同时考虑前面和后面的文本。BERT 使用了 Masking Input 的模式来进行的预训练，也就是通过填空题的方式，随机遮盖住句子的某一些词进行训练，所以 BERT 需要对数据集进行标注。GPT 的预训练方式主要是推测下一个可能出现的词，GPT 不需要对数据集进行标注。这也是两种模型的主要差别，在超大规模数据集的情况下，GPT 有很大的优势。  
GPT 会通过评估前面的一个或几个词来生成下一个词，所以它的主要能力是生成，更适合生成流畅和连贯的句子。主要的场景有语言翻译、文章总结和文本完成等。但是 GPT 只能做单向编码。  
BERT 会通过一个句子中的前后两个词来生成缺失的词 ，所以它的主要能力是理解，更适合理解句子的情感，主要的场景有搜索、问答、情感分析和补全缺失词等。BERT 拥有双向编码的能力，但失去了直接生成文本的能力

#### 怎么理解多模态计算？
在过去，机器学习模型通常只能处理一种类型的数据，比如文本类型、图像类型或者音频类型。但是人类的大脑可以理解很多种类型的数据，比如文字、图像、音频和视频等。让 AI 更接近人类的大脑，是 AI 的一个研究方向，这种处理多种数据的能力就是多模态。  
多模态模型一般有多项特征：

+ 输入的数据和输出的数据不同。比如输入文本输出图像，输入图像输出文本。
+ 同时处理多种类型的输入。比如同时输入文本和图像。
+ 同时生成多种类型的输出。比如同时生成文本和图像。

#### 如何理解从 CLIP、BLIP 到DALL 的发展过程？
CLIP 是 OpenAI 在 2021 年 1 月发布的论文中提到的概念，主要作用是通过训练来实现文本和图像的对应关系。CLIP 主要原理是通过超大规模模型预训练提取视觉特征，进行图片和文字之间的对比学习。预训练结束后不经过微调可以直接推理，也就是零样本学习，用见过的特征去判断没见过的特征类型。OpenAI 当时从互联网收集了 4 亿个文本图像对，也叫做 WIT（WebImageText）训练过后的 CLIP 在不经过任何一张图片训练的情况下，在 ImageNet 数据集上和有监督、训练好的 ResNet-50 在精度上持平，大概 76.2%。这在之前被认为是不可能的。CLIP 的训练过程是输入配对好的文字图片对，然后分别通过 Text Encoder 和 Image Encoder 输出对应的特征矩阵，然后在这些特征矩阵上进行对比学习。然后根据任务的分类标签来构建每个类别的描述文本，把这些文本送到 Text Encoder 里面得到对应的文本特征，文本特征的数量和类别数目的数量相等。最后把要预测的图像送到 Image Encoder 的到图像特征，和上一步得到的 N 个文本特征进行余弦相似度，把相似度最大的文本对应的类别作为图像分类预测结果。2021 年 10 月 发布的 Disco Diffusion，就是第一个结合了 CLIP 模型和 diffusion 模型的开源 AI 绘图工具。  
CLIP 本质上就是拿图片特征和文本特征做余弦相似度对比，很简单粗暴。BLIP 是另一种不同的多模态模型，比 CLIP 复杂得多。BLIP 由 4 个部分组成。第一部分是视觉编码器（Image Encoder），它会把一张图片分割成很多块，并把它们编码成一个系列的 Image Embedding，这一步就是在提取图片特征。第二部分是文本编码器，它的作用是提取文本特征，其实就是 BERT 的架构，就不多赘述了。第三部分是视觉文本编码器，它是一种 BERT 的变种。在 block 的自注意层和前馈网络层中间中间插入了一个交叉注意层，引入了视觉特征，用来给图像和文本做而分类，作为图像文本的联合表征。第四部分是视觉文本解码器，它也是 BERT 的变种，它的作用是根据图片特征和文本特征做文本生成。  
DALL 模型又分多个版本。目前有 DALL E、DALL E2 和 DALL E3。这部分我就不展开聊了。

#### 介绍下stable diffusion
Stable Diffusion 是 Stability AI 公司开源的、基于扩散模型 Diffusion Model 算法模型。目前最流行的几个图像生成 AI，DALL E、Midjourney、Stable Diffusion 都是基于扩散模型研发的。图像生成领域有很多种生成式模式，像生成对抗模型 GAN、变分自动编码器 VAE、流模型 Flow based Model 等。它们都是基于深度学习的方式进行训练的模型，但是他们的工作原理是不同的。

准确的来说，Stable Diffusion 模型是一个由多个模型组成的运作系统，其中包含了文本编码器 CLIP、扩散模型 Diffusion 和图像解码器 VAE。CLIP 我之前介绍过了，这里就不再赘述了。扩散模型的部分还包含了 U-Net 和 Scheduler 采样算法两部分。U-Net 原来是用在生物医学图像分割的神经网络模型，工作结构像一个 U 型字母，所以也叫 U 型神经网络训练模型。U-Net 可以辅助提取并解构训练图像的特征，这样可以在只有很少的训练样本的情况下获得更加准确多样的数据信息，提升模型在出图上的精度。在训练扩散模型的过程中可以采用不同的算法来添加噪音，Scheduler 可以定义使用哪种算法来运行程序，它可以定义降噪的步骤、是否具备随机性、查找去噪后样本的算法等。所以它也叫采样算法。我们可以根据图像类型和使用的模型来选择不同的采样器，来达到最佳的出图效果。解码器 VAE 的作用就是把高维数据映射到低维空间，再把低维空间的计算结果解码到高位像素空间。所以它也是由编码器和解码器组成的。编码器的过程也就是我之前举例的高维像素空间映射到低维潜空间的过程。解码器就是将潜空间的计算结果转换为原始图像。SD 的推理过程只用到了 VAE 的解码器部分。

#### 介绍下扩散模型
稳定扩散模型的工作原理，它的训练过程分为正向扩散和逆向扩散。正向就是向训练图像不断添加噪声，直到让一张图片变成一张无意义的纯噪声图。逆向扩散就是逐步去除噪声，恢复到最初的样子。这个过程就有点像朝干净的水中不断滴墨水，直到水变得非常浑浊，再用蒸馏的方式把水变清澈。  
扩散模型也分两种，一种是标准扩散模型，还有一种是潜在扩散模型 Latent Diffusion Model。标准扩散模型是在高维像素空间中进行的，比如一张 512*512 的 RGB 图片，对应的就是一个 76 万多个维度的空间，我们普通的民用级显卡根本无法完成这种级别的计算任务，需要多台专业级显卡一起运算。潜在扩散模型就是为了解决这个问题，它的过程就是把训练图像线索小 48 倍，再进行运算，运算结束后再恢复原始尺寸。这样可以降低硬件要求，同时提升了运算速度。在这个过程中，压缩后运行计算的空间也称作潜空间。在这样大幅减少运算量的情况下，普通的民用级显卡也可以轻松完成图像的生成任务。

#### 分享下langchain构建实际项目的常见步骤流程
第一个大步骤是获取并处理数据，转换为向量，并存入到向量数据库。这一步又分几个小步骤。第一步是通过 LangChain 的 document loaders 模块中 cheerio 模块爬取 react.dev 网站上的所有内容。第二步是通过 LangChain 的 text splitter 模块对文档进行分割。第三步是通过 LangChain 的 embeddings 模块调用 OpenAI 的 embedding API，把分割后的文本转换为向量。再调用 LangChain 的 vectorstores 模块，把向量存储到向量数据库中，这个项目里我用到的是开源的 qdrant。  
第二个大步骤是为前端提供问答接口，这一步也分几个小步骤。第一步是通过 LangChain 的 chat models 模块创建 GPT 的实例。第二步是通过 LangChain 的 retriever 模块，基于 qdrant 数据库和 GPT 来创建 retriever 实例。第三步就是通过 LangChain 的 chains 模块，基于 retriever 实例创建 qaChain 实例。第四步就是当用户请求进入时，将问题传入 qaChain，得到答案，并返回给前端。  
以上基本就是通过 LangChain 和 OpenAI 来开发问答类应用的通用流程。其中会用到几个外部资源，比如数据源、embedding 接口、向量数据库等。它的原理就是将 LLM 不理解的上下文信息存储到向量数据库中，在进行问答时，先把原始问题转换为向量，然后去向量数据库中匹配相似度最高的上下文内容，填充到 question 中发送给 LLM，这样 LLM 就可以知晓它原本不知道的内容，从而给出更准确的答案。

#### 详细介绍下langchain框架
LangChain 是一个由大语言模型驱动的应用开发框架。在 AIGC 应用开发领域的定位有点像前端里的 React 或者 Java 里的 Spring。它的设计原则有两个：

+ data-aware：数据感知，可以将 LLM 模型和外部数据源进行连接。
+ agentic：代理，可以让 LLM 模型和外部环境进行交互。

这两个原则也是它的核心能力，目前 LangChain 的所有功能模块都是围绕这两个点进行设计的。  
LangChain 原来主要的模块有 6 个，分别是模型 Models、提示 Prompt、索引 Indexes、链 Chains、代理 Agents 和记忆 Memory。后来经过调整，目前主要的模块有 6 个，分别是模型 Model I/O（原来的 Prompts 被归类到了 Model I/O 中）、恢复 Retrieval（实际上就是原来的 Indexes 模块）、链 Chains、代理 Agents、记忆 Memory和新增加的回调 Callbacks。每个模块都有不同的作用，这里我简单介绍一下它们的作用。  
Models 模块主要是对 LLM 层进行抽象。可以简化我们调用 LLM 接口的过程，这样我们可以很轻易的替换底层的 LLM。Models 目前分为两类，一类是普通 LLM，LLM 一般是输入文本字符串并返回文本字符串的模型，比如 text-davinci-003。一类是 Chat Model，这类模型通常支持传入消息列表作为输入并返回的模型，比如 GPT-3.5、GPT-4 和 Claude 等。  
Prompts 模块主要是提供了一些工具，方便我们更容易构建需要的提示词。这部分提供的工具有 PromptTemplates、ChatPrompt Templates 和 Example Selectors。PromptTemplates 主要就是通过模板的方式让我们更好的复用提示词。我们给定一个模板字符串，再从用户输入中获取一组参数进行生成，得到最终的提示。主要包含语言模型的说明，也就是模型当前扮演的角色，一组少量的示例，可以帮助 LLM 完成零样本学习，更好的生成响应，最后就是具体的问题需求。ChatPrompt Templates 是针对聊天模型设计的，功能与 PromptTemplates 类似，不过可以接受消息列表作为输入，列表会包含一组消息，每个消息都会有一个角色。Example Selectors 是对示例的选择。我们可以定义很多示例，然后根据不同的规则去选择示例。比如根据输入长度选择示例，输入长时选择多一些示例，输入短时少选择一些示例。也可以根据相关性来选择和输入最相关的示例。  
Indexes 模块主要作用就是和外部数据进行集成，根据外部数据来获取答案。索引主要包含 Document Loaders、Text Splitters、Vectorstores 和 Retrievers 几个模块。使用它们可以完成从外部获取数据的标准步骤。首先通过 Document Loaders 模块加载不同类型的数据源，比如网络的网页、本地的文档等。第二步是通过 Text Splitters 把第一步获取的文本进行分割，因为 LLM 都会限制上下文窗口大小，有 4k、16k、32k 等。把文本分割成制定大小的 chunk，可以更好的控制 token 大小。第三步就是通过 Vectorstores 模块把文本转换为向量，并存入向量数据库中。我们可以使用 OpenAI 的 Embedding API，也可以使用 HuggingFace 的 Embeddings 加载本地模型，可以节省调用费用，提高数据安全性。最后一步通过 Retriever 模块根据相关性从向量数据库中检索对应的文档。  
Chains 模块通过 Chain 的概念，来将各个组件进行链接，来简化开发复杂应用的难度。Chain 有很多种类型，主要的有 LLM Chain、Sequential Chain 和 Router Chain。LLM Chain 是最常用的，由 PromptTemplate、LLM 和 OutputParser 组成。LLM 输出是文本，OutputParser 可以让 LLM 结构化输出并且进行结果解析，比如指定输出为 JSON 格式，这样方便后续的处理。Sequential Chain 是顺序链，可以完成多个步骤的任务，每个步骤都有一个或多个输入和输出，上一个步骤的输出就是下一个步骤的输入，有点像编程中管道的概念。比如我们可以先将文本进行翻译，再对翻译后的文本进行总结。Router Chain 更加复杂，它是根据输入来动态选择下一个 chain。它由两个组件组成，分别是路由器链本身，负责选择调用下一个链。路由器链又分两种，一种是 LLM Router Chain，由 LLM 做路由决策，一种是 Embeding Router Chain，通过向量搜索做路由决策。另一个组件是目标链列表，也就是路由器链可以路由到的子链。  
Agent 模块也就是代理模块。在代理出现之前，我们使用 LLM 的方式通常是给定一个较为复杂的目标，LLM 无法一次性完成，然后我们还需要设计为了达成目标的每一个步骤的 Prompt。代理模式就是我们只需要给定一个目标，由 LLM 自己去思考每个步骤。代理模块主要由 4 个组件组成，分别是代理 Agent、工具 Tools、工具包 Toolkits 和代理执行器 Agent Executor。Agent 的作用是调用 LLM，获取下一步的 Action；Tools 是 Agent 可以调用的方法列表，LangChain 内部有很多工具，比如查询数据库、发送邮件、处理 JSON 等等，Tool 有一个 description 属性，LLM 会通过这个属性来决定是否使用这个工具；ToolKits 是一组工具集，为了实现某个特定目标而提供的工具集合；Agent Executor 负责迭代运行代理的每个 Action。从功能上看，Agent 模块和 Chains 模块的功能有些相似，不过 Chains 是由开发者预先定义好一系列 Action，再由 LLM 从其中选择最合适的 Action。Agent 是由 LLM 自己来定义并执行 Action。所以我们可以把 Agent 理解成自由度和随机性更高的 Chain。  
Memory 模块也叫记忆模块或者存储模块，它的主要作用是用来存储之前交互的信息。无论是 Chain 还是 Agent，每次交互都是无状态的，我们无法在当前交互中得知之前历史交互的信息。Memory 就是一种可以跨多轮交互提供历史上下文的能力。Memory 支持在多种存储中存储历史数据，比如 MongoDB、SQLite 和 Redis 等。它还支持通过 Buffer Memory 直接在内存中存储信息。存储的形式主要有三种，第一种是 ConversationSummaryMemory，也就是用摘要的形式保存记录；第二种是 ConversationBufferWindowMemory，用原始形式保存最近的 N 条记录；第三种是 ConversationBufferMemory，用原始形式保存所有记录。  
Callbacks 模块的主要作用就是在 LLM 执行的各个流程环节中插入回调函数，来获取整个 LLM 执行的所有参数，适合做监控、日志记录等工作。相比较其他模块，回调函数可以在任意的环节进行，比如产生新的 Token、链的开始和结束、代理 Action 的开始和结束、Tool 的开始和结束等。

#### 针对一个AI大模型相关的项目，如何思考解决思路？
我认为 AIGC 项目需要从四个方面进行规划。  
第一个方面是安全，安全对 AIGC 产品来说是至关重要的，如果没有做好安全，产品将面临约谈和关闭。安全的第一层就是内容安全，我们需要检测涉政、色情、暴恐、反人类等内容，我们可以在输入和输出两个层面进行过滤。必要的情况，可以对生产的内容进行标注和跟踪，比如在图片使用水印和在文字中按特定方式插入零宽字符进行追溯。或者对用户采取实名制措施。安全的第二个部分是虚假信息，也就是幻觉。在 GPT-4 之前出现幻觉的概率很高，GPT-4 可以很大程度降低幻觉出现的概率，一些关键逻辑可以使用 GPT-4。安全的第三部分是模型的安全，每个模型都有后门，通过一些特殊的输入就可以打破模型的限制，或者一些特殊的输入就可以让模型做一些奇怪的行为。模型攻击分好几类，比如威胁攻击、后门攻击、模型越狱等。我们需要了解模型的特点，在应用层面进行防护，防止被攻击。  
第二个方面是吞吐量。我们需要衡量用户的体量、以及在高峰期用户的请求峰值。很多 LLM 都会有使用限制，我们需要在高并发情况下对用户请求进行排队，必要情况下，可以使用池技术来变相提升 LLM 的并发处理能力。  
第三个方面是成本。不同的模型、使用频率、平均文本量等因素，成本是不同的。比如 GPT4 的平均每个 Token 是 0.06 美元，是 GPT3.5 的 30 倍。图像生成模型通常也是按照分辨率来计费的。比如 DALL-E 的价格是 1024*1024 的图片每张价格是 0.02 美元。而 Midjourney 则是按照每月来收取会员费。除了闭源模型之外，我们还可以选择开源模型，这样在成本上可以进一步节省。比如可以自己搭建 Llama 和 SD。这时候就需要设计流量的削峰填谷，加入排队机制，最大程度上合理化的使用服务器。  
第四个方面是合规。目前 AI 生成的内容在版权上一直有争议。中国目前的《著作权法》无法认定 AI 作品的作者。所以目前所有 AIGC 商业化都可能会涉及到侵权问题。我们应该尽量做到数据脱敏和匿名化。必要的情况下，可以采取对内容做隐形水印、用户实名制等措施。  
目前 AIGC 项目的技术方案比较单一，我会优先使用最成熟的 LangChain。模型方面没有特殊要求我会使用 GPT4。

#### 为什么大模型推理的时候，显存占用增加并且持续占用的现象
显存一直大型模型在推理时显存占用增加并且持续占用的现象，可能由以下几个原因导致：

1.  **激活和中间状态**：在推理过程中，模型需要存储每一层的激活输出以及中间计算状态。对于大型模型，这些激活和状态可能占用大量显存。 
2.  **批处理**：为了提高推理效率，通常会使用批处理，即同时处理多个输入样本。每个样本的激活和梯度信息都需要额外的显存。 
3.  **优化器状态**：如果使用了某些优化器（如Adam或RMSprop），它们会为每个参数维护额外的状态信息，如梯度的一阶和二阶矩。这些状态信息也会占用显存。 
4.  **并行处理**：在GPU上并行处理数据时，可能需要额外的缓冲区来协调不同处理单元之间的数据传输。 
5.  **内存分配策略**：深度学习框架（如TensorFlow或PyTorch）通常会在开始时分配足够的显存来满足整个模型的需求，即使在推理过程中某些显存可能暂时未被使用。 
6.  **框架开销**：深度学习框架本身也会占用一定的显存，用于管理模型的执行、存储操作和调试信息等。 
7.  **显存碎片化**：在推理过程中，可能会有一些小的显存块被释放，但由于显存碎片化，这些小块显存可能无法被有效利用。 
8.  **未释放的内存**：在某些情况下，即使推理任务完成，框架可能没有及时释放不再需要的显存，导致显存占用看起来仍然很高。 

为了减少显存占用，可以采取以下措施：

+ **减小批量大小**：减少同时处理的样本数量可以降低显存占用，但可能会降低推理效率。
+ **使用混合精度训练**：通过使用半精度浮点数（FP16）代替全精度（FP32），可以减少显存占用并可能加速推理。
+ **优化模型结构**：对模型进行剪枝或量化，减少模型大小和复杂度，从而降低显存需求。
+ **清理缓存**：在推理过程中定期清理不再需要的中间变量和缓存，可以帮助释放显存。
+ **使用显存管理工具**：一些深度学习框架提供了显存管理工具，可以帮助监控和优化显存使用。

需要注意的是，显存占用的具体情况会根据所使用的深度学习框架、模型架构、硬件配置以及推理任务的具体需求而有所不同。在实际应用中，可能需要根据具体情况进行调整和优化。

#### 为什么要进行增量训练
增量预训练是在已有的预训练模型基础上，使用新的数据集继续进行预训练的过程。这种方法常常用于适应新的领域、数据分布，或者包含最近信息的数据集。增量预训练有几个关键的原因和好处：

1. 适应新领域

当预训练模型需要应用于特定领域（如医疗、法律或金融）时，原始预训练数据可能无法充分覆盖该领域的专业术语和知识。通过使用特定领域的数据进行增量预训练，模型可以更好地理解和生成相关领域的文本，提高在该领域内的表现。

2. 更新模型知识

随着时间的推移，许多领域的知识和信息会发生变化。通过增量预训练，可以用最新的数据更新模型，使其反映当前的知识和信息状态，保持模型的时效性和相关性。

3. 提高数据效率

直接在大量通用数据上从头开始训练一个全新的模型非常耗时且计算成本高昂。相比之下，增量预训练可以更有效地利用新数据，通过相对较少的计算资源改进模型，尤其是当新数据量相对较小时。

4. 减少遗忘

在连续学习的场景中，模型需要学习新任务而不遗忘旧任务。通过增量预训练，模型可以在保留已学习知识的基础上，逐步吸收新信息，减少灾难性遗忘（catastrophic forgetting）的风险。

5. 自定义和优化

增量预训练允许用户根据自己的数据和任务需求自定义模型。用户可以选择最相关的数据进行增量预训练，使模型更好地适应特定的应用场景。

#### 介绍下Prefix-tuning
Prefix-tuning 是一种针对大型语言模型的微调方法，特别适用于自然语言生成任务。与传统的微调方法相比，prefix-tuning 的主要特点是它固定了原有的语言模型参数，只对模型输入序列的一小部分进行优化。这部分被称为"prefix"，即一系列连续的、特定于任务的向量。通过这种方法，prefix-tuning 能够在维持模型原有能力的同时，针对特定任务进行有效的调整。

在模型结构上，prefix-tuning 主要是在模型的每一层都加入了prefix参数。这些prefix可以看作是一些虚拟的输入tokens，使得语言模型（如Transformer）能够对这些虚拟tokens实施attention处理，而这些prefix的参数与真实token的参数是完全独立的。

实际应用中，prefix-tuning 的过程包括：

1. 初始化一个可训练的参数矩阵，用于存储prefix向量参数。

2. 在模型训练过程中，这些prefix参数会被优化以适应特定任务。

3. 使用稳定的优化过程，如AdamW和线性学习率衰减策略，以提高训练的稳定性和效果。

prefix-tuning 提供了一种资源高效且灵活的方法，用于微调大型语言模型，特别适合于资源有限但需要高效利用预训练模型的场景。

#### prefix-tuning的优点
1. 参数数量减少：与传统的微调相比，prefix-tuning 只需调整极小比例的参数（通常少于0.1%），大大减少了计算资源的需求。

2. 灵活性高：可以针对不同任务调整特定的prefix，而无需更改整个模型的参数。

3. 适用性广泛：对于各种自然语言生成任务均有良好的适用性，包括文本摘要、文本生成等。

#### 介绍下P-Tuning v1
P-Tuning v1 是由清华大学人工智能研究院提出的一种基于提示的微调方法，用于提升预训练语言模型在自然语言处理任务中的性能。P-tuning v1 是一种针对大型预训练语言模型（如GPT）的微调方法，它旨在通过引入可学习的提示（prompts）来适应特定的下游任务，而无需对整个模型进行大规模的参数更新。这种方法的核心思想是在模型的输入层之上添加一个可训练的嵌入层，这个嵌入层被称为“prompt embedding”，它将传统的离散提示转换为连续的向量表示。

P-tuning v1的关键特点：

1.  **可学习的Prompt Embedding**：P-tuning v1通过将prompt转换为可学习的嵌入层，使得这些prompt在训练过程中可以与模型的其他参数一起更新。这种方法允许模型根据任务的需要调整prompt的表示。 
2.  **MLP+LSTM处理**：P-tuning v1使用多层感知机（MLP）和长短期记忆网络（LSTM）来处理prompt embedding。这种结构使得模型能够捕捉prompt中的序列信息，并将其与输入文本的嵌入相结合。 
3.  **任务和规模的通用性**：尽管P-tuning v1在某些复杂的自然语言理解任务（如序列标注）上的表现可能不如其他方法，但它提供了一种相对通用的微调策略，适用于不同规模的预训练模型。 
4.  **参数效率**：与传统的微调方法相比，P-tuning v1只更新模型的一小部分参数，这使得它在参数效率上具有优势。然而，这也意味着它可能无法充分利用预训练模型的全部能力，特别是在较小规模的模型上。 
5.  **灵活性**：P-tuning v1允许在输入层的任何位置插入可训练的prompt，这提供了灵活性，使得模型可以根据不同任务的需求调整prompt的位置。 

P-tuning v1的局限性：

尽管P-tuning v1提供了一种有效的微调策略，但它也存在一些局限性。例如，由于它只更新了模型的一小部分参数，这可能限制了模型在某些任务上的性能。此外，P-tuning v1在处理更复杂的任务时可能不如其后续版本P-tuning v2表现良好，后者在每个隐藏层中都添加了可训练的线性层，从而提高了模型的灵活性和性能。

P-tuning v1是微调大型预训练语言模型的一种创新方法，它通过引入可学习的prompt embedding来适应特定任务，同时保持了参数更新的效率。尽管它在某些方面可能不如后续版本，但它为后续的微调技术发展奠定了基础。

#### 介绍下P-Tuning v2
P-tuning v2 是一种针对大型预训练语言模型的微调方法，它在P-tuning v1的基础上进行了改进，以提高模型在各种自然语言理解（NLU）任务上的通用性和性能。P-tuning v2的核心思想是在预训练模型的每个隐藏层中添加可训练的连续提示（continuous prompts），这些提示仅包含少量的参数，但可以显著提高模型对特定任务的适应性。

P-tuning v2的关键特点：

1.  **多层连续提示**：与P-tuning v1不同，P-tuning v2在模型的每个Transformer层中都添加了连续提示，这使得模型能够更灵活地适应不同的任务。 
2.  **参数效率**：P-tuning v2仅调整模型的一小部分参数（0.1%-3%），这大大减少了训练时的内存消耗和每个任务的存储成本。 
3.  **通用性**：P-tuning v2在多种模型规模和NLU任务上都显示出与微调（fine-tuning）相当甚至更优的性能，这表明了其在不同场景下的通用性。 
4.  **性能提升**：P-tuning v2通过在每个隐藏层添加可训练的提示，提高了模型的性能，尤其是在处理复杂的序列标注任务时。 
5.  **优化和实施细节**：P-tuning v2的实现考虑了一系列优化和实施的细节，如重参数化（reparameterization）的使用、提示长度的选择、多任务学习的应用等，以确保与微调相当的性能。 
6.  **适用性**：P-tuning v2适用于从330M到10B不同参数规模的模型，这表明它在不同大小的模型上都能有效工作。 

P-tuning v2的优化策略：

+ **前缀提示策略**：将提示信息添加到模型的每一层中，提高了模型的输出准确性。
+ **自适应优化策略**：根据模型在训练过程中的表现，动态调整微调参数的权重，以提高模型的收敛速度和性能。

P-tuning v2在大模型上的应用：

在大模型上应用P-tuning v2时，需要特别注意模型规模、计算资源、提示信息的选择和训练策略。P-tuning v2在大模型上的应用能够有效减少微调过程中的计算资源和时间消耗，同时保持或提高模型的性能。

P-tuning v2通过在每个Transformer层中添加可训练的连续提示，提高了模型的通用性和性能，使其成为一种有效的微调方法，适用于各种规模的预训练模型和NLU任务。

#### 介绍下Adapter Tuning
Adapter Tuning 是一种用于在特定任务上微调预训练语言模型 (LLM) 的方法。它在 LLM 的每个 Transformer 层中插入一个称为 Adapter 的小型附加模块，并在微调过程中仅训练这些 Adapter 和 LayerNorm 层的参数，而保持 LLM 主体参数不变。

**Adapter Tuning 的优势:**

+ **高效性:** 只需训练少量参数，即可有效利用 LLM 的知识，降低训练成本和时间。
+ **灵活性:** 可以在不同的 LLM 和任务上进行应用，具有较强的通用性。
+ **可解释性:** 可以通过分析 Adapter 参数来理解模型学习到的特定任务知识。

**Adapter Tuning 的具体方法:**

1. **模型准备:** 选择合适的 LLM 和预训练模型，并根据任务需求构建训练数据集。
2. **Adapter 插入:** 在 LLM 的每个 Transformer 层中插入 Adapter 模块。Adapter 的结构可以是简单的线性层，也可以是更复杂的多层网络。
3. **微调训练:** 冻结 LLM 主体参数，仅训练 Adapter 和 LayerNorm 层的参数。可以使用不同的优化算法和学习率策略。
4. **模型评估:** 在验证集和测试集上评估模型的性能，并根据需要进行模型调整。

**Adapter Tuning 的应用:**

Adapter Tuning 已被广泛应用于各种自然语言处理任务，包括：

+ **文本分类:** 将文本分类到不同的类别中。
+ **情感分析:** 判断文本的情感倾向。
+ **命名实体识别:** 识别文本中的命名实体。
+ **机器翻译:** 将文本翻译成另一种语言。
+ **文本生成:** 生成文本。

**Adapter Tuning 的局限性:**

+ **依赖预训练模型:** Adapter Tuning 的效果依赖于预训练模型的质量。
+ **参数设计:** Adapter 的结构和参数需要根据任务进行调整，这需要一定的经验和技巧。

Adapter Tuning 是一种高效、灵活且可解释的微调方法，可以有效提升 LLM 在特定任务上的性能。然而，它也存在一些局限性，例如依赖预训练模型和参数设计等。

**以下是一些关于 Adapter Tuning 的额外信息:**

+ Adapter Tuning 可以与其他微调方法，例如 Prompt Tuning 结合使用，以获得更好的效果。
+ Adapter Tuning 可以用于多任务学习，在多个任务上同时训练模型。
+ Adapter Tuning 还可以用于模型压缩，通过去除不重要的 Adapter 来减小模型大小。

#### 介绍下Lora
LoRA（Low-Rank Adaptation of Large Language Models）是一种针对大型预训练语言模型（如GPT-3、BERT等）的参数高效微调方法。这种方法旨在减少微调过程中所需的计算资源和存储需求，同时保持或提高模型在特定任务上的性能。LoRA的核心思想是在模型的注意力机制中引入低秩矩阵，通过这些矩阵来适应新的任务，而不是更新模型的所有参数。

LoRA的关键特点：

1.  **参数效率**：LoRA通过仅更新模型中的一小部分参数（低秩矩阵），显著减少了微调过程中的参数量。这使得LoRA成为一种轻量级的微调方法。 
2.  **低秩矩阵**：LoRA在模型的注意力权重矩阵（如查询（Query）、键（Key）、值（Value）矩阵）中引入低秩矩阵。这些矩阵通过降维和升维操作来模拟原始权重矩阵的低秩结构。 
3.  **保持预训练参数不变**：在LoRA中，预训练模型的主体参数保持不变，只有低秩矩阵在微调过程中被更新。这有助于保留模型在预训练阶段学到的知识和特征。 
4.  **适用性**：LoRA适用于各种规模的预训练模型，并且可以应用于多种不同的NLP任务。 
5.  **训练效率**：由于参数量的减少，LoRA可以更快地完成微调，这对于需要快速迭代和部署的应用场景非常有用。 

LoRA的工作原理：

在Transformer模型的注意力机制中，LoRA主要应用于查询（Q）、键（K）、值（V）的权重矩阵。具体来说，LoRA通过以下步骤进行：

1.  **初始化**：在模型的注意力层中添加两个小型的全连接层（或称为投影层），分别用于降维和升维操作。这些层的参数在初始化时接近零，以确保微调的有效性。 
2.  **微调**：在微调过程中，只有这些新增的低秩矩阵的参数会被更新，而模型的其他参数保持不变。这样，LoRA可以学习到与特定任务相关的新知识，同时保留预训练模型的通用知识。 
3.  **输出**：在模型的输出端，LoRA将低秩矩阵的输出与原始的注意力输出相结合，以生成最终的输出。 

LoRA的优势：

+ **资源节约**：LoRA显著减少了微调所需的显存和计算资源，使得在资源受限的环境中也能有效地微调大型模型。
+ **快速部署**：由于参数量的减少，LoRA可以快速部署到生产环境，加速模型的迭代和更新。
+ **性能保持**：LoRA在减少参数量的同时，仍能保持或提高模型在特定任务上的性能。

LoRA作为一种新兴的微调技术，为大型预训练语言模型的高效微调提供了一种有效的解决方案，尤其适用于资源受限或需要快速迭代的场景。

#### 介绍下QLora
QLoRA（Quantized Low-Rank Adaptation）算法是一种针对大型预训练语言模型（如GPT-3、BERT等）的高效微调方法，旨在减少微调过程中的内存占用，同时保持或接近全精度微调的性能。

QLoRA算法的核心原理是在保持预训练模型权重不变的情况下，通过引入低秩适配器（LoRA）和量化技术来适应特定任务。这种方法通过量化预训练模型的权重到4位精度，并在每个Transformer层中添加小型的可学习适配器，使得模型能够在不更新全部参数的情况下适应新任务。QLoRA利用了NormalFloat（NF4）量化和双重量化技术来优化内存使用，同时通过分页优化器管理内存峰值，从而在资源有限的硬件上实现大型模型的微调。

QLoRA的训练目标是实现大型预训练语言模型的高效微调，以便在保持或接近16位微调性能的同时，显著减少所需的内存资源。这使得大型模型的微调变得更加可行，尤其是在资源受限的环境中。

训练步骤：

1.  **量化预训练模型**：将预训练模型的权重量化为4位精度，通常使用NormalFloat（NF4）量化技术，以适应正态分布的权重。 
2.  **添加低秩适配器**：在预训练模型的每个Transformer层中添加低秩适配器（LoRA）。这些适配器是小型的、可学习的参数集，它们通过反向传播梯度来优化，而预训练模型的主体参数保持固定。 
3.  **双重量化**：对量化常数进行二次量化，以进一步减少内存占用。这通常涉及到使用8位浮点数进行第二次量化，以节省内存。 
4.  **分页优化器**：使用NVIDIA的统一内存特性，当GPU内存不足时，自动将部分数据转移到CPU内存中，以管理内存峰值。 
5.  **反向传播与优化**：通过量化的权重和适配器进行反向传播，更新适配器的参数，而预训练模型的权重保持不变。 
6.  **性能评估**：在微调后，对模型进行性能评估，确保微调后的模型在特定任务上达到预期的性能水平。 

QLoRA算法通过这些步骤实现了大型模型的高效微调，为资源受限的研究和开发提供了新的解决方案。

#### 介绍下模型编辑
模型编辑（Model Editing）是一种特殊的模型修改策略，它旨在精确地修改预训练模型以便编码特定的知识，同时最大限度地保留现有知识，并确保不影响模型在不相关输入上的行为。在大型语言模型（LLM）的背景下，模型编辑特别关注两个关键特性：局部性和泛化性。

1.  **局部性（Locality）**：编辑后的模型不应无意中影响其他无关的具有不同语义输入的输出。例如，当更新关于美国总统的信息时，模型编辑应确保模型对英国首相的知识不受影响。 
2.  **泛化性（Generalizability）**：编辑后的模型应能够泛化到与编辑后知识相关的更广泛输入。这意味着模型在处理与编辑知识语义相似的输入时，能够表现出一致的行为。 

模型编辑在自然语言处理（NLP）领域中的应用包括执行关于文本信息的编辑，例如通过受约束的微调损失来修改基于Transformer模型中的特定事实知识。此外，模型编辑也涉及到如何将新知识引入预训练的LLM，以及如何在不重新训练整个模型的情况下，更新模型以包含新知识。

在LLM的背景下，模型编辑（KME）的任务是精确修改预训练LLM的行为，以便引入新知识，保持LLM的当前性和相关性，同时不会对其他与编辑无关的预训练知识产生负面影响。这通常通过引入辅助网络、更新部分参数或使用特定的编辑策略来实现，旨在提高模型的实用性和效率。

#### 模型编辑常见的方法有哪些
1. 基于**外部记忆**的方法，利用外部存储器来存储新知识进行编辑，无需修改预训练的权重，其中预训练知识可以完全保留在LLM权重中。通过用外部参数存储新知识，基于记忆的策略能够以良好的可扩展性精确地表示新知识，因为存储器很容易扩展以包含新知识。
2. 基于**全局优化**的方法，寻求在新知识的指导下进行优化，将新知识普遍纳入预训练的LLM，其中引入了量身定制的策略来限制其他预训练知识的影响，与天真的微调区分开来。然而，当应用于LLM时，这些方法可能在编辑效率方面达不到要求，因为需要优化大量参数。
3. 基于**局部修整**的方法，旨在定位LLM中特定知识的相关参数，并对其进行相应更新，纳入与编辑相关的新知识。局部修整的主要优点是可以只更新一小部分模型参数，从而与基于记忆的方法相比提供了相当大的存储效率，与全局优化相比提供了计算效率。

如图是三种KME方法的示意图：

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1710728491382-639e76c4-31e5-4bec-9bb7-4c11c1ee244b.png)

#### 什么是大模型幻觉？
幻觉在语言模型中指的是虽然在语法和流畅性上正确，但在忠实度（Factuality hallucination: 生成内容与现实世界事实不一致）或真实性（Faithfulness hallucination: 生成的内容与用户指令或输入提供的上下文偏离）方面出现偏差的文本生成。这包括与输入信息冲突的内在幻觉和产生难以验证信息的外在幻觉。

大模型幻觉的类型:

+ 事实性幻觉: LLM 生成的内容与现实世界事实不一致。例如， LLM 可能会错误地声称地球是平的。
+ 忠实性幻觉: LLM 生成的内容与用户输入或上下文不一致。例如，用户要求 LLM 总结一篇文章，但 LLM 生成的摘要却与文章内容无关。

大模型幻觉的原因:

+ 数据偏差: LLM 是基于大量数据进行训练的，如果训练数据存在偏差，模型可能会学习到错误的知识，并在生成内容时出现幻觉。
+ 缺乏常识: LLM 缺乏人类的常识，因此可能会生成逻辑上不合理或与现实世界不符的内容。
+ 过度拟合: LLM 在训练过程中可能会过度拟合训练数据，导致其在生成内容时过于依赖训练数据中的模式，而无法泛化到新的情况。

大模型幻觉的危害:

+ 误导性信息: LLM 生成的幻觉可能会误导用户，使其相信错误的信息。
+ 偏见: LLM 生成的幻觉可能会包含偏见，例如性别歧视或种族歧视。
+ 不安全: LLM 生成的幻觉可能会被用于恶意目的，例如生成虚假新闻或网络攻击。

#### 如何缓解大模型幻觉？
数据层面:

+ 使用高质量、无偏的数据训练模型: 这是缓解大模型幻觉问题最根本的方法。可以通过人工筛选、数据增强等方式来提高数据质量。
+ 增强数据的多样性: 训练数据应该尽可能包含各种不同的观点和信息，以避免模型学习到偏见。

模型层面:

+ 改进模型架构: 可以通过改进模型架构来增强模型的泛化能力，使其能够更好地处理新的情况。
+ 加入对抗训练: 对抗训练可以帮助模型学习区分真实信息和虚假信息，从而降低幻觉的发生概率。
+ 使用正则化技术: 正则化技术可以帮助模型避免过度拟合训练数据，从而提高模型的泛化能力。

训练层面:

+ 使用更有效的训练方法: 例如，可以使用带有奖励机制的强化学习来训练模型，引导模型生成更符合事实和逻辑的内容。
+ 提前终止训练: 在模型开始出现过度拟合之前提前终止训练，可以有效避免幻觉的发生。

评估层面:

+ 使用严格的评估指标: 在部署模型之前，对其进行严格的评估，以确保其不会产生幻觉。
+ 结合人工评估: 人工评估可以帮助识别模型无法识别的幻觉。

其他方法:

+ 使用知识库: 将外部知识注入模型，可以帮助模型学习到正确的知识，从而降低幻觉的发生概率。
+ 解释模型输出: 解释模型输出可以帮助我们理解模型生成内容的原因，从而识别潜在的幻觉。

#### 什么是量化？
根据提供的链接内容，量化是一种在深度学习领域中用于模型压缩的技术，旨在减少模型的存储大小和推理时的内存消耗，同时提高推理速度。以下是对量化的全面、详细解释：

量化的基本概念

量化是将模型中的浮点型权重近似为有限多个离散值的过程。这通常通过减少表示浮点数据所需的位数来实现。例如，将权重从32位浮点数（float32）量化为8位整数（int8），可以显著减少模型尺寸和内存占用，同时在支持低精度运算的处理器上加快推理速度。

量化的主要方法

量化是提升大模型推理速度和效果的方法之一，还包括：

1. **剪枝（Pruning）**：通过移除模型中的一些权重来减少模型大小。
2. **知识蒸馏（Knowledge Distillation）**：训练一个小型模型来模仿大型模型的行为。
3. **量化（Quantization）**：将浮点数权重转换为整数，以减少所需的存储空间和计算资源。

量化的详细过程，量化过程涉及以下几个关键步骤：

1.  **确定量化比特数**：根据业务需求，将模型量化为不同比特数的模型。8比特量化是工业界最常用的，而1比特量化可以将模型压缩至原来的1/32。 
2.  **量化对象**：量化可以应用于模型的权重、激活值、KV缓存和梯度等。 
3.  **量化形式**：量化方法根据数据表示的原始数据范围是否均匀，可以分为线性量化和非线性量化。线性量化是实际应用中更常见的方法。 
4.  **量化分类**：根据量化压缩模型的阶段，模型量化可以分为量化感知训练（QAT）、量化感知微调（QAF）和训练后量化（PTQ）。 

量化感知训练（QAT）

QAT在模型训练过程中加入伪量化算子，通过训练时统计输入输出的数据范围来提升量化后模型的精度。这种方法使模型在训练过程中适应低精度表示，增强其处理由量化引起的精度损失的能力。

训练后量化（PTQ）

PTQ在模型训练完成后对其参数进行量化，主要目标是减少模型的存储和计算复杂性。PTQ可以进一步分为权重量化和全量化。权重量化仅量化模型的权重，而全量化还会量化模型的激活值。

+ **LLM-QAT**：利用预训练模型生成的结果来实现无数据蒸馏，支持更长的序列依赖。
+ **PEQA**：采用双阶段过程运行，压缩模型大小，降低推理延迟。
+ **QLORA**：引入新的数据类型和优化器，节省内存。
+ **PTQ**：简单高效的量化方法，可能会引入精度损失。
+ **LUT-GEMM**：通过优化矩阵乘法提高计算效率。
+ **LLM.int8()**：采用混合精度分解的量化方法。
+ **ZeroQuant**：对权重进行group-wise量化，对激活值进行token-wise量化。
+ **GPTQ**：对权重和激活值逐个量化，需要校准数据集。
+ **AWQ**：通过保留显著权重来减少量化误差。
+ **SpQR**：隔离异常权重，并将其他权重压缩为3-4比特。

量化是大模型压缩和加速的关键技术之一。通过上述方法，可以在保持模型性能的同时，显著降低模型的存储和计算需求，从而使得大型深度学习模型更易于部署和使用。

#### 为什么需要RAG？
LLM存在以下几点局限性，使用RAG的方法可以在一定程度上缓解。

+ **幻觉问题**：LLM 文本生成的底层原理是基于概率的 token by token 的形式，因此会不可避免地产生“一本正经的胡说八道”的情况。
    - RAG缓解方式：在LLM推理阶段，RAG将检索到的与用户输入相关的信息补充到LLM的输入中，以使得LLM的输出尽可能以检索到的“证据”为基础，因此可以在一定程度上减轻“幻觉”问题。
+ **时效性问题**：LLM 的规模越大，大模型训练的成本越高，周期也就越长。那么具有时效性的数据也就无法参与训练，所以也就无法直接回答时效性相关的问题，例如“帮我推荐几部热映的电影？”。
    - RAG缓解方式：由于RAG的检索数据源来自于外部数据，外部数据可以实时更新，因此当LLM在推理阶段会接受到外部的新知识，从而回答时效性相关的问题。
+ **数据安全问题**：通用的 LLM 没有企业内部数据和用户数据，如果企业需要定制一个行业模型，需要将数据提供给LLM服务厂商来进行微调训练，因此会有数据泄露的风险。
    - RAG缓解方式：RAG的检索数据库可以部署到本地，企业数据的业务计算全部在本地完成，在线的LLM仅完成一个推理归纳的功能。

#### 说下RAG局限性
+ **上下文长度**：由于LLM的输入存在上下文长度的限制，因此RAG检索出的信息需要考虑长度的问题。检索结果如果过长会导致信息被稀释，如果过短会导致信息丢失的问题。
+ **鲁棒性**：检索过程中的噪声或者矛盾信息会对检索结果的质量造成影响。对LLM的输入而言，错误的信息比没有的信息要更糟糕。
+ **RAG评估中使用的LLM**：RAG的生成阶段评估，需要用到LLM来对生成结果与用户输入和检索结果来进行一致性校验，这取决于使用的LLM本身的能力。
+ **生产使用的RAG**：由于RAG的实用性，使得其在工业生产中广泛应用，但是RAG在检索阶段的效率和召回率都需要进一步的优化，并且需要确保数据安全问题（防止LLM无意中泄露文档源或元信息）

#### 对比分析当前向量知识库
| 向量数据库名称 | 是否开源 | 托管方式 | 优点 | 缺点 |
| --- | --- | --- | --- | --- |
| Vespa | 开源 | 本地/云 | 1. 提供了最“企业级就绪”的混合搜索能力，将**关键字搜索和自定义向量搜索与HNSW相结合**。 | 1. 与使用性能导向语言（如Go或Rust）编写的更现代的替代方案相比，**开发人员体验不够流畅，**这是由于应用层是用Java编写的。 |
| Weaviate | 开源 | 本地/云 | 1. 令人惊叹的文档（包括技术细节和持续实验）并且通过Docker**非常容易上手**。 | 1. 在数据变得非常大时**需要大量的基础设施资源**（与Milvus类似） |
| Milvus | 开源 | 本地 | 1. Milvus**非常成熟**，并提供了许多向量索引的选项<br/>2. 它完全使用Golang构建，**具有极强的可扩展性**。<br/>3. 截至2023年，它是唯一一个提供可工作的DiskANN实现的主要供应商，据说这是磁盘上**最高效的向量索引**。 | 1. 数据的大小不是太大时，Milvus可能会显得**过于复杂** |
| zilliz（Milvus的母公司） | 闭源 | 云 | | |
| Vald | 开源 | 本地/云 | 1. 通过**高度分布式**的架构，设计用于处理多模态数据存储，同时具有**索引备份**等有用功能。<br/>2. 使用**非常快速**的ANN搜索算法NGT（邻域图和树），当与高度分布式的向量索引结合使用时，它是最快的ANN算法之一。 | 1. **没有那么多的关注度和使用量**，并且文档没有明确描述使用了什么向量索引<br/>2. 主要满足Yahoo! Japan的搜索需求 |
| Qdrant | 开源 | 本地/云 | 1. 有很好的文档，可以帮助开发人员通过Docker**轻松上手**<br/>2. 它完全使用Rust构建，提供了**开发人员**可以通过其Rust、Python和Golang客户端访问的API | 1. **相对较新**的工具，Qdrant在查询用户界面等距离成熟的产品还有些差距，但在快速缩小这个差距 |
| Pinecone | **闭源** | 云 | 1. **非常容易上手**（无需托管负担，完全云原生），不需要用户了解向量化或向量索引的任何知识。 | 1. **完全专有**，无法了解其内部运作和路线图 |
| Elasticsearch | 开源 | 本地/云 | 1. 利用它们的向量索引和搜索功能是**相当简单**的，无需使用新技术 | 1. 现有的数据库不一定以最优的方式存储或索引数据，因为它们被设计为通用目的，结果是，在涉及百万级向量搜索及以上规模的数据时，**性能会受到影响** |
| Redis | 开源 | 本地 | 1. 利用它们的向量索引和搜索功能是**相当简单**的，无需使用新技术 | 1. 现有的数据库不一定以最优的方式存储或索引数据，因为它们被设计为通用目的，结果是，在涉及百万级向量搜索及以上规模的数据时，**性能会受到影响** |
| Chroma | 开源 | 本地/云<br/>/嵌入式 | 1. 为**开发人员**提供了**方便**的Python/JavaScript接口，可以快速启动向量存储。<br/>2. 是市场上第一个默认提供嵌入模式的向量数据库，其中数据库和应用层紧密集成，**使开发人员能够快速构建、原型设计和展示他们的项目**。 | 1. Chroma主要是一个围绕现有的OLAP数据库（Clickhouse）和现有的开源向量搜索实现（hnswlib）的Python/TypeScript封装。它**没有实现自己的存储层**。 |
| LanceDB | 开源 | 云/嵌入式 | 1. LanceDB专为**多模态数据**（图像、音频、文本）的分布式索引和搜索而设计，构建在Lance数据格式之上，这是一种创新的、用于机器学习的新型列式数据格式。<br/>2. 利用Rust的**速度、内存安全性和相对较低资源利用率**的主要向量数据库供应商 | 1. LanceDB是一个**非常年轻**的数据库，因此许多功能正在积极开发中 |
| pgVector | 开源 | 本地 | 1. 利用它们的向量索引和搜索功能是**相当简单**的，无需使用新技术 | 1. 现有的数据库不一定以最优的方式存储或索引数据，因为它们被设计为通用目的，结果是，在涉及百万级向量搜索及以上规模的数据时，**性能会受到影响** |


注：

1. HNSW（Hierarchical Navigable Small-World graphs）

结论：

1. 开源和托管云：如果您倾向于开源解决方案，Weviate、Milvus和Chroma将成为顶级竞争者。Pinecone虽然不是开源的，但凭借其开发人员经验和强大的完全托管解决方案而大放异彩。
2. 性能：在每秒查询率的原始性能方面，Milvus领先，紧随其后的是Weviate和Qdrant。然而，在延迟方面，Pinecone和Milvus都提供了令人印象深刻的2毫秒以下的结果。
3. 社区优势：Milvus拥有最大的社区存在，其次是Weviate和Elasticsearch。强大的社区通常会转化为更好的支持、增强和bug修复。
4. 可扩展性、高级功能和安全性：基于角色的权限改造是许多企业应用程序的关键功能，可在Pinecone、Milvus和Elasticsearch中找到。在标定方面，Milvus和Chroma提供了动态段放置，使它们适用于不断发展的数据集。如果您需要具有多种索引类型的数据库，Milvus对11种不同类型的支持是无与伦比的。虽然混合搜索得到全面支持，但Elasticsearch在磁盘索引支持方面确实不足。
5. 定价：对于预算有限的初创企业或项目，Qdrant估计的9美元50k向量定价很难被击败。另一方面，对于需要高性能的大型项目，Pinecone和Milvus提供有竞争力的定价层。

#### 简单介绍强化学习？
强化学习（Reinforcement Learning, RL）是机器学习的一个领域，它涉及智能体（agent）在环境（environment）中通过试错（trial-and-error）来学习如何做出决策的过程。在强化学习中，智能体通过与环境的交互来学习策略（policy），即在给定的状态下选择动作（action）的规则，目标是最大化累积的奖励（reward）。

**核心概念**

+ 智能体（Agent）：执行动作的实体，它试图通过学习最佳策略来最大化奖励。
+ 环境（Environment）：智能体所处的外部世界，它提供智能体的状态信息和奖励。
+ 状态（State）：环境的一个描述，代表智能体所处的情况。
+ 动作（Action）：智能体在给定状态下可以执行的操作。
+ 奖励（Reward）：智能体执行动作后从环境中获得的反馈，用于评价动作的好坏。
+ 策略（Policy）：从状态到动作的映射，定义了在特定状态下应采取的动作。
+ 价值函数（Value Function）：预测智能体从某个状态开始，遵循特定策略所能获得的累积奖励的期望值。
+ Q函数（Q-Function）：预测智能体在某个状态采取特定动作，并遵循特定策略所能获得的累积奖励的期望值。

**学习过程**

强化学习的学习过程通常涉及以下几个步骤：

1. 探索（Exploration）：智能体尝试不同的动作以发现更有价值的策略。
2. 利用（Exploitation）：智能体根据已知的信息来最大化即时奖励。
3. 学习算法：如Q学习（Q-Learning）、SARSA（State-Action-Reward-State-Action）、深度Q网络（DQN）、策略梯度（Policy Gradient）等，用于更新智能体的策略或价值函数。

**强化学习的优势:**

+ **不需要人工标注数据:** 强化学习可以直接从环境中学习，不需要人工标注数据。
+ **可以解决复杂问题:** 强化学习可以解决很多传统机器学习方法无法解决的复杂问题。

**强化学习的挑战:**

+ **样本效率低:** 强化学习通常需要大量的样本才能学习到好的策略。
+ **探索与利用的权衡:** 强化学习需要在探索新策略和利用现有策略之间进行权衡。

#### 简单介绍一下 RLHF？
RLHF（Reinforcement Learning from Human Feedback）是一种结合了强化学习和人类反馈的方法，用于训练人工智能模型。这种方法的核心思想是利用人类的直觉和判断来指导和改进智能体的学习过程，特别是在那些难以明确定义奖励函数的复杂任务中。

在许多实际应用中，定义一个能够准确反映所有期望行为的奖励函数是非常困难的。例如，在教育或游戏设计中，智能体的目标可能包括创造性、适应性和与人类用户的自然交互，这些目标很难用简单的数值奖励来量化。RLHF通过引入人类的反馈来解决这个问题，使智能体能够学习更复杂和高级的行为。

RLHF通常包含以下几个步骤：

1.  **预训练**：使用监督学习对智能体进行预训练，使其获得基本的行为模式。这可以通过模仿学习（Imitation Learning）或反向强化学习（Inverse Reinforcement Learning）等方式实现。 
2.  **人类反馈**：收集人类对智能体行为的评价。这可以通过让人类专家标注样本、通过游戏或任务与智能体互动并提供评分，或者通过自然语言指令和评价来完成。 
3.  **奖励建模**：根据收集到的人类反馈，建立一个奖励模型。这个模型可以是一个神经网络，它学习将状态和动作映射到奖励值。 
4.  **强化学习训练**：使用奖励模型来指导智能体的学习过程。智能体通过尝试不同的动作并观察奖励模型的输出来优化其策略。 
5.  **迭代改进**：智能体在环境中执行动作，收集经验，并不断更新其策略以提高性能。 

RLHF在多种场景中都有潜在的应用，例如：

+ **自然语言处理**：训练聊天机器人或助手以更自然和有帮助的方式与人类交流。
+ **游戏AI**：开发能够适应玩家行为和偏好的智能游戏对手或伙伴。
+ **模拟和教育**：创建能够在模拟环境中学习复杂任务的智能体，如驾驶模拟器或虚拟医疗助手。

RLHF面临的挑战包括如何有效地收集和利用人类反馈，如何确保奖励模型的准确性和一致性，以及如何设计能够从人类反馈中有效学习的算法。RLHF是一种强大的方法，它结合了强化学习的自适应性和人类反馈的指导性，使得智能体能够在复杂和不确定的环境中学习更加复杂和高级的行为。随着技术的发展，RLHF有望在人工智能的多个领域中发挥重要作用。

#### 奖励模型需要和基础模型一致吗？
奖励模型在RLHF（Reinforcement Learning from Human Feedback）中扮演着至关重要的角色，它通常是根据人类反馈来构建的，用以指导强化学习智能体的学习过程。奖励模型并不要求必须与基础模型（即智能体的决策模型或策略模型）完全一致，但它们应该相互兼容并协同工作。

1.  **目标一致性**：奖励模型的目的是反映人类的偏好和评价标准，而基础模型的目标是根据这些偏好来学习最优的行为策略。因此，奖励模型和基础模型的最终目标是一致的，即最大化累积奖励。 
2.  **结构差异**：奖励模型可以是基于规则的简单模型，也可以是复杂的机器学习模型，如神经网络。基础模型通常是通过强化学习算法（如Q学习、策略梯度方法等）训练得到的。两者在结构上可以有很大的不同。 
3.  **数据依赖性**：奖励模型通常依赖于人类反馈数据来训练，而基础模型则依赖于智能体与环境交互产生的数据。奖励模型提供了一种将人类直觉和专业知识整合到强化学习过程中的方法。 
4.  **迭代改进**：在RLHF中，奖励模型和基础模型可以相互促进，通过迭代过程不断改进。基础模型的行为可以用来收集更多的人类反馈，这些反馈又可以用来更新奖励模型，从而提高基础模型的性能。 

奖励模型在RLHF中的作用是提供一种量化的奖励信号，这个信号反映了人类对智能体行为的评价。这种评价可以是基于特定任务的性能，也可以是更抽象的概念，如“好”或“坏”。奖励模型使得智能体能够在没有明确数值奖励的环境中学习，因为它提供了一种从人类反馈中学习的方法。

奖励模型不需要与基础模型在结构上完全一致，但它们必须能够协同工作，以确保智能体能够根据人类的偏好和评价来学习和适应。在实践中，奖励模型的设计和实现需要考虑到如何有效地整合人类反馈，以及如何与基础模型的学习和决策过程相匹配。通过这种方式，RLHF能够使智能体在复杂的环境中表现出更加符合人类期望的行为。

#### RLHF 在实践过程中存在哪些不足？
RLHF（Reinforcement Learning from Human Feedback）在实践中虽然展现出了巨大的潜力，但也存在一些挑战和不足之处。以下是一些主要的问题：

1.  **人类反馈的获取和成本**：高质量的人类反馈是RLHF成功的关键，但获取这种反馈可能既昂贵又耗时。需要大量的人力资源来进行标注、评估或与智能体交互提供反馈。 
2.  **反馈的一致性和偏差**：不同个体的反馈可能存在差异，这可能导致奖励模型的偏差。人类的主观性和不一致性可能会影响反馈的质量，从而影响智能体的学习过程。 
3.  **奖励建模的复杂性**：根据人类反馈构建准确的奖励模型是一项挑战。奖励模型需要能够捕捉到人类偏好的细微差别，并且在智能体探索新行为时能够提供稳定的奖励信号。 
4.  **探索与利用的平衡**：在RLHF中，智能体需要在探索新行为以收集更多反馈和利用已知策略以最大化奖励之间找到平衡。这个平衡很难实现，尤其是在奖励信号稀疏或延迟的情况下。 
5.  **泛化能力**：智能体可能在特定任务上表现出色，但将其泛化到新任务或环境可能会遇到困难。这是因为奖励模型可能过度适应于特定反馈，而忽略了更广泛的应用场景。 
6.  **可解释性和透明度**：RLHF中的智能体决策过程可能不够透明，难以理解和解释。这在需要可解释AI（Explainable AI）的应用中尤其成问题。 
7.  **安全性和道德考量**：在安全关键的应用中，如自动驾驶，RLHF智能体的错误行为可能导致严重后果。此外，人类反馈可能包含偏见，这些偏见可能会被智能体学习并放大。 
8.  **计算资源需求**：RLHF通常需要大量的计算资源来处理和分析人类反馈数据，以及训练复杂的模型。这可能限制了RLHF在资源受限的环境中的应用。 

#### RLHF中，人类偏好数据少的问题如何缓解？
在RLHF（Reinforcement Learning from Human Feedback）中，人工产生的偏好数据集成本较高和难以量产的问题是实践中的一个主要挑战。以下是一些可能的解决方案：

1.  **数据增强（Data Augmentation）**： 
    - 利用现有的少量高质量人类反馈数据来生成更多的训练样本。这可以通过技术如插值、重采样或生成模型来实现。
    - 创造模拟环境，在其中可以生成大量的合成反馈数据，这些数据可以用来训练和改进奖励模型。
2.  **半监督学习（Semi-Supervised Learning）**： 
    - 结合少量的标注数据和大量的未标注数据进行训练。通过使用未标注数据的分布信息，可以提高模型的泛化能力。
3.  **迁移学习（Transfer Learning）**： 
    - 利用在其他任务上预训练的模型作为起点，只在目标任务上进行少量的微调。这样可以减少对大量标注数据的需求。
4.  **多任务学习（Multi-Task Learning）**： 
    - 训练一个模型来同时处理多个相关任务。这种方法可以提高数据效率，并减少对大量标注数据的需求。
5.  **众包（Crowdsourcing）**： 
    - 通过在线平台收集人类反馈，如Amazon Mechanical Turk。虽然众包可能引入噪声，但可以通过聚合多个工人的反馈来提高数据质量。
6.  **合成人类反馈（Synthetic Human Feedback）**： 
    - 使用模拟人类行为的算法来生成反馈。例如，可以使用预训练的生成模型来模拟人类的决策过程。
7.  **元学习（Meta-Learning）**： 
    - 训练模型快速适应新任务，只需少量的样本。元学习可以使模型在新任务上快速学习，减少对大量标注数据的需求。
8.  **强化学习本身的改进**： 
    - 开发新的强化学习算法，这些算法能够在较少的人类反馈下仍然能够有效地学习。例如，通过改进探索策略或奖励信号的估计方法。
9.  **交互式学习（Interactive Learning）**： 
    - 在线地与用户交互，通过实时反馈来调整模型。这种方法可以减少预先收集大量数据的需求。
10.  **偏好学习（Preference Learning）**： 
    - 直接从用户的偏好中学习，而不是从显式的标注数据中学习。这可以通过用户在与系统交互时的点击、评分或其他形式的反馈来实现。

#### 缓解RLHF方法（SFT->RM->PPO）过程较长，更新迭代较慢问题？
在使用三个阶段的训练流程（SFT->RM->PPO）时，确实可能会遇到训练过程较长和更新迭代较慢的问题。以下是一些可能的解决方案：

1.  **模型并行化和分布式训练**： 
    - 利用多GPU或多节点并行化训练过程，可以显著加快模型的学习和更新速度。
    - 使用高效的分布式训练框架，如TensorFlow或PyTorch的分布式版本，可以在多个计算设备上同时进行训练。
2.  **更高效的优化算法**： 
    - 使用或开发更高效的优化算法，如AdamW、RMSProp等，这些算法能够在较少的迭代次数下达到更好的性能。
    - 调整学习率和其他超参数，以加快收敛速度。
3.  **模型结构和复杂度的优化**： 
    - 简化模型结构，减少不必要的复杂度，可以加快训练速度。
    - 使用知识蒸馏（Knowledge Distillation）等技术，将大型复杂模型的知识迁移到更小、更高效的模型中。
4.  **增量学习（Incremental Learning）**： 
    - 通过增量学习，只在模型检测到显著变化或新趋势时才进行更新，而不是在整个数据集上重新训练。
5.  **数据预处理和增强**： 
    - 通过有效的数据预处理和增强技术，可以提高数据的质量和多样性，从而加快学习过程。
6.  **中间表示的选择**： 
    - 在SFT（Supervised Fine-Tuning）阶段，选择合适的中间表示可以加快学习速度。例如，使用自监督学习预训练的表示作为起点。
7.  **更有效的探索策略**： 
    - 在RM（Reinforcement Learning from Human Feedback）阶段，设计更有效的探索策略，以更快地收集到有用的人类反馈。
8.  **使用预训练模型**： 
    - 在PPO（Proximal Policy Optimization）阶段，使用在相关任务上预训练的模型作为起点，可以加速收敛。
9.  **模型更新策略的改进**： 
    - 采用如经验回放（Experience Replay）和目标网络（Target Networks）等技术，可以提高PPO算法的稳定性和效率。
10.  **异步计算**： 
    - 在训练过程中，异步执行数据加载、模型训练和评估等任务，可以减少等待时间，提高整体训练效率。
11.  **训练流程的自动化和监控**： 
    - 自动化训练流程，并实时监控模型性能和训练指标，可以及时发现问题并进行调整。

#### RAG查询中，query的优化方法有哪些？
将单个查询扩展为多个查询丰富了查询的内容，提供了进一步的上下文来解决任何缺乏特定细微差别的问题，从而确保生成答案的最佳相关性。包括：

+ multi-query：通过使用提示工程通过LLMs扩展查询，这些查询可以并行执行。查询的扩展不是随机的，而是精心设计的。这种设计的两个关键标准是查询的多样性和覆盖率。使用多个查询的挑战之一是用户原始意图的潜在稀释，为了缓解这种情况，我们可以指示模型在提示工程中为原始查询分配更大的权重。
+ sub-query：子问题规划的过程代表了生成必要的子问题，以使上下文化，并在组合时充分回答原始问题。这种添加相关上下文的过程原则上类似于查询扩展。具体来说，将一个复杂的问题分解成一系列更简单的子问题。
+ 重写：原始查询并不总是最适合大模型检索，尤其是在现实世界的场景中。因此，我们可以提示大模型重写查询。除了使用大模型进行查询重写之外，还可以利用专门的较小语言模型
+ HyDE：在响应查询时，大模型构造假设文档（假设答案），而不是直接在向量数据库中搜索查询及其计算向量，它侧重于从答案到答案的嵌入相似性，而不是为问题或查询寻求嵌入相似性，此外，它还包括Reverse HyDE，它侧重于从查询到查询的检索。
    - ![](https://cdn.nlark.com/yuque/0/2024/png/406504/1706577759187-fc86584b-f672-429c-9b21-9e123ec5513a.png)
+ step-back：利用Google DeepMind提出的[Step-back Prompting](https://arxiv.org/abs/2310.06117)方法，对原始查询进行抽象，生成高级概念问题（step-back问题），在RAG系统中，同时使用step-back问题和原始查询进行检索，并将两者的结果作为语言模型答案生成的基础。

#### 2021年 P-Tuning技术进行介绍下
该方法的提出主要是为了解决这样一个问题：大模型的Prompt构造方式严重影响下游任务的效果。比如：GPT-3采用人工构造的模版来做上下文学习（in context learning），但人工设计的模版的变化特别敏感，加一个词或者少一个词，或者变动位置都会造成比较大的变化。

同时，近来的自动化搜索模版工作成本也比较高，以前这种离散化的token的搜索出来的结果可能并不是最优的，导致性能不稳定。

基于此，作者提出了P-Tuning，设计了一种连续可微的virtual token（同Prefix-Tuning类似）。

![](https://cdn.nlark.com/yuque/0/2023/png/35381469/1694137764879-124aa1b7-3478-466e-ae39-e46f4fe6f676.png)

P-Tuning（论文：**GPT Understands, Too**），该方法**将Prompt转换为可以学习的Embedding层，并用MLP+LSTM的方式来对Prompt Embedding进行一层处理**。

相比Prefix Tuning，**P-Tuning加入的可微的virtual token，但仅限于输入层，没有在每一层都加**；另外，virtual token的位置也不一定是前缀，插入的位置是可选的。这里的出发点实际是把传统人工设计模版中的真实token替换成可微的virtual token。

经过预训练的LM的词嵌入已经变得高度离散，如果随机初始化virtual token，容易优化到局部最优值，而这些virtual token理论是应该有相关关联的。因此，作者通过实验发现用一个prompt encoder来编码会收敛更快，效果更好。即用一个LSTM+MLP去编码这些virtual token以后，再输入到模型。

方法：

+ 为了解决discrete prompt的不稳定性，提出一种P-Tuning的方法来进行soft prompt（和Prompt-Tuning类似），与prefix-tuning的区别在于，prefix-tuning注重生成任务，而P-Tuning注重理解任务NLU
+ 具体来说，就是通过一个prompt encoder来对prompt进行编码和对齐（encoder采用LSTM或者MLP）

细节：

+ 想法和prompt-tuning类似（同期工作），prompt-tuning论文中提到了P-Tuning
+ 和prefix-tuning的不同还在于其prompt插入的位置不局限于prefix，并且仅仅在输入层添加，没有在所有的子层添加
+ 本质上利用prompt encoder是为了对virtual token（soft prompt）做词嵌入空间上的alignment，使得其更加适配下游任务

![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1705973649269-d3b7fe5e-4680-4e99-9a67-93df64d6550a.png)

#### 2021-P-Tuning v2方法进行介绍下
背景：

+ P-Tuning和prompt-tuning存在两个问题：对于正常规模的模型效果不好（10B以下的）；对于序列标记任务表现不佳
+ 而仅在输入层进行prompt tuning会有两个挑战：由于序列长度的限制，参数量较少；由于仅在输入的embedding上进行调整，对模型最终的预测没有直接影响

> 之前的Prompt Tuning和P-Tuning等方法存在两个主要的问题：
>
> **第一，缺乏模型参数规模和任务通用性。**
>
> + 缺乏规模通用性：Prompt Tuning论文中表明当模型规模超过100亿个参数时，提示优化可以与全量微调相媲美。但是对于那些较小的模型（从100M到1B），提示优化和全量微调的表现有很大差异，这大大限制了提示优化的适用性。
> + 缺乏任务普遍性：尽管Prompt Tuning和P-tuning在一些 NLU 基准测试中表现出优势，但提示调优对硬序列标记任务（即序列标注）的有效性尚未得到验证。
>
> **第二，缺少深度提示优化**，在Prompt Tuning和P-tuning中，连续提示只被插入transformer第一层的输入embedding序列中，在接下来的transformer层中，插入连续提示的位置的embedding是由之前的transformer层计算出来的，这可能导致两个可能的优化挑战。
>
> + 由于序列长度的限制，可调参数的数量是有限的。
> + 输入embedding对模型预测只有相对间接的影响。
>
> 考虑到这些问题，作者提出了Ptuning v2，它利用深度提示优化（如：Prefix Tuning），对Prompt Tuning和P-Tuning进行改进，作为一个跨规模和NLU任务的通用解决方案。
>

原理：

P-Tuning v2（论文： **P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks**），**该方法在每一层都加入了Prompts tokens作为输入**，而不是仅仅加在输入层，这**带来两个方面的好处**：

+ 更多可学习的参数（从P-tuning和Prompt Tuning的0.01%增加到0.1%-3%），同时也足够参数高效。
+ 加入到更深层结构中的Prompt能给模型预测带来更直接的影响。![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1705973985180-0e3c2605-cfcb-4ee3-a870-e36d41ba9439.png)

细节：

+ 针对前人工作做了以下改进： 
    - Reparameterization（重参数化）的操作（利用MLP, LSTM来提高prompt token训练的稳定性）仅在一些数据集和任务上有效，在某些任务甚至会掉点，所以进行了移除
    - prompt的长度很关键，通常不同的NLU任务需要不同的长度来得到最佳性能，通常来说任务越难，prompt的长度越长（简单分类任务的提示长度小于20，序列标注任务的最佳提示长度在100左右）
    - 多任务学习的引入：在针对单任务做fine-tuning时，可以先用multi-task learning来做多任务的prompt训练，效果会更好
    - 重新使用传统的CLS分类头来识别序列标注

> 具体做法基本同Prefix Tuning，可以看作是将文本生成的Prefix Tuning技术适配到NLU任务中，然后做了一些改进：
>
> + **移除重参数化的编码器**。以前的方法利用重参数化功能来提高训练速度和鲁棒性（如：Prefix Tuning中的MLP、P-Tuning中的LSTM））。在 P-tuning v2 中，作者发现重参数化的改进很小，尤其是对于较小的模型，同时还会影响模型的表现。
> + **针对不同任务采用不同的提示长度**。提示长度在提示优化方法的超参数搜索中起着核心作用。在实验中，我们发现不同的理解任务通常用不同的提示长度来实现其最佳性能，这与Prefix-Tuning中的发现一致，不同的文本生成任务可能有不同的最佳提示长度。
> + **引入多任务学习**。先在多任务的Prompt上进行预训练，然后再适配下游任务。多任务学习对我们的方法来说是可选的，但可能是相当有帮助的。一方面，连续提示的随机惯性给优化带来了困难，这可以通过更多的训练数据或与任务相关的无监督预训练来缓解；另一方面，连续提示是跨任务和数据集的特定任务知识的完美载体。我们的实验表明，在一些困难的序列任务中，多任务学习可以作为P-tuning v2的有益补充。
> + **回归传统的分类标签范式，而不是映射器**。标签词映射器（Label Word Verbalizer）一直是提示优化的核心组成部分，它将one-hot类标签变成有意义的词，以利用预训练语言模型头。尽管它在few-shot设置中具有潜在的必要性，但在全数据监督设置中，Verbalizer并不是必须的。它阻碍了提示调优在我们需要无实际意义的标签和句子嵌入的场景中的应用。因此，P-Tuning v2回归传统的CLS标签分类范式，采用随机初始化的分类头（Classification Head）应用于tokens之上，以增强通用性，可以适配到序列标注任务。
>

来自清华大学的团队发布的两种参数高效Prompt微调方法P-Tuning、P-Tuning v2，可以简单的将P-Tuning认为是针对Prompt Tuning的改进，P-Tuning v2认为是针对Prefix Tuning的改进。

![](https://cdn.nlark.com/yuque/0/2023/png/35381469/1694137770755-894c0be0-81c8-40d6-8cfc-964951bdd72a.png)

#### 请说下你对Lora的理解
背景：

神经网络包含很多全连接层，其借助于矩阵乘法得以实现，然而，很多全连接层的权重矩阵都是满秩的。当针对特定任务进行微调后，模型中权重矩阵其实具有很低的本征秩（intrinsic rank），因此，论文的作者认为权重更新的那部分参数矩阵尽管随机投影到较小的子空间，仍然可以有效的学习，可以理解为针对特定的下游任务这些权重矩阵就不要求满秩。

原理：

LoRA（论文：**LoRA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS**），该方法的核心思想就是通过低秩分解来模拟参数的改变量，从而以极小的参数量来实现大模型的间接训练。

方法：

+ 通过低秩分解的方法来模拟参数的改变，训练好的A,B矩阵权重直接加和就完成更新
    - 在涉及到矩阵相乘的模块，在原始的PLM旁边增加一个新的通路，通过前后两个矩阵A,B相乘，第一个矩阵A负责降维，第二个矩阵B负责升维，中间层维度为r，从而来模拟所谓的本征秩（intrinsic rank）。

![](https://cdn.nlark.com/yuque/0/2023/png/35381469/1694137642364-6d95f445-329d-4884-b973-e48eb52bdf40.png)

    - 可训练层维度和预训练模型层维度一致为d，先将维度d通过全连接层降维至r，再从r通过全连接层映射回d维度，其中，r<<d，r是矩阵的秩，这样矩阵计算就从d x d变为d x r + r x d，参数量减少很多。
        * ![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1705994967998-0bb6c890-bed5-493e-b1d8-ac968850f291.png)
        * ![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1705974860579-a34b77f8-606c-4d0b-82b1-4c38861e80b0.png)
+ 并且LoRA可以和其他的peft方法结合（比如prefix-tuning）



在下游任务训练时，固定模型的其他参数，只优化新增的两个矩阵的权重参数，将PLM跟新增的通路两部分的结果加起来作为最终的结果（两边通路的输入跟输出维度是一致的），即h=Wx+BAx。第一个矩阵的A的权重参数会通过高斯函数初始化，而第二个矩阵的B的权重参数则会初始化为零矩阵，这样能保证训练开始时新增的通路BA=0从而对模型结果没有影响。

![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1705994976385-25e7040b-2f2f-405b-aab9-e9e2b7e0c389.png)

在推理时，将左右两部分的结果加到一起即可，h=Wx+BAx=(W+BA)x，所以只要将训练完成的矩阵乘积BA跟原本的权重矩阵W加到一起作为新权重参数替换原本PLM的W即可，对于推理来说，不会增加额外的计算资源。

此外，Transformer的权重矩阵包括Attention模块里用于计算query, key, value的Wq，Wk，Wv以及多头attention的Wo,以及MLP层的权重矩阵，LoRA只应用于Attention模块中的4种权重矩阵，而且通过消融实验发现同时调整 Wq 和 Wv 会产生最佳结果。

细节：

+ A, B矩阵分别用于降维和升维，中间维度为，而可以非常小，叫做intrinsic rank，从而大大较少训练的参数量
+ 训练完成后直接右边+左边完成参数更新后，可以直接舍弃右边的结构，所以推理的成本不会提高
+ 而A由高斯函数初始化，B初始化为0，则BA=0，保证了其不会损害模型的能力
+ LoRA主要应用在K, Q, V, 还有MHA的多头参数上
+ 模型base：Roberta-base-large-XXL, GPT-2-M-L, GPT-3-175B
+ 方法baseline：包括了adapter-tuning， adapterDrop， prefix-tuning， prompt-tuning, bitfit（2021年放到arxiv, 2022中的ACL, 所以目录上bitfit时间比LoRA晚）

#### 请说下AdaLora方法
背景：

LORA是一种有效的低资源方式，仅仅微调万分之一参数就能达到全量参数微调的效果。但本文指出**LORA是将可微调参数平均分布在每个权重矩阵上，忽略了不同权重参数的重要程度，因此LORA微调的效果可能不是最优的**。  
AdaLORA改进了LORA可微调参数的分配方式，根据每个参数的重要程度自动得为其分配可微调参数的预算。具体的，AdaLORA采用奇异值分解(SVD)的形式参数化增量更新。**这种参数化方式在规避大量SVD运算的同时，允许我们高效裁剪不重要更新中的奇异值，降低增量过程中的资源消耗。**

LORA的局限性在于其预先规定了每个增量矩阵Δ的秩r必须相同。这样就忽略了不同层、不同类型参数对下游任务的重要程度。为了弥补这一差距，作者提出了AdaLoRA，它根据权重矩阵的重要性得分，在权重矩阵之间自适应地分配参数预算。

原理：

AdaLoRA（论文：**ADAPTIVE BUDGET ALLOCATION FOR PARAMETEREFFICIENT FINE-TUNING**），是对LoRA的一种改进，它根据重要性评分动态分配参数预算给权重矩阵。

![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1705996760834-2bb937bb-6516-4d04-b946-f3ec2bff3d12.png)

![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1705996839260-d5fb7ea2-f242-4641-be30-c2779c9c559b.png)

方法：

+ 基于奇异值分解（SVD）的AdaLora，将增量矩阵以奇异值分解的形式表达
+ 基于设计的importance-aware rank allocation方法，来裁剪冗余的奇异值，从而减少计算开销

细节：

+ **调整增量矩分配**。AdaLoRA将关键的增量矩阵分配高秩以捕捉更精细和任务特定的信息，而将较不重要的矩阵的秩降低，以防止过拟合并节省计算预算。
+ **以奇异值分解的形式对增量更新进行参数化，并根据重要性指标裁剪掉不重要的奇异值，同时保留奇异向量**。由于对一个大矩阵进行精确SVD分解的计算消耗非常大，这种方法通过减少它们的参数预算来加速计算，同时，保留未来恢复的可能性并稳定训练。

![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1705979818051-e853f85d-6db6-4ef1-ad58-15dbe6c5cc35.png)

+ **在训练损失中添加了额外的惩罚项**，以规范奇异矩阵P和Q的正交性，从而避免SVD的大量计算并稳定训练。

#### 请介绍下QLora训练过程
QLORA训练过程跟LORA基本上是一致的。区别在于QLORA模型是按照NF4保存的，训练时需要把参数反量化到bf16后进行训练。

![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1705995877473-9be3be34-adf8-4617-b665-6fa8f6f4e316.png)

![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1705979985140-66b0d7c9-d41f-4397-aad3-5a8d786e5f8f.png)

#### 模型量化具体是怎么实现的？
对称量化中，零点 Z = 0，一般不记录，我们只需要关心如何求解 Scale。由于 weight 几乎不存在异常值，因此我们可以直接取 Scale 为一个 layer 或 block 内所有参数的**最大绝对值**，于是所有的参数都在 [-1, 1] 的区间内。随后，这些参数将找到最近的量化格点，并转化成定点数。

![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1705996044075-25e52cd5-2ed4-4a40-b099-3e58f18bb0d1.png)

图中展示的是量化到 int8，因此 Round to Grid 实际上是先乘以 127(=2**(8-1)-1)，然后 round 到最近的整数

这里需要引入 **<font style="background-color:rgb(248, 248, 250);">Block-wise quantization</font>** 的概念。通常情况，为了避免异常值（outlier）的影响，我们会将输入 tensor 分割成一个个 block，每个 block 单独做量化，有单独的 scale 和 zero，因此能使量化的精度损失减少（见下图橙色的误差部分）。

![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1705996044071-ed4b8ba2-b174-4586-8743-b5fadfc12e18.png)

#### 请介绍下Prompt Tuning
大模型全量微调对每个任务训练一个模型，开销和部署成本都比较高。同时，离散的prompts（指人工设计prompts提示语加入到模型）方法，成本比较高，并且效果不太好。

基于此，作者提出了Prompt Tuning，通过反向传播更新参数来学习prompts，而不是人工设计prompts；同时冻结模型原始权重，只训练prompts参数，训练完以后，用同一个模型可以做多任务推理。

原理：

Prompt Tuning（论文：**The Power of Scale for Parameter-Efficient Prompt Tuning**），该方法可以看作是**Prefix Tuning的简化版本**，它给每个任务定义了自己的Prompt，然后拼接到数据上作为输入，但**只在输入层加入prompt tokens**，并且不需要加入 MLP 进行调整来解决难训练的问题。

细节：

+ 可以看作prefixes-tuning的简化版本，移除了稳定训练所需要的MLP，仅仅在输入层添加prompt tokens；
+ 另外提出了一种prompt-ensemble的方法，通过对同一任务施加不同的prompt进行训练来达到集成的效果，大大减小了模型集成的开销；

![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1705973334301-f0e5a34f-689f-45c6-bbdc-b0db6b6139ad.png)

通过实验发现，随着预训练模型参数量的增加，Prompt Tuning的方法会逼近全参数微调的结果。![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1705973484138-03c15e80-45d0-47a6-aab0-6bc3cc08809f.png)

image.png

同时，Prompt Tuning 还提出了 Prompt Ensembling，也就是在一个批次（Batch）里同时训练同一个任务的不同 prompt（即采用多种不同方式询问同一个问题），这样相当于训练了不同模型，比模型集成的成本小多了。

![](https://cdn.nlark.com/yuque/0/2023/png/35381469/1694137820159-f238615b-0401-4e45-8ba4-d11ec71e7578.png)

#### 请介绍下Prefix Tuning
+ 受prompt启发,模型对prompt比较敏感,在GPT-2, BART中想让prompt发挥作用需要精巧的设计,费时费力
+ 一般的fine-tuning方法需要多次拷贝参数和重新训练整个transformer,计算开销大
+ 所以想要一种soft-prompt方法可以真正通过参数来”引导“模型往下游任务上靠

> 在Prefix Tuning之前的工作主要是人工设计离散的模版或者自动化搜索离散的模版。对于人工设计的模版，模版的变化对模型最终的性能特别敏感，加一个词、少一个词或者变动位置都会造成比较大的变化。而对于自动化搜索模版，成本也比较高；同时，以前这种离散化的token搜索出来的结果可能并不是最优的。
>
> 除此之外，传统的微调范式利用预训练模型去对不同的下游任务进行微调，对每个任务都要保存一份微调后的模型权重，一方面微调整个模型耗时长；另一方面也会占很多存储空间。
>
> 基于上述两点，Prefix Tuning提出固定预训练LM，为LM添加可训练，任务特定的前缀，这样就可以为不同任务保存不同的前缀，微调成本也小；同时，这种Prefix实际就是连续可微的Virtual Token（Soft Prompt/Continuous Prompt），相比离散的Token，更好优化，效果更好。
>

![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1705972754540-f1485c55-8eee-4468-ac98-295103b23bb3.png)

原理：

Prefix Tuning（论文：**Prefix-Tuning: Optimizing Continuous Prompts for Generation**），在输入token之前**构造一段任务相关的virtual tokens作为Prefix**，然后**训练的时候只更新Prefix部分的参数**，而PLM中的其他部分参数固定。

方法其实和构造Prompt类似，只是Prompt是人为构造的“显式”的提示，并且无法更新参数，而Prefix则是可以学习的“隐式”的提示。  


![](https://cdn.nlark.com/yuque/0/2023/png/35381469/1694137817830-b7bc3970-6507-43e5-b084-e63117cb330e.png)

方法：

+ 提出了prefix-tuning, 在token前面加上prefix token,冻结住模型的参数,仅仅更新这部分prefix token的参数, 起到一个"提示任务"的作用, 从而在改善模型在下游任务的表现

细节：

针对不同的模型结构，需要构造不同的Prefix。

+ **针对encoder-decoder架构，会在encoder的输入和decoder的输入embedding都加上prefix token；**
    - **针对自回归架构模型：**在句子前面添加前缀，得到 <font style="background-color:rgb(246, 246, 246);">z = [PREFIX; x; y]</font>，合适的上文能够在固定 LM 的情况下去引导生成下文（比如：GPT3的上下文学习）。
    - **针对编码器-解码器架构模型：**Encoder和Decoder都增加了前缀，得到 <font style="background-color:rgb(246, 246, 246);">z = [PREFIX; x; PREFIX0; y]</font>。Encoder端增加前缀是为了引导输入部分的编码，Decoder 端增加前缀是为了引导后续token的生成。![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1705972813775-c6823514-2227-45a1-b5b7-2f040a83560c.png)

image.png

+ 消融实验中对比了embedding-only和infix的效果，可以发现embedding-only和infix（virtual token加在embedding的中间）的效果并不好；
    - 其中，Prefix-tuning形式为 <font style="background-color:rgb(246, 246, 246);">[PREFIX; x; y]</font>，Infix-tuning形式为 <font style="background-color:rgb(246, 246, 246);">[x; INFIX; y]</font>。![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1705973007684-ca73d879-6554-4b1d-878c-f18da25568e4.png)
+ **由于训练的不稳定性，会在prefix token层前加上一层MLP来稳定训练，训练完成后会移除MLP；**
    - 同时，为了防止直接更新Prefix的参数导致训练不稳定和性能下降的情况，在Prefix层前面加了MLP结构，训练完成后，只保留Prefix的参数。

![](https://cdn.nlark.com/yuque/0/2023/png/35381469/1694137817995-d9a1b055-b5a5-42a4-a83c-1e2814a87e63.png)

+ **由消融实验embedding-only的结果进行改进，在所有layer的输入层都加上prefix token效果会更好；**
    - 除此之外，通过消融实验证实，只调整embedding层的表现力不够，将导致性能显著下降，因此，在每层都加了prompt的参数，改动较大。

![](https://cdn.nlark.com/yuque/0/2023/png/35381469/1694137818368-dbd7bf84-50aa-40c5-bc91-c67fa5a78103.png)

#### 请介绍下AdapterDrop方法
背景：

近年来Adapter已被证明可以很好地用于机器翻译、跨语言迁移、社区问答和迁移学习的任务组合。作者通过对Adapter的计算效率进行分析，发现与全量微调相比，Adapter在训练时快60%，但是在推理时慢4%-6%。

基于此，作者提出了AdapterDrop方法缓解该问题。

原理：

AdapterDrop（论文：AdapterDrop: On the Efficiency of Adapters in Transformers），在不影响任务性能的情况下，对Adapter动态高效的移除，尽可能的减少模型的参数量，提高模型在反向传播（训练）和正向传播（推理）时的效率。

细节：

+ 对于adapter而言,采用两种方法进行删减:第一种是从输入的transformer layer开始删除n个模块的adapter;另外一种是随机的删除n个模块
+ 对于adapter-fusion而言,希望在inference推理时有更好的速度,两种删除方法:其一是删除整个AF结构;其二是删除重要性最小的adapter,使其不参与AF中的计算

  


![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1705974705801-100599e2-156b-4d67-96b2-95279370d9a7.png)

结果：

+ 引入adapter后,与全量微调相比,训练快了60%,推理慢了4-6% (引入了额外的参数导致推理变慢是正常的现象)
+ 删除底层transformer中的几个adapter可以加速推理并不损害task performance
+ 对adapter-fusion的的删除以及adapter输入的剪枝也可以有效的提高推理速度,并获得较好的推理和训练性能

#### 请介绍下Adapter Fusion方法
背景：

+ sequential fine-tuning和multi-task learning存在遗忘和数据平衡的挑战。想在一个模型上实现多任务的信息共享有两种方法：sequential fine-tuning的数据训练顺序很重要，容易出现遗忘的风险；multi-task learning任务数据的大小和质量存在挑战（ these models often overfit on low resource tasks and underfit on high resource tasks.）
+ 而adapter-tuning方法仅仅提供了adapter的插入来解决全参数微调难以训练和知识遗忘的问题，而并没有将每个任务下游adapter收集的信息利用最大化，从而解决信息共享，下游通用，数据顺序的问题。

原理：

Adapter Fusion（论文：**AdapterFusion:Non-Destructive Task Composition for Transfer Learning**），**一种融合多任务信息的Adapter的变体，在 Adapter 的基础上进行优化，通过将学习过程分为两阶段来提升下游任务表现。**

方法：

+ 提出一个两阶段的迁移训练方法：首先在**知识抽取阶段利用各下游任务训练adapter，然后在知识整合阶段将训练好的adapter通过adapterFusion进行信息共享**。
+ 提出一个adapter-fusion模块用于融合各adapter学习到的信息。
+ ![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1705974605181-8f07863a-e69e-438b-b2a8-f2f7a18eb3ac.png)

细节：

+ 训练数据集会被使用两次：训练下游任务的adapter时和训练信息融合的adapter-fusion模块
+ 以Bert为backbone，知识抽取阶段更新adapter的参数，知识融合阶段更新adapter-fusion的参数
+ adapter-fusion模块直接添加到adapter后面，其本质是一个attention计算过程。Q是进入adapter前的输入，而K,V则是adapter的输出
+ 第一阶段的训练方法分为两种：ST-A（一个任务使用一个adapter），MT-A（多个任务使用一个adapter联合优化）
    - Single-Task Adapters(ST-A)：对于N个任务，模型都分别独立进行优化，各个任务之间互不干扰，互不影响。
    - Multi-Task Adapters(MT-A)：N个任务通过多任务学习的方式，进行联合优化。
        * 对于第二阶段，为了避免通过引入特定任务参数而带来的灾难性遗忘问题，AdapterFusion提出了一个共享多任务信息的结构。针对特定任务m，AdapterFusion联合了第一阶段训练得到的N个Adapter信息。固定语言模型的参数跟N个Adapter的参数，新引入AdapterFusion的参数，目标函数也是学习针对特定任务m的AdapterFusion的参数。

**AdapterFusion结构**：

AdapterFusion具体结构就是一个Attention，它的参数包括query，key, value的矩阵参数，在transformer的每一层都存在，它的query是transformer每个子模块的输出结果，它的key跟value则是N个任务的adapter的输出。通过AdapterFusion，模型可以为不同的任务对应的adapter分配不同的权重，聚合N个任务的信息，从而为特定任务输出更合适的结果。

![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1705974635247-c1a986fc-19c1-40bf-be0d-9d2b008274d5.png)

#### 请介绍下Adapter Tuning
背景：

+ 当预训练模型的规模较大时，为每一个下游任务进行全参数微调会耗费很多计算资源和时间
+ 研究表明fine-tuning比feature-base-transfer的效果要好（fine-tuning是保证模型结构的完整，针对下游任务，利用特定数据进行参数微调的过程；而feature-base-transfer则是保留预训练的embedding比如word、sentence，paragraph，然后将其迁移到其他任务中）
+ 同时全参数微调的fine-tuning容易出现知识遗忘的问题（原文中写的是multi-task和continual learning会出现这种情况）

原理：

Adapter Tuning（论文：**Parameter-Efficient Transfer Learning for NLP**），该方法设计了Adapter结构，并将其嵌入Transformer的结构里面，针对每一个Transformer层，增加了两个Adapter结构(分别是多头注意力的投影之后和第二个feed-forward层之后)，在训练时，**固定住原来预训练模型的参数不变，只对新增的 Adapter 结构和 Layer Norm 层进行微调****，**从而保证了训练的高效性。

每当出现新的下游任务，通过添加Adapter模块来产生一个易于扩展的下游模型，从而避免全量微调与灾难性遗忘的问题。![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1705974409943-09d412ce-79d6-4693-855e-88bee83b7fac.png)

image.png

**Adapter结构具体细节**：

每个 Adapter 模块主要由两个前馈（Feedforward）子层组成，第一个前馈子层（down-project）将Transformer块的输出作为输入，将原始输入维度d（高维特征）投影到m（低维特征），通过控制m的大小来限制Adapter模块的参数量，通常情况下，m<<d。然后，中间通过一个非线形层。在输出阶段，通过第二个前馈子层（up-project）还原输入维度，将m（低维特征）重新映射回d（原来的高维特征），作为Adapter模块的输出。同时，通过一个skip connection来将Adapter的输入重新加到最终的输出中去，这样可以保证，即便 Adapter 一开始的参数初始化接近0，Adapter也由于skip connection的设置而接近于一个恒等映射，从而确保训练的有效性。

![](https://cdn.nlark.com/yuque/0/2023/png/35381469/1694137687531-0f99533f-3adf-4716-b623-5b2c6928013e.png)

image.png

通过实验发现，只训练少量参数的Adapter方法的效果可以媲美全量微调，这也验证了Adapter是一种高效的参数训练方法，可以快速将语言模型的能力迁移到下游任务中去。同时，可以看到，Adapter 最佳的中间层特征维度m视数据集的大小而异，如：MINI数据集为256，最小的RTE数据集为8。如果始终将维度限制在64，将导致平均准确率略微下降。![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1705974544926-a613464a-b218-4331-b3cd-42d60c247c7e.png)

image.png

Adapter通过引入0.5%～5%的模型参数可以达到不落后全量微调模型1%的性能。

#### 全参数微调（FFT） VS 参数高效微调（PEFT）
**全参数微调的问题：**

+ 消费级显卡的全参数微调在时间上和硬件上不可行
+ 全参数微调会损失多样性，出现灾难性遗忘，且下游部署、维护繁琐

**高效微调：**

+ 冻结大部分预训练参数，仅仅更新一部分参数使得模型获得更好的下游任务性能
+ 主要的方法包括：引入额外参数、选取部分参数

#### 请介绍下PEFT
参数高效微调是指微调少量或额外的模型参数，固定大部分预训练模型（LLM）参数，从而大大降低了计算和存储成本，同时，也能实现与全量参数微调相当的性能。参数高效微调方法甚至在某些情况下比全量微调效果更好，可以更好地泛化到域外场景。

高效微调技术可以粗略分为以下三大类：**增加额外参数（A）、选取一部分参数更新（S）、引入重参数化（R**）。而在增加额外参数这类方法中，又主要分为类适配器（Adapter-like）方法和软提示（Soft prompts）两个小类。

![](https://cdn.nlark.com/yuque/0/2023/png/35381469/1694137865694-b1ba808b-c724-4895-a874-5bbe9c86fb60.png)

常见的参数高效微调技术有Prefix Tuning、Prompt Tuning、P-Tuning、Adapter Tuning、LoRA等。

#### RAG常见的评估指标有哪些？
一般从**检索质量**和**生成质量**两方面进行评估。

**检索质量：**

首先需要评估数据，可以采用现有信息检索数据集或用LLM构造自己领域的专属数据集，对**每个查询-文档对**都需要标注**是否相关**，或者**相关程度**。

一般采用搜索引擎，**推荐系统和信息检索**系统中的**指标**来衡量RAG检索效果：

+ 给定检索器和一组问题，使用排名指标评估检索结果。
    - 评估指标：
        * Hit Ratio (HR, 命中率)：用户的需求项是否包含在模型的推荐项中
        * Mean Reciprocal Rank (MRR, 平均倒数排名)：用户的需求项在模型推荐列表中的位置，越靠前越佳
        * cohere_rerank_relevancy：排序时的相关性分数

**生成质量：**

生成质量评估可以根据生成内容的目标来分类，即无标签的和有标签。对于无标签内容，主要从生成答案的**诚实性，相关性和无害性**来评估。对于有标签内容，可以从生成内容的**准确性**来评估。检索和生成质量都可以用**人工**或**自动评估**方法。

+ Correctness 正确性：生成的答案是否与给定查询的参考答案匹配（需要标签）。
    - 指标含义：0-5的之间的数字，数字越大越正确
    - 评估方式：GPT-4等大模型评估

```plain
You are an expert evaluation system for a question answering chatbot.

You are given the following information:
- a user query, and
- a generated answer

You may also be given a reference answer to use for reference in your evaluation.

Your job is to judge the relevance and correctness of the generated answer.
Output a single score that represents a holistic evaluation.
You must return your response in a line with only the score.
Do not return answers in any other format.
On a separate line provide your reasoning for the score as well.

Follow these guidelines for scoring:
- Your score has to be between 1 and 5, where 1 is the worst and 5 is the best.
- If the generated answer is not relevant to the user query, \
you should give a score of 1.
- If the generated answer is relevant but contains mistakes, \
you should give a score between 2 and 3.
- If the generated answer is relevant and fully correct, \
you should give a score between 4 and 5.

Example Response:
4.0
The generated answer has the exact same metrics as the reference answer, \
    but it is not as concise.
```

    - LlamaIndex评估方式：[https://docs.llamaindex.ai/en/stable/examples/evaluation/correctness_eval.html#](https://docs.llamaindex.ai/en/stable/examples/evaluation/correctness_eval.html#)
+ Semantic Similarity 语义相似度：预测答案在语义上是否与参考答案相似（需要标签）。
    - 指标含义:
        * score：相似度分数
        * passing：相似度是否高于阈值

```plain
"""Embedding similarity evaluator.

Evaluate the quality of a question answering system by
comparing the similarity between embeddings of the generated answer
and the reference answer.

Inspired by this paper:
- Semantic Answer Similarity for Evaluating Question Answering Models
    https://arxiv.org/pdf/2108.06130.pdf

Args:
    service_context (Optional[ServiceContext]): Service context.
    similarity_threshold (float): Embedding similarity threshold for "passing".
        Defaults to 0.8.
"""
```

    - LlamaIndex评估方式：[https://docs.llamaindex.ai/en/stable/examples/evaluation/semantic_similarity_eval.html](https://docs.llamaindex.ai/en/stable/examples/evaluation/semantic_similarity_eval.html)
+ Faithfulness 忠实性：评估答案是否忠实于检索到的上下文（换句话说，是否存在幻觉）。
    - 指标含义：
        * Yes：表示答案存在上下文支持；No：表示存在幻觉，与上下文无关
        * 评估方式：GPT-4等大模型评估

```plain
"Please tell if a given piece of information "
"is supported by the context.\n"
"You need to answer with either YES or NO.\n"
"Answer YES if any of the context supports the information, even "
"if most of the context is unrelated. "
"Some examples are provided below. \n\n"
"Information: Apple pie is generally double-crusted.\n"
"Context: An apple pie is a fruit pie in which the principal filling "
"ingredient is apples. \n"
"Apple pie is often served with whipped cream, ice cream "
"('apple pie à la mode'), custard or cheddar cheese.\n"
"It is generally double-crusted, with pastry both above "
"and below the filling; the upper crust may be solid or "
"latticed (woven of crosswise strips).\n"
"Answer: YES\n"
"Information: Apple pies tastes bad.\n"
"Context: An apple pie is a fruit pie in which the principal filling "
"ingredient is apples. \n"
"Apple pie is often served with whipped cream, ice cream "
"('apple pie à la mode'), custard or cheddar cheese.\n"
"It is generally double-crusted, with pastry both above "
"and below the filling; the upper crust may be solid or "
"latticed (woven of crosswise strips).\n"
"Answer: NO\n"
"Information: {query_str}\n"
"Context: {context_str}\n"
"Answer: "
```

    - LlamaIndex评估方式：[https://docs.llamaindex.ai/en/stable/examples/evaluation/faithfulness_eval.html](https://docs.llamaindex.ai/en/stable/examples/evaluation/faithfulness_eval.html)
+ Answer Relevancy 答案相关性：生成的答案是否与查询相关。
    - 指标含义：0-2分，数字越大越相关
    - 评估方式：GPT-4等大模型评估

```plain
"Your task is to evaluate if the response is relevant to the query.\n"
"The evaluation should be performed in a step-by-step manner by answering the following questions:\n"
"1. Does the provided response match the subject matter of the user's query?\n"
"2. Does the provided response attempt to address the focus or perspective "
"on the subject matter taken on by the user's query?\n"
"Each question above is worth 1 point. Provide detailed feedback on response according to the criteria questions above  "
"After your feedback provide a final result by strictly following this format: '[RESULT] followed by the integer number representing the total score assigned to the response'\n\n"
"Query: \n {query}\n"
"Response: \n {response}\n"
"Feedback:"
```

    - LlamaIndex评估方式：[https://docs.llamaindex.ai/en/stable/examples/evaluation/answer_and_context_relevancy.html](https://docs.llamaindex.ai/en/stable/examples/evaluation/answer_and_context_relevancy.html)
+ Guideline Adherence 准则遵守情况：预测答案是否遵守特定准则。
    - 指标含义：
        * Guideline：准则；Pass：回答是否符合要求；Feedback：由llm提出的反馈意见
        * 评估方式：GPT-4等大模型评估

```plain
"The response should fully answer the query.\n"
"The response should avoid being vague or ambiguous.\n"
"The response should be specific and use statistics or numbers when possible.\n"
```

```plain
"Here is the original query:\n"
"Query: {query}\n"
"Critique the following response based on the guidelines below:\n"
"Response: {response}\n"
"Guidelines: {guidelines}\n"
"Now please provide constructive criticism.\n"
```

    - LlamaIndex评估方式：[https://docs.llamaindex.ai/en/stable/examples/evaluation/guideline_eval.html](https://docs.llamaindex.ai/en/stable/examples/evaluation/guideline_eval.html)

#### RAG的评估基准有哪些？
主流的基准有RGB和RECALL，侧重评估RAG的基础能力。

+ **RGB**
    - 开源地址：[https://github.com/chen700564/RGB](https://github.com/chen700564/RGB)
    - 问答对形式，1000个问题，600个基础问题，200个专门面向信息集成的问题，200个专门面向反事实鲁棒性的问题。一半中文一半英文。
    - 从互联网和新闻中搜寻文档，利用编码模型m3e（中文）和mpnet（英文）进行重排。
    - 评估模型：ChatGPT，ChatGLM，Vicuna-7B，Qwen-7B，BELLE-7B-2M
    - 结论：对于噪声鲁棒性，随着**噪声文档的增加**（即不相关文档数量增加），RAG的**性能逐渐下降**；负排斥，信息集成，反事实鲁棒性对于RAG都很**困难**，提升空间都很大。
    - 关注**RAG的整体**
+ **RECALL**
    - 问答对形式，每个问题的提示都含有一段包含错误内容的文本，有的文本修改答案文本，有的文本修改非答案文本。模型被提供正确和错误答案两个选项。
    - 评估模型：ChatGLM2，Llama2，Vicuna，Baichuan2
    - 结论：模型很容易被错误的上下文误导，且内在知识和上下文矛盾时很容易产生质量较低的回复。通过**提示词优化**与**推理干预**方法没法很好的解决，需要设计更好的方法。
    - 只关注**生成**。

#### RAG的评估工具
最先进的自动化工具有RAGAS，ARES和Trulens，使用LLM来自动评估。这部分工作主要以和人类的标注对齐为主要贡献。

+ **RAGAS**：开源地址：[https://github.com/explodinggradients/ragas](https://github.com/explodinggradients/ragas)
    - 创建了WikiEval数据集：[https://huggingface.co/datasets/explodinggradients/WikiEval](https://huggingface.co/datasets/explodinggradients/WikiEval)。根据50个wiki页面提示ChatGPT得到问题和答案。	
    - 提示ChatGPT的方式评估，实验表明人类标注和RAGAS提出的标注方式的结果基本一致。
    - 评估目标包含上下文相关性，答案诚实性，答案相关性。
    - 关注**RAG整体**
+ **ARES：**开源地址：[https://github.com/stanford-futuredata/ARES](https://github.com/stanford-futuredata/ARES)
    - 和RAGAS的评估目标一致。训练一个小型LM来进行评估。
    - 包含KILT和SuperGLUE中的6个不同的知识密集型任务。
    - 关注**RAG整体**
+ **Trulens**：开源地址：[https://github.com/truera/trulens](https://github.com/truera/trulens)
    - 和RAGAS的评估目标一致。
    - 关注**RAG整体**

#### <font style="background-color:rgb(253, 253, 253);">为什么需要Reranker？</font>
**<font style="background-color:rgb(253, 253, 253);">因为在搜索的时候存在随机性（ANN），这应该就是我们在RAG中第一次召回的结果往往不太满意的原因</font>**<font style="background-color:rgb(253, 253, 253);">。但是这也没办法，如果你的索引有数百万甚至千万的级别，那你只能牺牲一些精确度，换回时间。这时候我们可以做的就是增加</font><font style="background-color:rgb(238, 238, 238);">top_k</font><font style="background-color:rgb(253, 253, 253);">的大小，比如从原来的10个，增加到30个。然后再使用更精确的算法来做rerank，使用一一计算打分的方式，做好排序。比如30次的遍历相似度计算的时间，我们还是可以接受的。</font>

#### <font style="background-color:rgb(253, 253, 253);">Reranker</font>评测方法有什么？
为了衡量我们的检索系统的有效性，我们主要依赖于两个被广泛接受的指标:**命中率**和**平均倒数排名(MRR)**。让我们深入研究这些指标，了解它们的重要性以及它们是如何运作的。我们来解释一下这两个指标：

+ **命中率:**Hit rate计算在前k个检索文档中找到正确答案的查询比例。简单来说，它是关于我们的系统在前几次猜测中正确的频率。
+ **平均倒数排名(MRR):**对于每个查询，MRR通过查看排名最高的相关文档的排名来评估系统的准确性。具体来说，它是所有查询中这些秩的倒数的平均值。因此，如果第一个相关文档是顶部结果，则倒数排名为1;如果是第二个，倒数是1/2，以此类推。

#### Rerank模型有哪些？
+ [<font style="background-color:rgb(253, 253, 253);">CohereAI</font>](https://txt.cohere.com/rerank/)<font style="background-color:rgb(253, 253, 253);">：API，收费</font>
+ <font style="background-color:rgb(253, 253, 253);">bge-reranker-base：智谱开源</font>
+ <font style="background-color:rgb(253, 253, 253);">bge-reranker-large：智谱开源</font>
+ bce-reranker-base_v1：网易有道开源

<font style="background-color:rgb(253, 253, 253);">有道基于llamaindex的评估leaderboard：</font>

![来源：网易有道测试结果](https://cdn.nlark.com/yuque/0/2024/png/406504/1705977165536-6d3ab416-eee1-4c6e-a849-76718d60ee1d.png)

+ WithoutReranker提供了没有重排情况下，不同Embedding模型的整体情况
+ 增加Reranker模块后，数据清楚地表明了重新排名在优化搜索结果方面的重要性。几乎所有嵌入都受益于重新排名，显示出命中率和MRR的提高

#### RAG中如何评估Embedding模型的效果
MTEB：2023年5月，Hugging Face和Cohere.ai联合发布Embedding的benchmark

C-MTEB：2023年8月，智谱和Hugging Face发布的中文评测Benchmark

leaderboard：[https://huggingface.co/spaces/mteb/leaderboard](https://huggingface.co/spaces/mteb/leaderboard)

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1705978864341-5d9c58db-7e87-4db0-aa99-38d7a9465018.png)

任务定义：

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1705977467800-d567ae1a-5668-4634-8fae-cf9e36783054.png)

**BitextMining(双语语料库挖掘)**：输入是来自两种不同语言的两组句子。对于第一组中的每个句子，需要在第二组中找到最佳匹配。匹配通常为翻译。提供的模型用于嵌入每句话，并通过余弦相似度查找最近的一对。F1 作为双语语料库挖掘的主要指标。还计算了准确率、精确率和召回率。

**Classification(分类)**：通过提供模型，生成训练集和测试集嵌入。使用训练集嵌入来训练逻辑回归分类器，最大迭代次数为100，该分类器在测试集上进行评分。主要指标是准确率、平均精度和f1值。

**Clustering(聚类)**：给定一组句子或段落，目标是将它们分组为有意义的群集。在嵌入文本上训练了一个小批量k-means模型（Pedregosa等人，2011年），批大小为32，k等于不同标签的数量。使用v-measure（Rosenberg和Hirschberg，2007）对模型进行评分。v-measure不依赖于聚类标签，因此标签的排列不会影响分数。

**PairClassification(配对分类)**：输入两段文本，需要分配一个标签。 标签通常是二进制变量，表示重复或近义词对。 这两个文本被嵌入并使用各种度量（余弦相似度、点积、欧几里得距离、曼哈顿距离）计算它们之间的距离。 使用最佳二元阈值精度、平均精度、F1、精确率和召回率进行计算。 基于余弦相似性的平均精度得分是主要指标。

**Reranking(重排序)：**输入是一组查询和一组相关或不相关的参考文本。目的是根据它们与查询的相关性对结果进行排名。模型用于嵌入参考文献，然后使用余弦相似度将其与查询进行比较。为每个查询生成一个排名，并对其所有查询取平均值。指标包括均值MRR@k和MAP，其中后者是主要指标。

**Retrieval(检索)：**每个数据集都包含一个语料库、查询和每个查询到语料库中相关文档的映射。目标是找到这些相关的文档。提供的模型用于嵌入所有查询和语料库中的所有文档，并使用余弦相似度计算相似性分数。根据分数对每个查询的语料库文档进行排名后，对于几个值k，可以计算nDCG@k、MRR@k、MAP@k、precision@k和recall@k。nDCG@10作为主要指标。MTEB重用BEIR（Thakur等人，2021）的数据集和评估。

**Semantic Textual Similarity(语义文本相似度（STS）)：**给定一对句子，目的是确定它们之间的相似性。标签是连续得分，数值越高表示越相似的句子。提供的模型用于嵌入句子，并使用各种距离度量计算其相似性。通过皮尔逊相关系数和斯皮尔曼等级相关系数与地面真实相似性进行比较。基于余弦相似性的斯皮尔曼等级相关系数用作主要指标（Reimers等人，2016）。

**Summarization(总结)**：提供了由人类编写的和机器生成的摘要。目的是对机器生成的摘要进行评分。首先，使用提供的模型对所有摘要进行嵌入。对于每个机器生成的摘要嵌入，计算其与所有人类生成的摘要嵌入之间的距离。保留最近的距离（例如，最高的余弦相似度），并将其用作单个机器生成的摘要的分数。计算与机器生成的摘要的地面真实评估之间的人类评估的相关性（Pearson相关性和Spearman相关性）。如STS所示，基于余弦相似性的Spearman相关性作为主要指标（Reimers等人，2016年）。

> **C-MTEB中文benchmark去除了****Summarization和BitextMining任务。**
>

#### 如何衡量大模型的幻觉？
人工评估是最常用且可靠的度量方法之一。但收集人工评估数据耗时耗力，因此还提出了几种自动度量方法。

+ **命名实体错误：**由于命名实体（NEs）是“事实”的核心构建块之一，因此可以使用命名实体匹配来计算生成输出与参考之间的重叠。直观地说，如果模型生成了一个在真实知识源中不存在的NE，那么可以认为模型在产生虚构错误（幻觉）。该度量可以定义为生成内容中不出现在真实文本中的NE数量除以生成内容中的总NE数量。较低的值更好。需要注意的是，该度量需要详尽的真实参考文本。
+ **蕴涵比率：**它定义为参考文本中蕴涵的句子数量除以生成的输出总句子数量。可以使用现成的蕴涵/自然语言推理模型来进行计算。
+ **基于模型的评估：**这些度量方法被提出来处理更复杂的句法甚至语义变化。
+ **使用问答系统：**这是基于这样的直觉，即如果生成的答案与源引用的内容在事实上是一致的，那么从一个问题生成的答案也应该是相似的。具体来说，给定生成的文本，问题生成（QG）模型生成一组问题-答案对。然后，问答模型使用地面真实源文本作为参考回答生成问题的答案，并计算相应答案的相似性。
+ **使用信息提取系统：**该方法使用信息提取模型将知识表示为更简单的关系元组，例如<主体，关系，客体>。它们从生成的输出中提取这些元组，并将它们与从源中提取的关系元组进行验证。

#### 如何进行幻觉检测？
+ 推理分类器：通过计算p（H）=FC（Q， A）来确定答案是否包含幻觉内容H，分类器不仅能判断文本中是否存在幻觉内容，还能通过不同的技术手段（如思考链的添加、生成中间过程标签和解释、采用与生成模型不同的分类器模型）来提高检测的准确性和可靠性。
+ 不确定性度量：通过使用多种不同的方法和指标，如模型的概率输出、BARTSCORE、KoK、SLAG、KLD和POLAR，可以从多个角度评估模型生成文本的不确定性和质量。
+ 自我评估：通过多种创新方法利用LLMs自身能力进行自我评估和事实核查，包括生成多答案比较一致性、触发自我矛盾检测、随机抽样简化判断、分解复杂陈述逐个核查以及双模型交互审查等技术。
+ 证据检索：通过多种方法帮助我们找出真实可靠的资料来验证生成文本中提到的信息是否准确无误，如FActScor和FacTool检测框架

#### 如何进行幻觉矫正？
+ 参数适应： LLM中的参数存储了预训练中学习到的偏差，通常与用户意图不一致。一个前沿策略是通过参数调节、编辑和优化来引导有效知识。方法包括对抗学习参数优化、引入上下文知识、编辑和去噪处理、隐藏状态对齐、任务特定参数利用等。
+ 后处理归因和编辑技术：通过分析、对齐推理方式、自我完善、反思性反馈和核实编辑等多种技术手段，减少LLMs在生成文本时的幻觉问题，提高生成内容的准确性和可靠性。
+ 利用外部知识：通过检索相关文档融入模型处理、结合查询与生成过程、针对性记忆检索、以及利用结构化数据和API调用等技术来增强LLM的能力并确保其答案的真实可靠性。
+ 评估反馈：通过使用强化学习评估人类偏好，反馈偏好微调，构建模型的自我评估功能，扩展集成检索系统等方法进行评估反馈
+ 心智社会(Mindset Society)：多个方法和框架通过多模型合作、辩论、裁判介入和多轮互动等方式，鼓励LLMs生成更准确、更一致的信息。

#### 大模型幻觉产生的机制是什么？
+ 数据：
    - 预训练数据的不完整或过时：LLMs在预训练过程中获取的知识可能因为数据源的限制而不完整或陈旧，导致模型在处理某些问题时缺乏准确的信息。
    - 依赖语料库的基本启发式方法：当模型无法获取相关信息时，可能会退回到基于词频的简单判断方法，这可能导致与实际情况不符的输出。
    - 上下文学习能力的偏差：当模型以一些示例作为输入的上下文时，其获取知识的能力可能受到标签类别差异和示例顺序的影响，从而导致输出的偏差。
    - 多语言处理中的挑战：对于多语言LLMs，特别是在处理资源有限或非英语翻译的语言对时，可能会出现幻觉，因为这些语言对的处理更加复杂和挑战性。
+ 知识：
    - 预训练和微调阶段输入格式的差异
    - 知识库问题：即使有自动更新机制，信息的准确性和时效性仍然存在问题。问题与存储知识的不对齐
    - 检索证据忽视导致的偏见：忽视检索到的证据会引入偏见的模型知识，而错误覆盖和过度思考则会扰乱模型行为。
    - 使用缓存组件时的不一致性：在训练中使用缓存组件提供历史记忆时，有些信息和当前正在学习的信息不一致。
+ 优化过程：
    - 随机模仿：模型被引导去模仿训练数据，而不是真正理解它们
    - 训练和测试阶段的暴露偏差：不同阶段模型接触到的数据差异可能导致模型在实际应用中出现错误。
    - 高不确定性的采样技术：具有高不确定性特征的采样技术（如top-p和top-k）加剧了幻觉问题。这些技术可能使模型在生成文本时更容易偏离准确和相关的信息。
    - 连锁幻觉：LLMs倾向于产生连锁幻觉，用于维持与早期幻觉的连贯性。

#### 请介绍下BGD（批量梯度下降）
批量梯度下降法（Batch Gradient Descent，简称BGD）是梯度下降法最原始的形式，它的具体思路是使用所有的样本来更新每一参数。批量梯度下降（BGD）是一种最优化算法，用于求解损失函数的最小值。它在每次迭代中使用整个训练数据集来计算梯度，然后使用梯度来更新模型参数。

BGD的优点是：

+ 它可以找到损失函数的全局最优解。
+ 它易于并行实现，可以提高训练速度。

BGD的缺点是：

+ 当训练数据集很大时，计算梯度会很耗时。
+ 它可能无法找到稀疏数据的良好解。

BGD的更新公式如下：θ = θ - α∇J(θ)

其中：

+ θ 是模型参数
+ α 是学习率
+ ∇J(θ) 是损失函数的梯度

BGD的具体步骤如下：

1. 初始化模型参数 θ
2. 计算损失函数 J(θ) 的梯度 ∇J(θ)
3. 使用梯度更新模型参数 θ
4. 重复步骤 2 和 3，直到达到收敛条件

BGD 的常见变体包括：

+ 随机梯度下降 (SGD)：SGD 在每次迭代中只使用一个随机样本来计算梯度。SGD 的优点是计算效率高，但可能无法找到损失函数的全局最优解。
+ 小批量梯度下降 (Mini-batch SGD)：Mini-batch SGD 在每次迭代中使用一小批随机样本来计算梯度。Mini-batch SGD 在计算效率和收敛精度之间取得了平衡。

BGD 在机器学习中被广泛使用，尤其是在训练大型模型时。

#### 梯度(在数学上的定义)
**梯度：**是一个向量（矢量），表示某一函数在该点处的方向导数沿着该方向取得最大值，即函数在该点处沿着该方向（此梯度的方向）变化最快，变化率最大（为该梯度的模）。

![](https://cdn.nlark.com/yuque/0/2024/webp/35381469/1707182986384-4a93dc26-74c8-4c8b-8ca5-b5bb68b3a028.webp)

#### 梯度下降背后的原理
**（一）梯度下降的本质是：**

**对****损失函数****进行****一阶泰勒展开的****近似****，然后对****这个****近似出来的函数****求最小值****，把最小值当作下一步用来迭代的值。**

**（二）具体实现细节：**

泰勒发现**任何一个函数****都可以写成关于N阶导数的一个多项式**。

以MSE为例：

  ![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1707183732795-020eba7b-8e9b-4abb-b8eb-13efbbfeaed5.png)

那么，任意函数只要(n+1)阶可导，那么其在**X0点附近**可以表达如下：

![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1707183446763-8dd09747-036b-4d7e-b3d3-3b0744dc25ea.png)

** 每一次做梯度下降，为的是让该值距离最优解更近点****。故下一次我们要找的** ![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1707183633616-b7200e22-e831-4ba8-93b4-3ff51cbe067d.png) **要尽可能的比** ![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1707183644401-00f776f9-d2d3-4229-bdb5-4df02056e77f.png) **小。**

![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1707183659603-77106ddb-1db3-46c9-b2b0-d19e1437fe4c.png)

（三）梯度下降法迭代步骤

一般线性回归函数的假设函数为：

![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1707184396018-5c108b93-a8f6-4e0e-9092-8b8b00fe854a.png)

对应的损失函数为：

![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1707184411278-d304cdc4-2147-40b3-8e6d-7705c845a9e4.png)

我们的目的是要**误差函数尽可能的小**。首先，我们随机初始化weigths，然后**不断的更新weights使得误差函数减小，**直到满足要求时停止。

![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1707184443790-bd471179-f5a0-4c3b-a3a8-23e25c428ce5.png)

为了更新weights，**我们需要求出函数J的偏导数。首先当我们只有****一个****数据点（x,y）的时候，J的偏导数****是：**

****![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1707184443609-a61a0427-56fb-40c8-a5bd-29d15d41d598.png)

则对**所有数据点****，**上述损失函数的偏导（**累和**）为：

![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1707184443689-6d8072a8-ffcc-4bf2-8282-9dcb54b2fea3.png)

再最小化损失函数的过程中，**需要不断反复的更新weights使得误差函数减小**，更新过程如下：

![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1707184443622-726af077-1cef-4aee-85e3-dd9769415fb8.png)

那么好了，**每次参数更新****的伪代码**如下：

![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1707184443660-b37dc6cf-055c-420d-a6a9-0ec7e7f606e0.png)

#### 请介绍下Momentum
**指数加权平均**（Exponentially weighted average）：**本质就是****以**[指数式](https://link.zhihu.com/?target=http%3A//zh.wikipedia.org/w/index.php%3Ftitle%3D%25E6%258C%2587%25E6%2595%25B0%26variant%3Dzh-cn)**递减加权的移动平均****。各数值的加权随时间指数式递减，越近期的数据加权越重。****（解决梯度下降法梯度消失问题提出）**

![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1707184756479-17725c50-7d57-4c4e-916a-cda5322c8401.png)

> ![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1707184768780-8db2ee1b-c625-487b-92ac-34cc43e2be4f.png)
>
> ![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1707184830652-845722ec-f8e4-43d4-a696-4c9b0a85dd7a.png)
>

**动量**的引入就是为了**加快学习过程**，主要思想是**积累了之前梯度指数级衰减的移动平均**（前面的指数加权平均）。

> **动量法中自变量在各个方向上得****移动幅度不仅取决于当前梯度，还取决于过去得各个梯度在各个方向上是否一致**。
>

Momentum 公式

![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1707102540020-36ebb61d-40d9-4b4c-819f-0376ff771f34.png)

#### 请解释全参微调
**全量微调**：预训练模型的所有层和参数都会被更新和优化，适用于任务和预训练模型之间存在较大差异的情况，需要较大的计算资源和时间。

+ **全量微调参数更新示例：**

以语言模型为例，在微调过程中模型加载预训练参数![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1707096991415-5c8d1fde-97b8-41db-bf93-8638cc069f0c.png)进行初始化,并通过最大化条件语言模型概率进行参数更新![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1707097009119-41b2f414-e2c2-472b-ab08-ff3970b01006.png)，即：

![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1707097101940-80f09e65-bfa1-41c1-b276-c3a227ad9055.png)

+ **全量微调缺点：**

·学习到的参数增量![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1707097129788-2e6cbf24-46db-4000-8f7a-a153837ac64d.png)的维度和预训练参数![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1707097147298-70302b37-a99e-4b7b-9b2c-c28142d95a94.png)是一致的，这种微调方式所需的资源很多。

#### 请解释PEFT
+ **参数高效微调：**指固定大部分预训练模型（LLM）参数，微调少量或额外的模型参数，从而大大降低了计算和存储成本，同时，也能实现与全量参数微调相当的性能。

**参数高效微调本质：用更少的参数表示要学习的参数增量**![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1707097177118-085aa93f-e19c-48b1-8f27-c683cdfa5388.png)，其中![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1707097192043-9b0f096d-c89c-4c01-a684-369ee248f16d.png)，原先寻找![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1707097129788-2e6cbf24-46db-4000-8f7a-a153837ac64d.png)的优化目标变为寻找![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1707097219054-b4aa3e9a-1e99-4ebf-af39-f888c027b3bc.png)：

![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1707097238419-791e0f48-6a25-4d02-b1c0-632425aff06d.png)

+ **PEFT（Parameter-Efficient Fine-Tuning）**是hugging face开源的一个参数高效微调大模型的工具，里面集成了多种微调大模型的方法，可以通过微调少量参数就达到接近微调全量参数的效果。

**PEFT发展历程：**

![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1707115642573-376f5362-3dc4-4456-a7b0-0e25cbf0cbe5.png)

PEFT代码开发：[https://github.com/huggingface/peft](https://github.com/huggingface/peft)

#### 请解释下SVD
SVD分解

奇异值分解（SVD）在降维，数据压缩，推荐系统等有广泛的应用，**任何矩阵都可以进行奇异值分解。**

正交变换

正交变换公式：![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1707185372699-9db63a8c-9454-427b-a1f2-4d8e3c410522.png)，表示X是Y的正交变换 ，其中U是正交矩阵，X和Y为列向量 。

SVD分解推导

![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1707101169801-fee1b871-07a3-489d-b5fd-6b38bbf7da39.png)

**（一）进行正交变化**

![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1707185543749-fb718acd-92b8-4dd5-9f26-f2e28b02f514.png)

   	即：![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1707185564533-fe12ff02-a41b-4e77-8dd7-2d339d70612a.png)

**（二）奇异值分解**

**当我们找到这样的** ![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1707101558758-558fdd94-5bf3-4370-991d-364c553d0e9b.png)**矩阵后，或者从三者中****取出对应的top r 行（或列），相当于关注到了**![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1707101569488-24a4bb93-7298-4625-af40-32ea6d18b85b.png) **最重要的几维特征，就可以用更低维的矩****阵来近似表达** ![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1707101576132-96102003-5e7d-495b-82ed-23d879d3ac08.png) **了。**按这种思维拆解M的方法，我们称为**SVD分解（奇异值分解）**。

```python
import torch
import numpy as np
torch.manual_seed(0)

# ------------------------------------
# n：输入数据维度
# m：输出数据维度
# ------------------------------------
n = 10
m = 10

# ------------------------------------
# 随机初始化权重W
# 之所以这样初始化，是为了让W不要满秩，
# 这样才有低秩分解的意义
# ------------------------------------
nr = 10
mr = 2
W = torch.randn(nr,mr)@torch.randn(mr,nr)

# ------------------------------------
# 随机初始化输入数据x
# ------------------------------------
x = torch.randn(n)

# ------------------------------------
# 计算Wx
# ------------------------------------
y = W@x
print("原始权重W计算出的y值为:\n", y)

# ------------------------------------
# 计算W的秩
# ------------------------------------
r= np.linalg.matrix_rank(W)
print("W的秩为: ", r)

# ------------------------------------
# 对W做SVD分解
# ------------------------------------
U, S, V = torch.svd(W)

# ------------------------------------
# 根据SVD分解结果，
# 计算低秩矩阵A和B
# ------------------------------------
U_r = U[:, :r]
S_r = torch.diag(S[:r])
V_r = V[:,:r].t()

B = U_r@S_r # shape = (d, r)
A = V_r     # shape = (r, d)

# ------------------------------------
# 计算y_prime = BAx
# ------------------------------------
y_prime = B@A@x

print("SVD分解W后计算出的y值为:\n", y)

print("原始权重W的参数量为: ", W.shape[0]*W.shape[1])
print("低秩适配后权重B和A的参数量为: ", A.shape[0]*A.shape[1] + B.shape[0]*B.shape[1])

#################  结果 ########################
原始权重W计算出的y值为:
 tensor([ 3.3896,  1.0296,  1.5606, -2.3891, -0.4213, -2.4668, -4.4379, -0.0375,
        -3.2790, -2.9361])
W的秩为:  2
SVD分解W后计算出的y值为:
 tensor([ 3.3896,  1.0296,  1.5606, -2.3891, -0.4213, -2.4668, -4.4379, -0.0375,
        -3.2790, -2.9361])
原始权重W的参数量为:  100
低秩适配后权重B和A的参数量为:  40
```

#### 为什么Lora是近似SVD而不是精确的？
**为什么需要近似：**高维矩阵进行奇异值分解的效率非常慢。

![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1707269106733-6218c5a9-510b-4d04-b49a-16f3fcdce832.png)

![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1707193479465-c9540e0f-effd-425e-8458-723003a9ab79.png)

![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1707185817588-1645dba9-0807-489a-a6c1-4e54809d483d.png)

#### 常见的数据精度有哪些？
+ **浮点数精度：**双精度（FP64）、单精度（FP32、TF32）、半精度（FP16、BF16）、8位精度（FP8）、4位精度（FP4、NF4）
+ **量化精度：**INT8、INT4 （也有INT3/INT5/INT6的）
+ **NF4（4-bit NormalFloat）：**一种用于量化的特殊格式，于23年5月由华盛顿大学在QLoRA量化论文中提出。NF4是建立在**分位数量化技术**的基础之上的一种信息理论上最优的数据类型。把4位的数字归一化到均值为 0，标准差为 [-1,1] 的正态分布上。
+ **FP（Floating Point）：**是最原始的，IEEE定义的标准浮点数类型。
    - **FP16：**也叫做 float16，全称是Half-precision floating-point(半精度浮点数)，在IEEE 754标准中是叫做binary16，由1位符号位、5位指数位和10位尾数位组成。它的表示范围远小于**FP32**，因此容易出现上溢出和下溢出问题（**引出BF**）。
    - **FP8（8-bit Float）**，它是由论文《FP8 for deep learning》提出，包括两种编码方式，E4M3 (4-bit 指数3-bit 尾数)和E5M2 (5-bit 指数2-bit 尾数)

![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1707187302878-3458ad03-a55e-4f2c-9c57-d7cb9c723dd8.png)

**E4M3**能表示的数值范围是-448 to 448，**E5M2**能表示的数据范围是-57344 to 57344。后者的精度相比前者更低，经验表示**E4M3**适合用于前向传播，**E5M2**适合用于反向传播。

+ **BF（Brain Floating Point）：**
    - **BF16：**也叫做bfloat16(这是最常叫法)，是由**Google Brain**开发的，所以这个brain应该是Google Brain的第二个单词。由1位符号位、8位指数位和7位尾数位组成。BF16能表示的动态范围与FP32一致，但是相比FP16损失3 bits的精度。
+ **TF32**

**TF32**诞生于NVIDIA显卡的安培架构，它综合了BF16和FP16的特点，只占用19 bits内存，但它只能用在某些限定算子内。

#### 请解释下量化
**量化**：从含有更多信息的表征映射为含有较少信息的表征的过程，如将FP32的数据转化为INT8，能够节省大量的内存。

**模型量化（quantization）**：被叫做模型的低精度表示，在不大幅降低模型效果的前提下使用更低的精度来表示模型中的参数，从而缩减模型的体积和训练模型时占用的显存。

最常见的 量化技术是最大绝对值量化(absolute maximum quantization，absmax) ，如下面公式

![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1707104209776-262b6426-234d-4ca0-98d6-aa73f690b07a.png)

量化分为：**对称量化** 和**非对称量化**

**对称量化** - 目标是为0的参数在量化后仍保持为0。将原浮点数的最小或最大值的绝对值作为映射值的范围。

**非对称量化 **- 如果量化过程中0的表示发生改变，则此方式被称为非对称量化。将原浮点数的最小和最大值映射为量化数据的最小和最大值。

![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1705996012489-2902b705-b630-4ccb-8428-5e41be9dcf7a.png)

#### 请介绍下通用int8量化
Int8量化是一种优化技术，它将32位的浮点数（float32）模型参数转换为8位整数（int8）。这有助于减少模型的内存占用，加快其执行速度，并有可能减少能耗，特别是在边缘设备上。以下是完成Int8量化所采用的基本计算公式：

通用 INT8-Bit量化

**（一）公式：**

量化和反量化过程如下所示：

![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1707188640453-34857517-9ae0-4602-b6f5-e33ce58fdde0.png)

其中 c 是量化常数

**（二）通用流程：**

**（1）确定缩放因子（Scale）和零点（Zero Point）：**

首先，计算缩放因子（scale）和零点（zero point）。缩放因子用于将浮点数映射到整数范围，零点则确保浮点数0可以被准确地表示为整数。

计算scale和zero_point通常需要最大值（max_val，127）和最小值（min_val，-128）：

```python
scale = (max_val - min_val) / (2^7 - 1)  
# 127是int8的最大值 
zero_point = round(-min_val / scale)
```

这里，zero_point需要在-128到127的范围内，因为这是8位有符号整数的范围。

**（2）应用量化：**

使用scale和zero_point，量化一个浮点数（float_val）成一个8位整数（int_val）：

```python
int_val = round(float_val / scale) + zero_point
```

**（3）整数裁剪：**

如果量化后的整数超出了-128到127的范围，它需要被裁剪以保持在有效范围内。

```python
int_val = min(127, max(-128, int_val))
```

这样，量化过程就可以将32位浮点数数组转换为能够在8位整数范围内表示的数组，这减少了内存占用，同时可能提高了执行速度。

#### 介绍下LLM.int8()
大模型的零退化矩阵乘法量化技术（int8量化的提升）

**方法：使用混合精度分解（Mixed-precision Decomposition）**：异常值会影响量化后模型的计算精度。transformers存在一些非常大的异常值，在数组中加入一个异常值98，处理后的结果为[-0. , -0. , 0. , -0. , -0. , -0.77, -2.31, 0. , -0.77, 98. ]，转化后的大部分信息都丢失了。因此，可以将异常特征的从矩阵中分离出来，保留原始精度进行矩阵乘法，剩余的（99.9%）特征进行int8量化计算。然后将两者的输出结合在一起，从而保留模型性能。

**LLM.int8()会通过以下三步完成矩阵乘法：**

1. 从矩阵隐藏层中，以列为单位，抽取outliers（数值超出全局阈值范围的）。
2. 分别通过**FP16精度**对**outliers**的部分做矩阵乘法，通过量化**int8精度**对其他的做矩阵乘法。
3. 将量化的部分恢复成FP16，然后将两部分合在一起。

![](https://cdn.nlark.com/yuque/0/2024/png/35381469/1707107178748-ae472a21-f12d-4691-9f8d-0e8799728b12.png?x-oss-process=image%2Fresize%2Cw_770%2Climit_0)

![](https://cdn.nlark.com/yuque/0/2024/gif/35381469/1707107288358-6d08a6b8-5af3-47a8-81bb-60d26a3afa81.gif)

#### 介绍下BPE算法
BPE（Byte Pair Encoding）算法是一种用于文本处理的字节对编码技术，广泛应用于自然语言处理（NLP）中的词汇表构建。BPE 是一种基于频率的迭代算法，用于将文本序列分解为更小的、更频繁出现的子单元，这些子单元可以是单个字符、字符对或其他符号。这种方法特别适用于处理词汇量大、出现频率低的单词，以及在处理多种语言时需要共享词汇表的情况。

BPE 算法的基本步骤如下：

1.  **初始化词汇表**：开始时，词汇表包含所有可能的单字符（例如，对于英文，这将是26个字母加上空格和其他标点符号）。 
2.  **合并规则**：算法会根据一定的规则（通常是合并频率最高的字符对）来迭代地扩展词汇表。在每次迭代中，算法会寻找并合并最常见的字符对，生成新的词汇表条目。 
3.  **重复迭代**：这个过程会重复进行，直到达到预定的词汇表大小或者合并规则不再有效为止。每次迭代后，新的词汇表条目会替换原来的字符对，成为文本中的新符号。 
4.  **生成分词规则**：最终，算法会生成一组分词规则，这些规则定义了如何将原始文本中的单词分解为词汇表中的子单元序列。 

BPE 算法的特点

+ **数据压缩**：BPE 算法可以有效地压缩文本数据，减少存储空间和计算资源的消耗。
+ **适应性强**：BPE 算法不依赖于特定的语言或符号集，可以灵活地应用于不同的文本处理任务。
+ **词汇共享**：在多语言模型中，BPE 算法可以帮助构建共享的词汇表，使得模型能够在不同的语言之间共享知识。

BPE 算法在自然语言处理领域有广泛的应用，尤其是在以下方面：

+ **机器翻译**：在构建多语言翻译系统时，BPE 可以帮助生成共享的词汇表，提高翻译效率。
+ **文本分类**：BPE 可以用于生成更紧凑的词汇表，提高文本分类模型的性能。
+ **词嵌入**：在构建词嵌入模型时，BPE 可以减少词汇表的大小，提高模型的训练和推理速度。

BPE 算法是一种简单而强大的文本预处理技术，它通过迭代合并字符对来优化词汇表，从而在不牺牲太多信息的情况下减少数据的复杂性。这种方法在处理大规模文本数据时尤其有用，能够显著提高自然语言处理任务的效率和效果。

#### 

# 思想链（CoT）

思维链最初是在谷歌研究人员的论文[“Chain-ofthought Prompting Elicits Reasoning in Large Language Models”中描述的。](https://arxiv.org/abs/2201.11903)这里的简单想法是，鉴于 LLM 已经接受过预测标记而不是明确推理的训练，如果你指定那些所需的推理步骤，你可以让它们更接近推理。这是原始论文中的一个简单示例：

![https://cdn.nlark.com/yuque/0/2023/png/406504/1697251199137-193447f4-e2e2-4b10-9bc1-872be37ef93a.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1697251199137-193447f4-e2e2-4b10-9bc1-872be37ef93a.png)

请注意，在这种情况下，“所需的推理步骤”在示例中以蓝色给出。这就是所谓的“手动 CoT”。有两种进行思维链提示的方法（见下文）。在称为零样本 CoT 的基本方法中，你只需要求大模型“一步一步思考”。在更复杂的版本中，称为“手动 CoT”，你必须逐步给出 LLM 思维示例，以说明如何推理。手动提示更有效，但更难扩展和维护。

![https://cdn.nlark.com/yuque/0/2023/png/406504/1697251201212-00d71c8f-b1ca-464a-9036-34f764a18007.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1697251201212-00d71c8f-b1ca-464a-9036-34f764a18007.png)

# 自动思维链（Auto-CoT）

如上所述，手动 CoT 比零样本更有效。然而，这种基于示例的 CoT 的有效性取决于不同示例的选择，并且用这种手动逐步推理的示例构建提示是困难且容易出错的。这就是“Automatic Chain of Thought Prompting in Large Language Models”论文中提出的自动 CoT 发挥作用的地方。

该方法如下图所示：

![https://cdn.nlark.com/yuque/0/2023/png/406504/1697251197399-6ca7cf0f-24cc-4b8d-89ce-db8f3b9aca74.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1697251197399-6ca7cf0f-24cc-4b8d-89ce-db8f3b9aca74.png)

# 格式化

大模型非常擅长以特定格式生成输出。你几乎可以将格式技巧用于任何事情。

![https://cdn.nlark.com/yuque/0/2023/png/406504/1697251201946-79223173-01af-4627-b6a7-08b61aaaa1b3.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1697251201946-79223173-01af-4627-b6a7-08b61aaaa1b3.png)

![https://cdn.nlark.com/yuque/0/2023/png/406504/1697251203740-0210686e-6484-44a4-b697-fd51dd6cf003.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1697251203740-0210686e-6484-44a4-b697-fd51dd6cf003.png)

# 工具、连接器和技能

工具通常被定义为大模型可以用来与外部世界交互的方法、手段。

例如，在 Langchain 的以下代码中，实例化了一个“Google 工具”并用于搜索网络：

![https://cdn.nlark.com/yuque/0/2023/png/406504/1697251204132-74a6af62-4e86-40e0-9987-5380681233f3.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1697251204132-74a6af62-4e86-40e0-9987-5380681233f3.png)

在论文《[Toolformer: Language Models Can Teach Themselves to Use Tools](https://arxiv.org/abs/2302.04761)》中，作者超越了简单的工具使用，通过训练大模型来决定何时使用什么工具，甚至 API 需要哪些参数。工具包括两个不同的搜索引擎或计算器。在以下示例中，大模型决定调用外部问答工具、计算器和维基百科搜索引擎。

![https://cdn.nlark.com/yuque/0/2023/png/406504/1697251206707-a3f225d5-b5ee-47db-9172-2eebafe9ad5e.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1697251206707-a3f225d5-b5ee-47db-9172-2eebafe9ad5e.png)

最近，伯克利的研究人员训练了一位名为[Gorilla](https://shishirpatil.github.io/gorilla/)的新大模型，该模型在 API的使用方面击败了 GPT-4。

# 自动多步推理和工具使用（ART）

[ART](https://arxiv.org/abs/2303.09014)结合了自动思维提示链和工具使用，因此它可以被视为我们迄今为止所看到的一切的组合。论文中的下图说明了总体方法：

![https://cdn.nlark.com/yuque/0/2023/png/406504/1697251207092-93a754f3-7441-4d58-9a3e-bb2c8dd1b4c6.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1697251207092-93a754f3-7441-4d58-9a3e-bb2c8dd1b4c6.png)

给定任务和输入，系统首先从任务库中检索“相似任务”。这些任务将作为示例添加到提示中。请注意，库中的任务是使用特定格式编写的。给定这些任务示例，大模型将决定如何执行当前任务，包括是否需要调用外部工具。

在生成时，ART 系统解析 LLM 的输出，直到调用工具，此时该工具被调用并集成到输出中。人工反馈步骤是可选的，用于改进工具库本身。

# 自我一致性

自我一致性，在论文《[SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models](https://arxiv.org/abs/2303.08896)》中介绍，是一种使用 LLM 进行事实检查的方法。这个想法是一种简单的基于集成的方法，其中大模型被要求对同一提示生成多个响应。这些响应之间的一致性表明响应的准确性。

![https://cdn.nlark.com/yuque/0/2023/png/406504/1697251208425-abb62ded-9522-484a-8e2a-098867a46b24.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1697251208425-abb62ded-9522-484a-8e2a-098867a46b24.png)

上图说明了 QA 场景中的方法。在这种情况下，“一致性”是通过与总体答案一致的段落答案的数量来衡量的。然而，作者引入了另外两种一致性度量（BERT 分数和 n-gram），以及结合了这三种度量的第四种度量。

# 思维树 (ToT)

思维树是 CoT 思想的演变，大模型可以考虑多种替代的“推理路径”（见下图）

![https://cdn.nlark.com/yuque/0/2023/png/406504/1697251211688-215ad96a-6ba4-433f-89f6-d4a42ce4636b.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1697251211688-215ad96a-6ba4-433f-89f6-d4a42ce4636b.png)

ToT 从传统人工智能工作中汲取灵感，计划构建一个系统，其中大模型可以维护多个并行“线程”，这些“线程”在生成过程中进行一致性评估，直到确定其中一个是最好的并用作输出。这种方法需要定义一个关于候选人数量以及评估这些候选人的步骤/想法数量的策略。例如，对于“创意写作”任务，作者使用 2 个步骤和 5 个候选。但是，对于“填字游戏”任务，他们最多保留 10 个步骤并使用 BFS 搜索。

# 无观察推理（ReWOO）

ReWOO 最近在论文[“ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models”](https://arxiv.org/abs/2305.18323)中提出。这种方法通过增加 LLM 的调用和令牌数量，从而增加成本和延迟。ReWOO 不仅提高了Token效率，还展示了对工具故障的鲁棒性，并且在使用较小模型时也显示出了良好的结果。

该方法如下图所示。给定一个问题，Planner会在工具响应之前提供完整的计划或计划列表。该计划指示 Worker 使用外部工具并收集证据。最后，计划和证据被发送给求解器Solver，由求解器得出最终答案。

![https://cdn.nlark.com/yuque/0/2023/png/406504/1697251213836-6a385ccd-b02d-437c-81ce-7357031013d5.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1697251213836-6a385ccd-b02d-437c-81ce-7357031013d5.png)

下图也来自论文，通过与观察推理的“标准”方法进行比较，说明了该方法的主要优点。在后者中，每次调用工具（观察）时都会查询 LLM，这会产生大量潜在的冗余（因此会产生成本和延迟）。

![https://cdn.nlark.com/yuque/0/2023/png/406504/1697251213976-ddb1b8f7-58ea-4957-8021-ee62c1097612.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1697251213976-ddb1b8f7-58ea-4957-8021-ee62c1097612.png)

# 检索增强生成 (RAG)

RAG 是一种用于增强大模型的技术已有一段时间了。它是[Facebook](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/) 在 2020 年作为改进 BART 的一种方式提出的，并作为[Huggingface 库的](https://huggingface.co/docs/transformers/model_doc/rag)一个组件发布。

这个想法很简单：将检索组件与生成组件相结合，使两个来源相互补充 。

![https://cdn.nlark.com/yuque/0/2023/png/406504/1697251215080-e43b81f5-397d-43a8-8702-b8c28fd1ebc7.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1697251215080-e43b81f5-397d-43a8-8702-b8c28fd1ebc7.png)

RAG 已成为即时工程师工具包的重要组成部分，并且已发展成为更加复杂的方法。事实上，此时你几乎可以将 RAG 视为工具的具体案例，其中该工具是一个简单的检索器或查询引擎。[Intel 的FastRAG 库](https://github.com/IntelLabs/fastRAG)不仅包含基本的 RAG 方法，还包含更复杂的 RAG 方法，感兴趣的可以尝试下。

# 主动检索增强生成（FLARE）

[FLARE](https://arxiv.org/abs/2305.06983)是一种先进的 RAG 方法，系统不是仅检索一次信息然后生成信息，而是迭代地使用即将出现的句子的预测作为查询来检索相关文档，以便在置信度较低时重新生成句子。论文中的下图清楚地说明了该方法。

![https://cdn.nlark.com/yuque/0/2023/png/406504/1697251215704-3536bbe0-609b-4dad-9dba-fd81424d6ca2.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1697251215704-3536bbe0-609b-4dad-9dba-fd81424d6ca2.png)

请注意，作者通过为生成的句子的每个标记设置概率阈值来衡量置信度。然而，其他信心措施也是可能的。

# 反射

在自我一致性方法中，我们看到了如何使用大模型来推断响应的置信度。在这种方法中，置信度是根据对同一问题的多个回答的相似程度来衡量的。更进一步，试图回答我们是否（或如何）可以直接向大模型询问其回答的信心的问题。

反射[论文](https://arxiv.org/abs/2303.11366)提出了一种定义为“通过言语反射强化”的方法，该方法具有不同的组成部分。Actor本身就是一名大模型，会产生一条轨迹（假设）。Evaluator会根据该假设的好坏程度给出分数。自反射组件生成存储在内存中的摘要。迭代重复该过程，直到评估器确定它有“足够好”的答案。

![https://cdn.nlark.com/yuque/0/2023/png/406504/1697251215829-cc483b79-8b11-4866-8ccf-e4a9ec61f495.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1697251215829-cc483b79-8b11-4866-8ccf-e4a9ec61f495.png)

# 对话解析 (DERA)

[DERA](https://arxiv.org/abs/2303.17071)定义了不同的代理，这些代理在对话的背景下扮演不同的角色。在医学对话等高风险情况下，定义一组“研究人员”和“决策者”是值得的。这里的主要区别在于，研究人员并行操作，而反射参与者仅在评估者决定时才按顺序操作。

![https://cdn.nlark.com/yuque/0/2023/png/406504/1697251216184-1aaf672b-2240-44a1-bf1a-b267e6c8e1f1.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1697251216184-1aaf672b-2240-44a1-bf1a-b267e6c8e1f1.png)

# 专家提示

最近提出的提示方法建议要求大模型作为专家做出回应。它涉及 3 个不同的步骤：

- 要求大模型找出与提示/问题相关的特定领域的专家
- 请大模型像每位专家一样回答问题
- 通过生成的响应之间的协作做出最终决定

# Chains

当然，现在有多种类型的链chain，你可以组合不同类型和数量的组件，从而增加复杂性。在最简单的情况下，一条链只有一个提示模板、一个模型和一个输出解析器。

但很快，链条就会变得更加复杂和复杂。例如，MapReduce 链对每个数据块运行初始提示，然后运行不同的提示来组合所有初始输出。

由于构建和维护链的过程可能成为一项相当大的工程任务，因此最近出现了许多支持它的工具。主要的就是已经提到的LangChain。在 [《PromptChainer：通过可视化编程链接大型语言模型提示》一文](https://arxiv.org/abs/2203.06566)中，作者不仅描述了设计链的主要挑战，还描述了支持这些任务的可视化工具。

![https://cdn.nlark.com/yuque/0/2023/png/406504/1697251217205-cf91a602-a91a-418e-9c2f-1a080fb2f9ac.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1697251217205-cf91a602-a91a-418e-9c2f-1a080fb2f9ac.png)

# Agent

代理是大模型，可以访问工具，知道如何使用它们，并且可以根据输入决定何时使用。

![https://cdn.nlark.com/yuque/0/2023/png/406504/1697251217916-664b4ddf-7720-44d2-9690-634f19b1dbca.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1697251217916-664b4ddf-7720-44d2-9690-634f19b1dbca.png)

代理的实施和维护并不简单，这就是为什么像 Langchain 这样的工具已经成为大多数有兴趣构建代理的人的起点。“流行的” [Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT)也是实施 LLM 代理的另一个工具包[。](https://en.wikipedia.org/wiki/Auto-GPT)

# React

React 是 Google 在[“ReAct: Synergizing Reasoning and Acting in Language Models”](https://www.promptingguide.ai/techniques/react)中介绍的一种设计代理的具体方法。该方法促使LLM以交错的方式生成言语推理轨迹和动作，从而使模型能够执行动态推理。重要的是，作者发现 React 方法减少了 CoT 的幻觉。然而，这种基础性和可信度的增加，也是以推理步骤的灵活性略有降低为代价的。

![https://cdn.nlark.com/yuque/0/2023/png/406504/1697251218617-4e8dcc10-305b-4842-82ab-6fa3bdf3fdec.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1697251218617-4e8dcc10-305b-4842-82ab-6fa3bdf3fdec.png)

与链和代理一样，设计和维护 React 代理是一项非常复杂的任务。

# 自动提示工程（APE）

APE 是指由大模型而不是人类自动生成提示的方法。该方法在 “Large Language Models Are Human-Level Prompt Engineers”论文中介绍，涉及以三种方式使用大模型：生成建议的提示、对它们进行评分以及向得分高的提示提出类似的提示（见下图） ）。

![https://cdn.nlark.com/yuque/0/2023/png/406504/1697251219393-5231c316-d3c3-4513-944b-e1d1fceae1d4.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1697251219393-5231c316-d3c3-4513-944b-e1d1fceae1d4.png)

# 约束提示

“约束提示”用于描述允许我们在 LLM 流程中交错生成、提示和逻辑控制的方法和语言。该工具与其说是一种提示方法，不如说是一种“提示语言”。使用指导模板，你几乎可以实现本文中的大多数（如果不是全部）方法。[Guidance 使用基于Handlebars 的](https://handlebarsjs.com/)语法，允许交错提示和生成，以及管理逻辑控制流和变量。由于指导程序是按照它们将被执行的精确线性顺序声明的，因此大模型可以在任何时候用于生成文本或做出逻辑决策。

![https://cdn.nlark.com/yuque/0/2023/png/406504/1697251220165-883e1649-ac22-4b72-b679-d61ea42f5e05.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1697251220165-883e1649-ac22-4b72-b679-d61ea42f5e05.png)
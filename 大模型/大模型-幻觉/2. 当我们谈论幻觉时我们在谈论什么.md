![https://cdn.nlark.com/yuque/0/2023/png/406504/1697244361193-5cef34b0-919e-48a3-81c7-e168e0e3e349.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1697244361193-5cef34b0-919e-48a3-81c7-e168e0e3e349.png)

在大模型（LLM）的背景下，“幻觉”一词经常出现。根据《Survey of Hallucination in Natural Language Generation》论文的定义，大模型中的幻觉是指“生成无意义或不忠实于所提供来源的内容”。

“幻觉”的人工智能语境甚至已经被词典承认。例如，韦氏词典（[Merriam-Webster](https://www.merriam-webster.com/dictionary/hallucination)）在人工智能的背景下将其定义为“由人工智能算法生成的看似合理但错误或误导性的响应”。

有趣的是，这个术语并不是新鲜产生的。Andrej Karpathy表示，他可能在 2015 年发表的富有启发性的[博客文章](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)中普及了这个术语。值得注意的是，2014 年ACL 的一篇[会议论文](https://aclanthology.org/W14-1617.pdf)讨论了“幻觉”翻译。更早之前，2009 年一篇题为“[Review Sentiment Scoring via a Parse-and-Paraphrase Paradigm](https://aclanthology.org/D09-1017.pdf)”的论文在“幻觉”主题的背景下使用了该术语。但我发现的最古老的参考文献可能是 1996 年的一篇论文“[Text Databases and Information Retrieval”](https://dl.acm.org/doi/pdf/10.1145/234313.234371)，其中讨论了可以“幻觉”原始文档中不存在的单词的系统。

很明显，人工智能中的“幻觉”已经出现在词典中一段时间了，它开辟了一个与其心理学根源不同的小众含义。

还值得注意的是，人工智能充满了从人类类比中借用的术语——例如“神经网络”。尽管最初有所保留，但这些术语已经成为人工智能话语中不可或缺的、基本上没有争议的组成部分。

# 幻觉的类型

幻觉可分为两种主要类型：

- 内在幻觉：这些幻觉直接与原始材料相矛盾，导致事实不准确或逻辑不一致。
- 外在幻觉：这些并不矛盾，但也无法根据来源进行验证，添加了可能被视为推测或无法证实的元素。

“来源”的概念根据大模型正在执行的具体任务而有所不同。在基于对话的任务中，来源可以被视为“世界知识”。然而，当任务涉及文本摘要时，源就是输入文本本身。这是一个关键的细微差别，会影响幻觉的评估和解释。

当然，我们要记住一点的是，幻觉不一定是错的。幻觉的影响高度依赖于情境。例如，在写诗等创造性应用中，幻觉的存在不仅可以接受，而且可能会丰富输出。

# 为什么LLM会产生幻觉

大模型经过海量的预料进行了预训练，但是，大模型没有真/假或正确/不正确的概念，而是根据概率生成文本。虽然这会带来一些意想不到的推理能力（例如能够通过法律 BAR 考试或医学 USMLE），但这只是大模型靠这种概率推理的结果。甚至说，大多数大模型的指令微调和 RLHF 的额外训练确实引入了更多的“对事实的认知”和“人类认知的对齐”，但这些并没有改变大模型的底层机制。

大模型接受过整个互联网数据的训练。但是在训练集中正确的和错误的知识是并存的。大模型生成过程中偏向于他们最常看到的东西。如果你向大模型提出一个问题，这个问题在网上能够找到标准答案，那么你大概率得到的结果就是网络的标准答案，至于答案对错与否，不是大模型关心的问题。

在最近一篇题为“Sources of Hallucination by Large Language Models on Inference Tasks”的论文中，作者展示了大模型训练数据集的两个方面如何产生幻觉：先验真实性和启发式相对概率。

# 如何在大模型中测量幻觉

理解幻觉是一回事，但如何量化幻觉呢？这就是事情变得非常有趣的地方。

# 评估幻觉：通用指标和方法

在大模型中量化幻觉不仅仅是认识到它们的存在，而是如何量化的评估它们。

### 统计指标

ROUGE 和 BLEU 等指标通常是文本相似性评估的首选。他们通过将生成的输出与来源文本进行比较来关注幻觉的内在类型。当结构化知识源可用时，诸如 PARENT、PARENT-T 和 Knowledge F1 等高级指标就会发挥作用。然而，这些指标也有局限性：它们主要关注内在的幻觉，在捕捉句法和语义的细微差别时可能会出现问题。

### 基于模型的指标

基于模型的指标利用神经网络，使其更能适应句法和语义的复杂性：

基于 IE 的指标：这些指标使用信息提取 (IE) 模型将知识提炼成更简单的关系三元组格式：（subject，relation，object）。然后，模型根据从源或参考中提取的元组验证这些元组。

- 基于 QA 的指标：这些指标隐式地测量生成的内容和源之间的重叠或一致性。如果内容与来源事实一致，则相同的问题将会生成相似的答案。
- 基于 NLI 的指标：利用自然语言推理 (NLI) 数据集，这些指标确定生成的“假设”在给定“前提”的情况下是真、假还是不确定。
- 事实性分类指标：这些指标通过创建特定于任务的数据集来改进基于 NLI 的指标，从而提供更细致的评估。

### 人类评价

尽管存在自动化指标，但人工评估仍然不可或缺。通常采用两种主要方法：

1. 评分：人工在定义的范围内分配分数来评估幻觉的水平。
2. 比较：人工根据基线或真实参考评估生成的内容，对生成内容进行排序。

### FActScore

FActScore是最近的一个提出的评估指标，可用于人工评估和基于模型的评估。该指标将大模型生成结果分解为“atomic facts（原子事实）”。最终分数计算为每个原子事实的准确性之和，并给予每个原子事实相同的权重。准确性是一个二进制数，它简单地说明原子事实是否得到源的支持。作者用了不同的自动化策略，使用大模型来估计这个指标。

![https://cdn.nlark.com/yuque/0/2023/png/406504/1697244361386-565ffd3d-b24f-44c3-90c8-b4cbf2b80e73.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1697244361386-565ffd3d-b24f-44c3-90c8-b4cbf2b80e73.png)

# 红蓝对抗测试

虽然统计和基于模型的指标对于测量大模型的幻觉是必不可少的，但将这些模型置于严格的人类评估中也同样重要。红蓝队方式提供了一个重要的方法，可以补充系统性的衡量：

保持红队互补：尽管红队和压力测试是非常宝贵的工具，但它们不应取代系统测量。它们的目的是增强，而不是替代。

在现实条件下进行测试：只要有可能，就在生产端点上进行测试。这可以更真实地评估模型在实际条件下的行为。

定义危害和指南：清楚地概述潜在危害并向测试人员提供具体指南。这可以确保每个人都对测试期间要寻找的内容保持一致。

优先考虑你的重点领域：确定在红队练习中应优先考虑的关键特征、危害和场景。这种集中的方法会产生更多可操作的见解。

多元化且熟练的测试人员：具有不同专业领域的多元化测试人员可以提供多方面的评估。这里的多样性可能意味着不同的知识领域、不同的文化背景，甚至不同的偏见。

文档是关键：提前决定你希望测试人员记录哪些类型的数据或发现。清晰的文档有助于更加结构化的评估过程。

红队的新方法包括使用大模型来阅读另一个大模型。请参阅 Deepmind 的[“Red Teaming Language Models with Language Models”](https://arxiv.org/abs/2202.03286)

# 如何减少大模型中的幻觉

减少幻觉的道路充满挑战，但又是大模型进化道路上不得不面对的事情。

# 利用产品设计最大限度地减少影响

第一条建议很简单：如果可能的话，设计用例时不要出现幻觉。例如，在生成书面内容的应用程序中，关注观点文章而不是事实文章可能自然会降低出现有问题的幻觉的风险，因为事实是确定的，而观点是主观的。以内容创作大模型产品为例，可以设计相应的条款和手段，使得大模型的结果尽量可控：

- 用户可编辑性：允许用户编辑人工智能生成的输出。这不仅增加了额外的审查层，而且还提高了内容的整体可靠性。
- 用户责任：明确用户对生成和发布的内容负有最终责任。
- 引文和参考文献：启用包含引文的功能可以充当安全网，帮助用户在传播信息之前验证信息。
- 用户可选性：提供各种操作模式，例如使用更准确（但计算成本昂贵）模型的“精确”模式。
- 用户反馈：实施反馈机制，用户可以将生成的内容标记为不准确、有害或不完整。这些数据对于在未来迭代中完善模型非常宝贵。
- 限制输出和回合：注意生成的响应的长度和复杂性，因为更长、更复杂的输出产生幻觉的机会更高。
- 结构化输入/输出：考虑使用结构化字段而不是自由格式文本来降低产生幻觉的风险。例如，如果应用程序涉及简历生成，则预定义的教育背景、工作经验和技能字段可能会有所帮助。
- 维护跟踪集：应维护动态数据库来记录不同类型的幻觉以及重现幻觉所需的信息。这可以作为回归测试的强大工具。
- 隐私和信任：鉴于跟踪集可能包含敏感数据，请遵守数据隐私和安全的最佳实践。

# 提示工程：掌握提示设计的艺术

尽管大模型 (LLM) 已经取得了长足的进步，但它们还不够完美。这就是为什么理解并有效利用提示可以使大模型变得不同。一项研究表明，简单地指导大模型不该做什么就可以显着降低幻觉发生率。

# 抑制幻觉的一般准则

- 简化复杂的任务：将复杂的操作分解为更简单的步骤。
- 使用少样本学习：尽可能包含示例。
- 迭代细化：多次的探索优化提示。

需要注意的一件重要事情是，虽然这些技术能够带来一定的改变，但这些尝试也是有不低的成本的。

# 调整提示

- 突出重点：突出显示某些指令可以提高模型合规性。
- 上下文为王：提供更多的背景信息可以更好地为模型奠定基础。
- 细化步骤：重新评估初始输出并进行必要的调整。
- 内联引用：要求模型证实其主张。
- 框架：与问答相比，以总结的方式处理任务通常会产生更有根据的结果。
- 重申要点：在提示结束时重复要突出的内容可以强调其重要性。
- 回显输入：要求模型重述重要的输入细节，确保与源数据保持一致。
- 算法过滤：利用算法筛选最相关的信息并确定优先级。

在接下来的部分中，将剖析先进的提示技术，例如“思想链”方法，并深入研究如何利用检索增强生成（RAG）来获得更好的基础。

# 思想链

思维链最初是在谷歌研究人员的论文“[Chain-ofthought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)”中描述的。这里的简单想法是，鉴于 LLM 已经接受过预测标记而不是明确推理的训练，如果你指定那些所需的推理步骤，你可以让它们更接近推理。这是原始论文中的一个简单示例：

![https://cdn.nlark.com/yuque/0/2023/png/406504/1697244361223-387ae9a1-5b31-4075-8ae1-b9052da0711b.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1697244361223-387ae9a1-5b31-4075-8ae1-b9052da0711b.png)

请注意，在这种情况下，“所需的推理步骤”在示例中以蓝色给出。这就是所谓的“手动 CoT”。事实上有两种方法可以进行基本的思维链提示（见下文）。在称为零样本 CoT 的基本方法中，你只需要求大模型“一步一步思考”。在更复杂的版本中，称为“手动 CoT”，你必须逐步给出 LLM 思维示例，以说明如何推理。手动提示更有效，但更难扩展和维护。

![https://cdn.nlark.com/yuque/0/2023/png/406504/1697244361561-fb08ff36-9b8c-45af-8c84-b1ab5e2a1303.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1697244361561-fb08ff36-9b8c-45af-8c84-b1ab5e2a1303.png)

CoT 只是上述“简化复杂任务”一般建议的一种更加结构化的方法，并且众所周知可以在许多情况下减轻幻觉。

# RAG

检索增强生成，通常称为 RAG，是一种旨在增强大模型 (LLM) 功能的技术。RAG最初由 Facebook 于 2020 年在其 BART 模型背景下[提出](https://ai.meta.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/)，此后已作为一项功能纳入Hugging Face 库中。

### 核心理念

RAG 背后的基本思想很简单：它将检索组件与生成组件合并，使两者能够相互补充。下图直观地解释了这个过程，摘自原始研究论文。

![https://cdn.nlark.com/yuque/0/2023/png/406504/1697244361490-36175458-07b7-4dd1-bdcf-05d97979d5e7.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1697244361490-36175458-07b7-4dd1-bdcf-05d97979d5e7.png)

通过结合这两个元素，RAG 使大模型能够访问和合并外部信息，从而更有效地为生成的内容奠定基础。检索组件获取相关数据，而模型的生成方面将这些数据合成为连贯且上下文适当的响应。

RAG 已发展成为提示工程师工具包中不可或缺的一部分。随着时间的推移，它已经扩展到更复杂的应用程序，有效地充当更广泛的工具包框架中的具体示例，其中“工具”通常是简单的检索器或查询引擎。

由于 RAG将LLM 的响应基于外部数据，因此它被认为是减轻幻觉的一种非常有效的技术。不过，也有一些注意事项。

### RAG的不足

过度依赖的问题：使用 RAG 的一个显着缺点是明显过度依赖检索结果，在某些情况下，这可能会导致幻觉。谨记RAG的检索可能会产生空的、不正确的或需要进一步消歧的结果是至关重要的。以下是处理每种情况的策略。

空结果：为空结果好准备。当检索引擎返回空结果时，可能是由于文档源中缺乏相关数据或查询公式不正确。提示设计应能够预测和防范这种情况。如果检索引擎没有返回结果，系统应该选择谨慎并拒绝回答，并说明类似“抱歉，我们没有关于这个主题的足够信息。你能重新表述一下你的问题吗？” 更高级的策略可能涉及在内部重新制定查询以处理用户拼写错误等问题，这可能会导致无效结果。

不明确的结果：寻求澄清。对于诸如“银座有什么好餐厅？”等不明确的查询，其中波特兰可以指多个地点，建议向用户寻求进一步的澄清。例如，“你是指旅游路的银座，还是经十路的银座？”

错误结果：不正确的检索结果尤其难以解决，因为如果没有外部事实真相，就很难识别它们。虽然提高检索引擎的准确性是一个复杂的问题，但建议在应用程序的特定用例中分析检索解决方案的性能。在检索引擎被识别为不太准确的区域中设计你的提示时要格外小心。

# 先进的提示工程方法

在过去的几个月里，人们做出了大量努力来减轻幻觉问题和大语言模型（LLM）的基础。这些努力催生了各种有趣的方法，从提示工程角度解决了这个问题。

### 自我一致性（Self-consistency）

在论文[SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models](https://arxiv.org/abs/2303.08896)中提到，是一种使用 LLM 进行事实检查的方法。这个想法是一种简单的基于集成的方法，其中大模型被要求对同一提示生成多个响应。这些响应之间的一致性表明响应的准确性。

![https://cdn.nlark.com/yuque/0/2023/png/406504/1697244362584-2d80a503-69e6-4332-89b7-531ee4840e53.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1697244362584-2d80a503-69e6-4332-89b7-531ee4840e53.png)

上图说明了 QA 场景中的方法。在这种情况下，“一致性”是通过与总体答案一致的段落答案的数量来衡量的。然而，作者引入了另外两种一致性度量（BERT 分数和 n-gram），以及结合了这三种度量的第四种度量。

### 理性与行动（Reason and act【React】）

React 是 Google 在[“ReAct: Synergizing Reasoning and Acting in Language Models”](https://www.promptingguide.ai/techniques/react)中介绍的一种设计代理的具体方法。该方法促使LLM以交错的方式生成言语推理轨迹和动作，从而使模型能够执行动态推理。重要的是，作者发现 React 方法减少了 CoT 的幻觉。然而，这种基础性和可信度的增加，也是以推理步骤的灵活性略有降低为代价的（更多细节请参阅论文）。

![https://cdn.nlark.com/yuque/0/2023/png/406504/1697244363141-7a8e71f0-72d5-4ea3-b59a-ea505351da8d.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1697244363141-7a8e71f0-72d5-4ea3-b59a-ea505351da8d.png)

### 反射（Reflection）

在自我一致性方法中，我们看到了如何使用大模型来推断响应的置信度。在这种方法中，置信度是根据对同一问题的多个回答的相似程度来衡量的。如果更进一步，我们是否（或如何）可以直接向大模型询问其对自己回答的信心呢？

反射[论文](https://arxiv.org/abs/2303.11366)提出了一种定义为“通过言语反射强化”的方法，该方法具有不同的组成部分。行动者（actor）本身就是一名大模型，会产生一条轨迹（假设）。评估者（evaluator）会根据该假设的好坏程度给出分数。自反射组件（self reflection component）生成存储在内存中的摘要。迭代重复该过程，直到评估器确定它有“足够好”的答案。作者通过实验展示了反射如何大大提高检测幻觉的能力，即使与 ReAct 代理相比也是如此。

![https://cdn.nlark.com/yuque/0/2023/png/406504/1697244363357-62f49515-bee4-4583-9556-fb9e83dc702f.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1697244363357-62f49515-bee4-4583-9556-fb9e83dc702f.png)

### 对话解析剂 (DERA: Dialog-Enabled Resolving Agents)

[DERA](https://arxiv.org/abs/2303.17071)定义了不同的代理，这些代理在对话的背景下扮演不同的角色。与反射方法相比，区别在于，研究人员（Researchers）并行操作，而反射参与者仅在评估者决定时才按顺序操作。

![https://cdn.nlark.com/yuque/0/2023/png/406504/1697244363570-5d1b2851-48b3-4ea0-ac78-6583eeba25fa.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1697244363570-5d1b2851-48b3-4ea0-ac78-6583eeba25fa.png)

### 验证链 (COVE: Chain-of-Verification)

Meta 最近提出的COVE，通过使用 LLM 的不同实例来生成多个响应和自我验证的另一种变体。在他们的方法中，如下图所示，模型首先 (i) 起草初步回应；然后 (ii) 计划验证问题以对其草案进行事实核查；(iii) 独立回答这些问题，以便答案不会受到其他回答的影响；(iv) 生成最终经过验证的响应。

![https://cdn.nlark.com/yuque/0/2023/png/406504/1697244363975-264cbf08-c2fd-4d4a-8e24-1bdb58dcda37.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1697244363975-264cbf08-c2fd-4d4a-8e24-1bdb58dcda37.png)

### Guidance（受限提示）

“约束提示”是 Andrej Karpathy 最近引入的一个术语，用于描述允许我们在 LLM 流程中交错生成、提示和逻辑控制的方法和语言。

Guidance 是我所知道的这种方法的唯一例子，尽管有人可能会说 React 也是一种受限的提示方法。该工具与其说是一种提示方法，不如说是一种“提示语言”。使用指导模板，你几乎可以实现本文中的大多数（如果不是全部）方法。Guidance 使用基于Handlebars 的语法，允许交错提示和生成，以及管理逻辑控制流和变量。由于指导程序是按照它们将被执行的精确线性顺序声明的，因此大模型可以在任何时候用于生成文本或做出逻辑决策。

# 模型方面

### 模型温度

模型温度作为影响模型输出的随机行为的关键超参数。简而言之，它决定了预测后续标记时的随机性级别。较高的温度会增加不太可能出现的标记的选择概率，从而使模型的输出更加多样化，但可能不太可靠。相反，较低的温度（接近零）会导致模型更接近高概率的标记，通常会产生更可靠的输出。

### RLHF

RLHF 方法可以在训练的后期应用，以优化更准确、更可靠的输出。这些方法在缓解幻觉方面显示出显着的改进，特别是对于经过特定领域微调的模型。

### 微调

微调是解决模型性能的兜底方案。对自己的数据和示例进行微调可以使LLM的输出最大限度地减少幻觉，特别是如果你想使用更小、更高效的大模型。

# 结论

模型幻觉是大模型得到进一步推广落地必须面对的问题，尽管当前有大量的方法来尝试缓解幻觉问题，在一些准确性要求非常高的场景下，例如医疗、金融等领域，大模型还有很长的一段路要走。
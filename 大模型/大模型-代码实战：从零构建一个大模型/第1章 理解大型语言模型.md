# 第1章 理解大型语言模型

本章涵盖:

- 大型语言模型(LLMs)背后基本概念的高层次解释
- 对Transformer架构的深入理解,包括ChatGPT等LLMs的衍生模型
- 从头构建LLM的计划

大型语言模型(LLMs),如OpenAI的ChatGPT所使用的模型,是近几年开发的深度神经网络模型。它们为自然语言处理(NLP)领域带来了新的时代。在大型语言模型出现之前,传统方法在电子邮件垃圾分类等分类任务和可以用手工规则或简单模型捕捉的简单模式识别方面表现出色。然而,在需要复杂理解和生成能力的语言任务中,如解析详细指令、进行上下文分析或创建连贯且符合上下文的原创文本,它们通常表现不佳。例如,以前几代语言模型无法根据关键词列表写出电子邮件 - 这对当代LLMs来说是一项简单的任务。

LLMs具有卓越的理解、生成和解释人类语言的能力。然而,需要澄清的是,当我们说语言模型"理解"时,我们指的是它们能以看似连贯和符合上下文的方式处理和生成文本,而不是它们具有类人的意识或理解能力。

由于深度学习的进步(深度学习是机器学习和人工智能(AI)的一个子集,专注于神经网络),LLMs可以在海量文本数据上进行训练。这使LLMs能够比以前的方法捕获更深层次的上下文信息和人类语言的细微差别。因此,LLMs在广泛的NLP任务中显著提高了性能,包括文本翻译、情感分析、问答等。

当代LLMs和早期NLP模型之间的另一个重要区别是,早期NLP模型通常是为特定任务设计的,例如文本分类、语言翻译等。虽然那些早期的NLP模型在其狭窄的应用领域表现出色,但LLMs在广泛的NLP任务中展现出更广泛的能力。

LLMs成功的原因可以归因于Transformer架构(许多LLMs的基础)以及LLMs训练所使用的海量数据,使它们能够捕捉到各种语言细微差别、上下文和模式,这些都是难以手动编码的。

这种向基于Transformer架构的模型实施和使用大型训练数据集来训练LLMs的转变,从根本上改变了NLP,为理解和与人类语言交互提供了更强大的工具。

从本章开始,我们将为实现本书的主要目标奠定基础:通过逐步用代码实现基于Transformer架构的类ChatGPT LLM来理解LLMs。

## 1.1 什么是LLM?

LLM(Large Language Model,大型语言模型)是一种设计用于理解、生成和响应类人文本的神经网络。这些模型是在海量文本数据上训练的深度神经网络,有时甚至包括互联网上公开可用的大部分文本。

"大型"语言模型中的"大型"既指模型在参数数量上的规模,也指其训练所用的庞大数据集。这类模型通常有数百亿甚至数千亿个参数,这些参数是网络中可调整的权重,在训练过程中被优化以预测序列中的下一个单词。下一个单词预测之所以合理,是因为它利用了语言固有的顺序性来训练模型理解上下文、结构和文本中的关系。然而,这是一个非常简单的任务,所以令许多研究人员感到惊讶的是,它竟然能产生如此强大的模型。我们将在后面的章节中逐步讨论和实现下一个单词的训练过程。

LLMs使用一种称为Transformer的架构(在1.4节中有更详细的介绍),它允许模型在进行预测时有选择地关注输入的不同部分,使它们特别擅长处理人类语言的细微差别和复杂性。

由于LLMs能够生成文本,LLMs也经常被称为一种生成式人工智能(AI),通常简称为生成式AI或GenAI。如图1.1所示,AI包括创建能执行需要人类智能的任务的机器的更广泛领域,包括理解语言、识别模式和做出决策,并包括机器学习和深度学习等子领域。

![](Images/大模型-从零构建一个大模型/第一章/Untitled.png)

图1.1 这个层次图描述了不同领域之间的关系,表明LLMs代表了深度学习技术的特定应用,利用其处理和生成类人文本的能力。深度学习是机器学习的一个专门分支,专注于使用多层神经网络。而机器学习和深度学习是旨在实现使计算机能够从数据中学习并执行通常需要人类智能的任务的算法的领域。

用于实现AI的算法是机器学习领域的重点。具体来说,机器学习涉及开发可以从数据中学习并基于数据进行预测或决策的算法,而无需显式编程。为了说明这一点,让我们想象一个垃圾邮件过滤器作为机器学习的实际应用。不是手动编写规则来识别垃圾邮件,而是向机器学习算法提供标记为垃圾邮件和合法邮件的例子。通过最小化其在训练数据集上的预测错误,模型然后学会识别垃圾邮件的特征和模式,从而能够将新的邮件分类为垃圾邮件或合法邮件。



如图1.1所示,深度学习是机器学习的一个子集,专注于利用具有三层或更多层(也称为深度神经网络)的神经网络来模拟数据中的复杂模式和抽象。与深度学习相比,传统机器学习需要手动特征提取。这意味着人类专家需要为模型识别和选择最相关的特征。

虽然AI领域现在主要由机器学习和深度学习主导,但它也包括其他方法,例如使用基于规则的系统、遗传算法、专家系统、模糊逻辑或符号推理。

回到垃圾邮件分类的例子,在传统机器学习中,人类专家可能会从电子邮件文本中手动提取特征,如某些触发词("奖品"、"赢"、"免费")的频率、感叹号的数量、全大写单词的使用或可疑链接的存在。然后,基于这些专家定义的特征创建的数据集将用于训练模型。与传统机器学习相比,深度学习不需要手动特征提取。这意味着人类专家不需要为深度学习模型识别和选择最相关的特征。(然而,在传统机器学习和深度学习的垃圾邮件分类中,您仍然需要收集标签,如垃圾邮件或非垃圾邮件,这些标签需要由专家或用户收集。)

接下来的部分将介绍LLMs今天可以解决的一些问题、LLMs解决的挑战以及我们将在本书中实现的通用LLM架构。

## 1.2 LLMs的应用

由于其先进的解析和理解非结构化文本数据的能力,LLMs在各个领域都有广泛的应用。今天,LLMs被用于机器翻译、生成新颖文本(见图1.2)、情感分析、文本摘要等许多任务。LLMs最近被用于内容创作,如写小说、文章,甚至计算机代码。

![](Images/大模型-从零构建一个大模型/第一章/Untitled1.png)

图1.2 LLM界面使用户和AI系统之间能够进行自然语言交流。这个截图显示了ChatGPT根据用户的规格写诗。

LLMs还可以为复杂的聊天机器人和虚拟助手提供动力,如OpenAI的ChatGPT或Google的Gemini(以前称为Bard),它们可以回答用户查询并增强传统搜索引擎如Google搜索或Microsoft Bing。

此外,LLMs可用于从医学或法律等专业领域的大量文本中进行有效的知识检索。这包括筛选文档、总结长篇段落和回答技术问题。

简而言之,LLMs对于自动化几乎任何涉及解析和生成文本的任务都是无价的。它们的应用几乎是无穷无尽的,随着我们继续创新和探索使用这些模型的新方法,很明显LLMs有潜力重新定义我们与技术的关系,使其更具对话性、直观性和可访问性。

在本书中,我们将专注于从头开始理解LLMs如何工作,编写一个可以生成文本的LLM。我们还将学习允许LLMs执行查询的技术,从回答问题到总结文本、将文本翻译成不同语言等。换句话说,在本书中,我们将通过逐步构建来学习像ChatGPT这样的复杂LLM助手是如何工作的。

## 1.3 构建和使用LLMs的阶段

为什么我们应该构建自己的LLMs?从头开始编码一个LLM是理解其机制和限制的绝佳练习。此外,它还为我们提供了预训练或微调现有开源LLM架构以适应我们自己的特定领域数据集或任务所需的知识。

研究表明,当涉及到建模性能时,定制构建的LLMs - 那些为特定任务或领域量身定制的LLMs - 可以胜过通用LLMs,如ChatGPT提供的那些为广泛应用设计的LLMs。这方面的例子包括专门用于金融的BloombergGPT,以及为医学问答量身定制的LLMs(更多详细信息请参见附录B中的进一步阅读和参考部分)。

使用定制构建的LLMs提供了几个优势,特别是在数据隐私方面。例如,由于保密问题,公司可能更倾向于不与OpenAI等第三方LLM提供商共享敏感数据。此外,开发定制LLMs使得可以直接在客户设备上部署,如笔记本电脑和智能手机,这是苹果等公司目前正在探索的。这种本地实施可以显著降低延迟并减少服务器相关成本。此外,定制LLMs给予开发人员完全的自主权,允许他们根据需要控制模型的更新和修改。

创建LLM的一般过程包括预训练和微调。"预训练"中的"预"指的是初始阶段,在这个阶段,像LLM这样的模型在一个大型、多样化的数据集上进行训练,以获得对语言的广泛理解。然后,这个预训练模型作为一个基础资源,可以通过微调进一步细化,微调是一个过程,其中模型在更窄的、更特定于特定任务或领域的数据集上进行专门训练。这种由预训练和微调组成的两阶段训练方法如图1.3所示。

![](Images/大模型-从零构建一个大模型/第一章/Untitled2.png)

图1.3 预训练LLM涉及在大型文本数据集上进行下一个单词预测。然后,预训练的LLM可以使用较小的标记数据集进行微调。

如图1.3所示,创建LLM的第一步是在大量文本数据语料库上训练它,有时称为原始文本。这里的"原始"指的是这些数据只是普通文本,没有任何标记信息[1]。(可能会应用过滤,如删除格式字符或未知语言的文档。)

LLM的这第一个训练阶段也被称为预训练,创建一个初始的预训练LLM,通常称为基础或基础模型。这种模型的一个典型例子是GPT-3模型(ChatGPT原始模型的前身)。这个模型能够进行文本补全，即完成用户提供的半写句子。它还具有有限的少样本学习能力，这意味着它可以基于只有几个例子来学习执行新任务，而不需要大量的训练数据。这一点将在下一节"介绍Transformer架构"中进一步说明。

在通过对大型文本数据集进行训练获得预训练LLM后，我们可以进一步在标记数据上训练LLM，这也称为微调。

微调LLMs最流行的两个类别包括指令微调和分类任务微调。在指令微调中，标记数据集由指令和答案对组成，例如翻译文本的查询伴随着正确翻译的文本。在分类微调中，标记数据集由文本及其相关的类别标签组成，例如与垃圾邮件和非垃圾邮件标签相关联的电子邮件。

在本书中，我们将涵盖预训练和微调LLM的代码实现，并且在预训练基础LLM之后，我们将在本书后面深入探讨指令微调和分类微调的具体细节。

## 1.4 介绍Transformer架构

大多数现代LLMs都依赖于Transformer架构，这是一种在2017年的论文《Attention Is All You Need》中引入的深度神经网络架构。为了理解LLMs，我们简要地回顾一下原始的Transformer，它最初是为机器翻译开发的，将英语文本翻译成德语和法语。图1.4展示了Transformer架构的简化版本。

![](Images/大模型-从零构建一个大模型/第一章/Untitled3.png)

图1.4 原始Transformer架构的简化描述，这是一个用于语言翻译的深度学习模型。Transformer由两部分组成，一个编码器处理输入文本并产生文本的嵌入表示（一个捕获不同维度中许多不同因素的数值表示），解码器可以使用这个表示一次生成一个单词的翻译文本。注意，这个图显示了翻译过程的最后阶段，其中解码器只需要生成最后一个单词（"Beispiel"），给定原始输入文本（"This is an example"）和部分翻译的句子（"Das ist ein"），以完成翻译。

图1.4中描绘的Transformer架构由两个子模块组成，一个编码器和一个解码器。编码器模块处理输入文本并将其编码为一系列数值表示或向量，这些向量捕获了输入的上下文信息。然后，解码器模块接受这些编码向量并从中生成输出文本。在翻译任务中，例如，编码器会将源语言的文本编码成向量，解码器则会将这些向量解码生成目标语言的文本。编码器和解码器都由许多层组成，这些层通过所谓的自注意力机制连接。您可能对输入如何预处理和编码有许多问题。这些将在后续章节中通过逐步实现来解决。

Transformers和LLMs的一个关键组件是自注意力机制（图中未显示），它允许模型相对于彼此权衡序列中不同单词或标记的重要性。这种机制使模型能够捕获输入数据中的长程依赖关系和上下文关系，增强了其生成连贯和上下文相关输出的能力。然而，由于其复杂性，我们将推迟解释到第3章，我们将在那里逐步讨论和实现它。此外，我们还将在第2章"处理文本数据"中讨论和实现创建模型输入的数据预处理步骤。

Transformer架构的后续变体，如所谓的BERT（双向编码器表示from Transformers的缩写）和各种GPT模型（生成式预训练Transformers的缩写），在这个概念的基础上进行了改进，以适应不同的任务。（参考文献可在附录B中找到。）

BERT，建立在原始Transformer的编码器子模块之上，在训练方法上与GPT不同。虽然GPT是为生成任务设计的，但BERT及其变体专门用于掩码词预测，模型预测给定句子中被掩码或隐藏的词，如图1.5所示。这种独特的训练策略使BERT在文本分类任务中具有优势，包括情感预测和文档分类。作为其能力的一个应用，在撰写本文时，Twitter使用BERT来检测有害内容。

![](Images/大模型-从零构建一个大模型/第一章/Untitled4.png)

图1.5 Transformer的编码器和解码器子模块的可视化表示。左侧的编码器部分展示了类似BERT的LLMs，它们专注于掩码词预测，主要用于文本分类等任务。右侧的解码器部分展示了类似GPT的LLMs，设计用于生成任务并产生连贯的文本序列。

另一方面，GPT专注于原始Transformer架构的解码器部分，设计用于需要生成文本的任务。这包括机器翻译、文本摘要、小说写作、编写计算机代码等。我们将在本章的剩余部分更详细地讨论GPT架构，并在本书中从头开始实现它。

GPT模型主要设计和训练用于执行文本补全任务，但在其能力方面也显示出显著的多功能性。这些模型擅长执行零样本和少样本学习任务。零样本学习指的是在没有任何特定例子的情况下推广到完全未见过的任务的能力。另一方面，少样本学习涉及从用户作为输入提供的最小数量的例子中学习，如图1.6所示。

![](Images/大模型-从零构建一个大模型/第一章/Untitled5.png)

图1.6 除了文本补全外，类似GPT的LLMs可以基于其输入解决各种任务，而无需重新训练、微调或任务特定的模型架构更改。有时，在输入中提供目标的示例是有帮助的，这被称为少样本设置。然而，类似GPT的LLMs也能够在没有具体示例的情况下执行任务，这被称为零样本设置。

TRANSFORMERS VS LLMS
今天的LLMs基于前面介绍的Transformer架构。因此，Transformers和LLMs在文献中经常被同义使用。然而，请注意，并非所有的Transformers都是LLMs，因为Transformers也可以用于计算机视觉。同样，并非所有的LLMs都是Transformers，因为还有基于循环和卷积架构的大型语言模型。这些替代方法的主要动机是改善LLMs的计算效率。然而，这些替代LLM架构是否能与基于Transformer的LLMs的能力相竞争，以及它们是否会在实践中被采用，仍有待观察。为简单起见，本书使用术语"LLM"来指代类似GPT的基于Transformer的LLMs。（感兴趣的读者可以在本章末尾的进一步阅读部分找到描述这些架构的文献参考。）

## 1.5 利用大型数据集

流行的GPT和BERT类模型的大型训练数据集代表了多样化和全面的文本语料库，包含数十亿个单词，涵盖了广泛的主题以及自然语言和计算机语言。为了提供一个具体的例子，表1.1总结了用于预训练GPT-3的数据集，GPT-3是ChatGPT第一个版本的基础模型。

| 数据集名称 | 数据集描述 | 标记数量 | 训练数据中的比例 |
| --- | --- | --- | --- |
| CommonCrawl (经过过滤) | 网络爬虫数据 | 4100亿 | 60% |
| WebText2 | 网络爬虫数据 | 190亿 | 22% |
| Books1 | 基于互联网的图书语料库 | 120亿 | 8% |
| Books2 | 基于互联网的图书语料库 | 550亿 | 8% |
| Wikipedia | 高质量文本 | 30亿 | 3% |

表1.1 流行的GPT-3 LLM的预训练数据集

表1.1报告了标记的数量，其中标记是模型读取的文本单位，数据集中的标记数量大致等同于文本中的单词和标点符号的数量。我们将在下一章更详细地介绍标记化，即将文本转换为标记的过程。

主要的结论是，这种训练数据集的规模和多样性使得这些模型能够在各种任务中表现良好，包括语言语法、语义和上下文，甚至一些需要一般知识的任务。

GPT-3 数据集细节
表1.1显示了用于GPT-3的数据集。表中的比例列加起来为100%的采样数据，经过四舍五入调整。尽管"标记数量"列中的子集总计为5090亿，但模型仅在3000亿标记上进行了训练。GPT-3论文的作者没有具体说明为什么不在所有5090亿标记上训练模型。

为了提供上下文，考虑CommonCrawl数据集的大小，仅这个数据集就包含4100亿个标记，需要约570 GB的存储空间。相比之下，GPT-3之后的模型迭代，如Meta的LLaMA，已经扩大了其训练范围，包括额外的数据源，如Arxiv研究论文（92 GB）和StackExchange的代码相关问答（78 GB）。

GPT-3论文的作者没有共享训练数据集，但有一个可比较的公开可用数据集是Soldaini等人2024年的"Dolma: an Open Corpus of Three Trillion Tokens for LLM Pretraining Research"（[https://arxiv.org/abs/2402.00159](https://arxiv.org/abs/2402.00159%EF%BC%89%E3%80%82%E7%84%B6%E8%80%8C%EF%BC%8C%E8%AF%A5%E9%9B%86%E5%90%88%E5%8F%AF%E8%83%BD%E5%8C%85%E5%90%AB%E7%89%88%E6%9D%83%E4%BD%9C%E5%93%81%EF%BC%8C%E7%A1%AE%E5%88%87%E7%9A%84%E4%BD%BF%E7%94%A8%E6%9D%A1%E6%AC%BE%E5%8F%AF%E8%83%BD%E5%8F%96%E5%86%B3%E4%BA%8E%E9%A2%84%E6%9C%9F%E7%9A%84%E7%94%A8%E4%BE%8B%E5%92%8C%E5%9B%BD%E5%AE%B6%E3%80%82)）。然而，该集合可能包含版权作品，确切的使用条款可能取决于预期的用例和国家。

这些模型的预训练性质使它们在下游任务的进一步微调方面非常versatile，这就是为什么它们也被称为基础或基础模型。预训练LLMs需要访问大量资源，并且非常昂贵。例如，GPT-3的预训练成本估计为460万美元的云计算信用。

好消息是，许多可作为开源模型使用的预训练LLMs可以作为通用工具来写作、提取和编辑不属于训练数据的文本。此外，LLMs可以在相对较小的数据集上针对特定任务进行微调，减少所需的计算资源并提高特定任务的性能。

在本书中，我们将实现预训练的代码，并将其用于出于教育目的预训练LLM。所有计算都将可在消费级硬件上执行。在实现预训练代码之后，我们将学习如何重用公开可用的模型权重并将其加载到我们将实现的架构中，这将允许我们在本书后面微调LLMs时跳过昂贵的预训练阶段。

## 1.6 深入了解GPT架构

在本章前面，我们提到了GPT类模型、GPT-3和ChatGPT这些术语。现在让我们更仔细地看看通用GPT架构。首先，GPT代表生成式预训练Transformer，最初是在以下论文中引入的：

>《通过生成式预训练改进语言理解》（2018）由OpenAI的Radford等人撰写，[http://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf](http://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)

GPT-3是这个模型的放大版本，具有更多的参数，并在更大的数据集上进行了训练。而ChatGPT中最初提供的模型是通过在大型指令数据集上微调GPT-3创建的，使用了OpenAI的InstructGPT论文中的方法，我们将在第7章"通过人类反馈微调以遵循指令"中更详细地介绍。正如我们在前面的图1.6中看到的，这些模型是很有能力的文本补全模型，并且可以执行其他任务，如拼写纠正、分类或语言翻译。考虑到GPT模型是在相对简单的下一个词预测任务上预训练的，这实际上是非常令人惊讶的，如图1.7所示。

![](Images/大模型-从零构建一个大模型/第一章/Untitled6.png)

图1.7 在GPT模型的下一个词预训练任务中，系统通过查看之前出现的词来学习预测句子中即将出现的词。这种方法帮助模型理解词语和短语在语言中通常如何组合在一起，形成可以应用于各种其他任务的基础。

下一个词预测任务是一种自监督学习形式，这是一种自标记方式。这意味着我们不需要明确地为训练数据收集标签，而可以利用数据本身的结构：我们可以使用句子或文档中的下一个词作为模型应该预测的标签。由于这种下一个词预测任务允许我们"即时"创建标签，因此可以利用大规模的未标记文本数据集来训练LLMs，正如我们在1.5节"利用大型数据集"中之前讨论的那样。

与我们在1.4节中介绍的原始Transformer架构相比，通用GPT架构相对简单。本质上，它只是没有编码器的解码器部分，如图1.8所示。由于像GPT这样的解码器风格模型一次生成一个词来生成文本，它们被认为是一种自回归模型。自回归模型将其先前的输出作为未来预测的输入。因此，在GPT中，每个新词的选择都基于前面的序列，这提高了生成文本的连贯性。

像GPT-3这样的架构也显著大于原始的Transformer模型。例如，原始Transformer重复编码器和解码器块六次。GPT-3有96个Transformer层，总共1750亿个参数。

![](Images/大模型-从零构建一个大模型/第一章/Untitled7.png)

图1.8 GPT架构仅使用原始Transformer的解码器部分。它设计用于单向、从左到右的处理，非常适合文本生成和下一个词预测任务，以迭代方式一次生成一个词的文本。

GPT-3于2020年推出，按照深度学习和大型语言模型（LLM）发展的标准来看，这被认为是很久以前的事了。然而，更新的架构，如Meta的Llama模型，仍然基于相同的基本概念，只引入了一些小的修改。因此，理解GPT仍然与以往一样相关，本书专注于实现GPT背后的突出架构，同时提供指向替代LLMs采用的特定调整的指针。

最后，有趣的是，尽管原始的Transformer模型（由编码器和解码器块组成）明确设计用于语言翻译，但GPT模型——尽管其架构更大但更简单，仅包含解码器，目标是下一个词预测——也能够执行翻译任务。这种能力最初让研究人员感到意外，因为它来自于主要针对下一个词预测任务训练的模型，这是一个并不特别针对翻译的任务。

模型执行未经明确训练的任务的能力被称为"涌现行为"。这种能力在训练过程中并未明确教授，而是作为模型接触大量多语言数据在不同上下文中的自然结果而出现的。GPT模型可以"学习"语言之间的翻译模式并执行翻译任务，尽管它们并未专门为此训练，这展示了这些大规模、生成式语言模型的优势和能力。我们可以使用单一模型执行多样化的任务，而不需要为每个任务使用不同的模型。

## 1.7 构建大型语言模型

在本章中，我们为理解LLMs奠定了基础。在本书的其余部分，我们将从头开始编写一个。我们将以GPT背后的基本思想为蓝图，并分三个阶段进行，如图1.9所示。

![](Images/大模型-从零构建一个大模型/第一章/Untitled8.png)

图1.9 本书涵盖的构建LLMs的阶段包括实现LLM架构和数据准备过程、预训练LLM以创建基础模型，以及微调基础模型成为个人助手或文本分类器。

首先，我们将学习基本的数据预处理步骤，并编写每个LLM核心的注意力机制。

接下来，在第2阶段，我们将学习如何编码和预训练一个能够生成新文本的GPT类LLM。我们还将介绍评估LLMs的基础知识，这对于开发能力强的NLP系统至关重要。

请注意，从头开始预训练LLM是一项重大工作，对于GPT类模型来说，需要数千到数百万美元的计算成本。因此，第2阶段的重点是出于教育目的使用小型数据集实现训练。此外，本书还将提供加载公开可用模型权重的代码示例。

最后，在第3阶段，我们将采用一个预训练的LLM，并对其进行微调以遵循指令，如回答查询或分类文本——这是许多现实世界应用和研究中最常见的任务。

我希望您期待开始这个令人兴奋的旅程！

## 1.8 总结

- LLMs彻底改变了自然语言处理领域，该领域之前主要依赖于显式的基于规则的系统和更简单的统计方法。LLMs的出现引入了新的深度学习驱动方法，导致了理解、生成和翻译人类语言方面的进步。
- 现代LLMs主要通过两个主要步骤进行训练。
    1. 首先，它们在大型未标记文本语料库上进行预训练，使用句子中下一个词的预测作为"标签"。
    2. 然后，它们在较小的标记目标数据集上进行微调，以遵循指令或执行分类任务。
- LLMs基于Transformer架构。Transformer架构的关键思想是注意力机制，它使LLM能够在一次生成一个词的输出时选择性地访问整个输入序列。
- 原始Transformer架构由一个用于解析文本的编码器和一个用于生成文本的解码器组成。
- 用于生成文本和遵循指令的LLMs，如GPT-3和ChatGPT，只实现解码器模块，简化了架构。
- 由数十亿单词组成的大型数据集对于预训练LLMs至关重要。在本书中，我们将出于教育目的在小型数据集上实现和训练LLMs，但也会看到如何加载公开可用的模型权重。
- 虽然GPT类模型的一般预训练任务是预测句子中的下一个词，但这些LLMs表现出"涌现"属性，如分类、翻译或总结文本的能力。
- 一旦LLM经过预训练，得到的基础模型可以更有效地针对各种下游任务进行微调。
- 在自定义数据集上微调的LLMs可能在特定任务上优于通用LLMs。

[1] 具有机器学习背景的读者可能会注意到，标记信息通常是传统机器学习模型和通过常规监督学习范式训练的深度神经网络所需要的。然而，这并不适用于LLMs的预训练阶段。在这个阶段，LLMs利用自监督学习，模型从输入数据中生成自己的标签。这个概念在本章后面有所涉及。

[2] 来源：[https://lambdalabs.com/blog/demystifying-gpt-3](https://lambdalabs.com/blog/demystifying-gpt-3)
# 第3章 编码注意力机制

在本章中,我们将深入探讨神经网络中注意力机制的实现。注意力机制是现代大语言模型(LLM)的核心组成部分,理解它的工作原理对于掌握LLM至关重要。

本章主要内容包括:

1. 探讨在神经网络中使用注意力机制的原因
2. 介绍基本的自注意力框架,并逐步发展到增强型自注意力机制
3. 实现因果注意力模块,使LLM能够逐个生成token
4. 使用dropout随机屏蔽部分注意力权重以减少过拟合
5. 将多个因果注意力模块堆叠成多头注意力模块

让我们先回顾一下上一章的内容。在第2章中,我们学习了如何准备输入文本用于训练LLM。这涉及将文本分割成单词和子词token,然后将它们编码成向量表示,即所谓的嵌入(embeddings)。

在本章中,我们将聚焦于LLM架构本身的一个核心部分 - 注意力机制,如图3.1所示。

![Untitled](Images/大模型-从零构建一个大模型/第三章/Untitled.png)

图3.1 LLM开发的三个主要阶段:编码LLM、在通用文本数据集上预训练LLM,以及在标记数据集上微调。本章重点关注注意力机制,这是LLM架构的核心部分。

注意力机制是一个综合性的话题,这就是为什么我们要用整整一章来讨论它。在本章中,我们将主要从机制层面来考察这些注意力机制。在下一章,我们将编码LLM中围绕自注意力机制的其余部分,以便看到它的实际运作并创建一个可以生成文本的模型。

在本章中,我们将实现四种不同的注意力机制变体,如图3.2所示。

![Untitled](Images/大模型-从零构建一个大模型/第三章/Untitled1.png)

图3.2 本章将编码的不同注意力机制,从简化版自注意力开始,然后添加可训练权重。因果注意力机制在自注意力的基础上添加了掩码,使LLM能够一次生成一个词。最后,多头注意力将注意力机制组织成多个头,允许模型并行捕获输入数据的各个方面。

图3.2中展示的不同注意力机制变体彼此构建,最终形成一个完整而高效的多头注意力实现。这将是我们在本章结束时所实现的,也是我们在下一章中用于构建完整的LLM架构的基础。

## 3.1 建模长序列的问题

在我们深入研究本章稍后将介绍的自注意力机制之前,让我们先思考一个问题:为什么没有注意力机制的架构在处理LLM方面表现不佳?假设我们想要开发一个语言翻译模型,将文本从一种语言翻译成另一种语言。如图3.3所示,我们不能简单地将文本逐词翻译,因为源语言和目标语言的语法结构可能完全不同。

![Untitled](Images/大模型-从零构建一个大模型/第三章/Untitled2.png)

图3.3 将文本从一种语言翻译成另一种语言(如从德语翻译成英语)时,不可能仅仅逐字翻译。相反,翻译过程需要理解上下文并调整语法结构。

为了解决无法逐词翻译的问题,通常的做法是使用一种称为编码器-解码器的神经网络架构。编码器的思想是首先处理整个输入文本,解码器然后生成翻译后的文本。

我们在第1章(1.4节,针对不同任务定制LLM)中简要讨论了编码器-解码器架构。在Transformer出现之前,递归神经网络(RNN)是最流行的编码器-解码器架构用于语言翻译。

RNN是一种神经网络,其输出从前一步作为当前步骤的输入,使其非常适合处理顺序数据如文本。如果你不熟悉RNN,别担心,你不需要了解RNN的详细工作原理就能理解我们的讨论;关键是要理解编码器-解码器设置的一般概念。

在编码器-解码器RNN中,输入文本首先通过编码器,编码器按顺序处理它。编码器更新其隐藏状态(神经网络的内部层)以每个词,试图在最后的隐藏状态中捕获整个输入句子的含义,如图3.4所示。然后解码器使用这个最终隐藏状态开始生成翻译后的句子,一次一个词。它还更新自己的隐藏状态以每个词,这应该捕获生成下一个词所需的上下文。

![Untitled](Images/大模型-从零构建一个大模型/第三章/Untitled3.png)

图3.4 在Transformer模型出现之前,编码器-解码器RNN是机器翻译的流行选择。编码器接收源语言的token序列作为输入,其中编码器的隐藏状态(中间神经网络层)编码整个输入序列的压缩表示。然后,解码器使用其当前隐藏状态开始翻译,逐个生成token。

虽然我们不需要了解这些编码器-解码器RNN的内部工作原理,但关键信息是编码器试图将整个输入文本压缩到一个隐藏状态(记忆细胞)中。解码器然后从这个隐藏状态开始产生输出。你可以将这个隐藏状态看作是一个嵌入向量,这是我们在第2章讨论过的概念。

编码器-解码器RNN的主要限制是RNN无法直接访问编码器在解码阶段的早期隐藏状态。因此,它严重依赖于当前隐藏状态,这可能导致相关信息的丢失。这可能导致上下文的丢失,尤其是在复杂的句子中,依赖关系可能跨越很长的距离。

对于早期使用RNN的读者来说,理解或研究这种架构并不重要,因为我们不会在代码中使用它。但这一部分的关键信息是编码器-解码器RNN有一个瓶颈,这促使了注意力机制的设计。

## 3.2 用注意力机制捕捉数据依赖关系

在Transformer LLM之前,使用RNN进行语言建模任务(如语言翻译)是很常见的,如前所述。RNN在处理短句子时表现良好,但对于较长的文本效果不佳,因为它们无法直接访问输入中的先前单词。

这种方法的主要缺点是RNN必须在将其传递给解码器之前,将整个编码输入记忆在单个隐藏状态中,如前一节图3.4所示。

然而,研究人员在2014年为RNN开发了所谓的Bahdanau注意力机制(以该论文的第一作者命名),它修改了编码器-解码器RNN,使得解码器可以选择性地访问输入序列的不同部分,如图3.5所示。

![Untitled](Images/大模型-从零构建一个大模型/第三章/Untitled4.png)

图3.5 使用注意力机制,网络的文本生成解码器部分可以选择性地访问所有输入token。这意味着某些输入token对于生成给定的输出token比其他token更重要。重要性由所谓的注意力权重决定,我们稍后将计算这些权重。请注意,该图显示了注意力背后的一般思想,并不描述Bahdanau机制的确切实现,Bahdanau机制是一种RNN方法,超出了本书的范围。

有趣的是,几年后,研究人员发现RNN架构对于构建良好的神经网络用于自然语言处理并不是必需的,并提出了原始的Transformer架构(在第1章中讨论),其中包含一个受Bahdanau注意力机制启发的自注意力机制。

自注意力是一种机制,允许输入序列中的每个位置关注该序列中的所有位置来计算序列的表示。自注意力是基于Transformer架构的LLM(如GPT系列)的关键组成部分。

本章专注于编码和理解GPT类模型中使用的这种自注意力机制,如图3.6所示。在下一章中,我们将编码LLM的其余部分。

![Untitled](Images/大模型-从零构建一个大模型/第三章/Untitled5.png)

图3.6 自注意力是Transformers中用于计算更高效的输入表示的机制,它允许序列中的每个位置与同一序列内的所有其他位置进行交互并权衡其重要性。在本章中,我们将从头开始编码这个自注意力机制,然后在下一章编码GPT类LLM的其余部分。

## 3.3 用自注意力关注输入的不同部分

让我们现在深入研究自注意力机制的内部工作原理,并学习如何从头开始实现它。自注意力作为每个LLM基于Transformer架构的核心组件。值得注意的是,这个主题确实需要很多专注和注意力(无意双关),但一旦你掌握了它的基本原理,你就会发现它是实现LLM最有趣的部分之一。

### "SELF"在自注意力中的含义

在自注意力中,"self"指的是机制能够通过关联单个输入序列内的不同位置来计算注意力权重。它评估输入本身的各个部分之间的关系和依赖性,如句子中的单词或图像中的像素。这与传统的注意力机制形成对比,后者关注的是两个不同序列之素的关系,例如在序列到序列模型中注意力可能在输入序列和输出序列之间,就像图3.5中描绘的例子。

由于自注意力可能看起来复杂,特别是如果你第一次遇到它,我们将从介绍一个简化版本的自注意力开始。之后,在3.4节中,我们将实现带有可训练权重的自注意力机制,这是LLM中使用的版本。

### 3.3.1 没有可训练权重的简单自注意力机制

在本节中,我们将实现一个简化版的自注意力,不包含可训练权重,这在图3.7中进行了总结。本节的目的是说明自注意力中的一些关键概念,然后在3.4节中添加可训练权重。

![Untitled](Images/大模型-从零构建一个大模型/第三章/Untitled6.png)

图3.7 自注意力的目标是为每个输入元素计算一个上下文向量,该向量结合了所有其他输入元素的信息。在这个图中描述的例子中,我们计算上下文向量z(2)。每个输入元素对计算z(2)的重要性或贡献由注意力权重α21到α2T决定。在计算z(2)时,注意力权重是相对于输入元素x(2)和所有其他输入计算的。这些注意力权重的具体计算将在本节后面讨论。

图3.7显示了一个输入序列,表示为x,由T个元素组成,从x(1)到x(T)。这个序列通常代表文本,例如一个句子,已经被转换成token嵌入,如第2章所解释的。

例如,考虑输入词序列"This journey starts with one step."在这种情况下,序列中的每个元素,如x(1),对应于一个3维嵌入向量,代表一个特定的token,比如"This"。在图3.7中,这些输入向量显示为3维嵌入。

在自注意力中,目标是为输入序列中的每个元素x(j)计算上下文向量z(j)。上下文向量可以被解释为一个增强的嵌入向量。

为了说明这个过程,让我们关注第二个输入元素的嵌入向量,x(2)(对应于token "journey"),和相应的上下文向量,z(2),如图3.7底部所示。这个增强的上下文向量,z(2),是一个嵌入,包含关于x(2)和所有其他输入元素x(1)到x(T)的信息。

在自注意力中,上下文向量起着至关重要的作用。它们的目的是为输入序列(如一个句子)中的每个元素创建增强的表示,通过整合来自序列中所有其他元素的信息,如图3.7所示。这在LLM中是必要的,LLM需要理解单词在句子中与其他单词的关系和相关性。稍后,我们将添加可训练权重,这些权重将帮助LLM学习构造这些上下文向量,使它们对LLM生成下一个token最相关。

在本节中,我们实现一个简化的自注意力机制来计算这些权重和resulting上下文向量,一次一个。

考虑以下输入句子,它已经被嵌入到3维向量中,如第2章所讨论的。我们选择一个小的嵌入维度用于说明目的,以确保它适合在页面上显示而不需要滚动条:

```python
import torch
inputs = torch.tensor(
  [[0.43, 0.15, 0.89], # Your     (x^1)
   [0.55, 0.87, 0.66], # journey  (x^2)
   [0.57, 0.85, 0.64], # starts   (x^3)
   [0.22, 0.58, 0.33], # with     (x^4)
   [0.77, 0.25, 0.10], # one      (x^5)
   [0.05, 0.80, 0.55]] # step     (x^6)
)

```

实现自注意力的第一步是计算中间值ω,称为注意力分数,如图3.8所示。(请注意,图3.8显示了预定义的inputs张量值的截断版本;例如,0.87被截断为0.8以节省空间。在这个截断版本中,"journey"和"starts"这两个词的嵌入可能看起来相似是随机巧合。)

![Untitled](Images/大模型-从零构建一个大模型/第三章/Untitled7.png)

图3.8 本节的总体目标是说明上下文向量z(2)的计算,使用第二个输入元素x(2)作为查询。该图显示了第一个中间步骤,计算查询x(2)与所有其他输入元素之间的注意力分数ω作为点积。(注意,图中的数字在小数点后截断为一位,以减少视觉杂乱。)

图3.8说明了我们如何计算查询token和每个输入token之间的中间注意力分数。我们通过计算查询x(2)与每个其他输入token的点积来确定这些分数:

```python
query = inputs[1]
attn_scores_2 = torch.empty(inputs.shape[0])
for i, x_i in enumerate(inputs):
    attn_scores_2[i] = torch.dot(x_i, query)
print(attn_scores_2)

```

计算得到的注意力分数如下:

```
tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])

```

### 理解点积

点积本质上是一种将两个向量相乘元素级并然后求和的简洁方式,我们可以演示如下:

```python
res = 0.
for idx, element in enumerate(inputs[0]):
    res += inputs[0][idx] * query[idx]
print(res)
print(torch.dot(inputs[0], query))

```

输出确认元素级乘法的和给出与点积相同的结果:

```
tensor(0.9544)
tensor(0.9544)

```

虽然将点积操作视为将两个向量合并为标量值的数学工具,但点积是一个相似性度量,因为它量化了向量的对齐程度:更高的点积表示向量之间更大程度的对齐或相似性。在自注意力机制的上下文中,点积决定了序列中的元素在多大程度上相互关注:点积越高,相似性越高,元素之间的注意力分数就越高。

在下一步中,如图3.9所示,我们对我们之前计算的注意力分数进行归一化。

![Untitled](Images/大模型-从零构建一个大模型/第三章/Untitled8.png)

图3.9 在计算相对于输入查询x(2)的注意力分数ω21到ω2T之后,下一步是通过归一化注意力分数来获得注意力权重α21到α2T。

图3.9中所示归一化的主要目的是获得总和为1的注意力权重。这种归一化是一种惯例,对于解释和维持LLM中的数值稳定性很有用。这里是一种直接的方法来实现这种归一化:

```python
attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()
print("Attention weights:", attn_weights_2_tmp)
print("Sum:", attn_weights_2_tmp.sum())

```

正如输出所示,注意力权重现在总和为1:

```
Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])
Sum: tensor(1.0000)

```

在实践中,更常见和建议的做法是使用softmax函数进行归一化。这种方法在处理极值时更好,并在训练期间提供更有利的梯度特性。这里是softmax函数的一个基本实现,用于归一化注意力分数:

```python
def softmax_naive(x):
    return torch.exp(x) / torch.exp(x).sum(dim=0)

attn_weights_2_naive = softmax_naive(attn_scores_2)
print("Attention weights:", attn_weights_2_naive)
print("Sum:", attn_weights_2_naive.sum())

```

正如输出所示,softmax函数也实现了目标,归一化注意力权重,使它们总和为1:

```
Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])
Sum: tensor(1.)

```

此外,softmax函数确保我们的注意力权重始终是正的。这使得输出可以解释为概率或相对重要性,其中更高的权重表示更大的重要性。

请注意,这个简单的softmax实现(softmax_naive)可能会遇到数值稳定性问题,例如在处理非常大或非常小的输入值时的上溢和下溢。因此,在实践中,建议使用PyTorch的内置softmax实现,它已经针对性能进行了广泛优化:

```python
attn_weights_2 = torch.softmax(attn_scores_2, dim=0)
print("Attention weights:", attn_weights_2)
print("Sum:", attn_weights_2.sum())

```

在这种情况下,我们可以看到它产生与之前的softmax_naive函数相同的结果:

```
Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])
Sum: tensor(1.)

```

现在我们已经计算了归一化的注意力权重,我们准备进行图3.10中说明的最后一步:通过将嵌入的输入token,x(j),与相应的注意力权重相乘然后求和来计算上下文向量z(2)。

![Untitled](Images/大模型-从零构建一个大模型/第三章/Untitled9.png)

图3.10 最后一步,在计算和归一化查询x(2)的注意力分数以获得注意力权重后,是计算上下文向量z(2)。这个上下文向量是所有输入向量x(1)到x(T)的加权组合,权重由注意力权重决定。

图3.10中描绘的上下文向量z(2)计算为所有输入向量的加权和。这涉及将每个输入向量乘以其相应的注意力权重:

```python
query = inputs[1] # 2nd input token is the query
context_vec_2 = torch.zeros(query.shape)
for i,x_i in enumerate(inputs):
    context_vec_2 += attn_weights_2[i]*x_i
print(context_vec_2)

```

这个计算的结果如下:

```
tensor([0.4419, 0.6515, 0.5683])

```

在下一节中,我们将推广这个过程,以同时计算所有上下文向量。

### 3.3.2 计算所有输入token的注意力权重

在上一节中,我们计算了输入2的注意力权重和上下文向量,如图3.11中的高亮行所示。现在,我们正在扩展这个计算以计算所有输入的注意力权重和上下文向量。

![Untitled](Images/大模型-从零构建一个大模型/第三章/Untitled10.png)

图3.11 高亮的行显示了我们在上一节中计算的第二个输入元素作为查询的注意力权重。本节将推广计算以获得所有其他注意力权重。

我们遵循与之前相同的三个步骤,如图3.12所示,只是我们现在做了一些修改,以计算所有上下文向量而不仅仅是第二个上下文向量z(2)。

![Untitled](Images/大模型-从零构建一个大模型/第三章/Untitled11.png)

图3.12 在自注意力中,我们首先计算注意力分数,然后将其归一化以获得总和为1的注意力权重。这些注意力权重用于计算上下文向量,作为输入的加权和。

首先,在图3.12中说明的步骤1中,我们使用一个额外的for循环来计算所有输入对的点积。

```python
attn_scores = torch.empty(6, 6)
for i, x_i in enumerate(inputs):
    for j, x_j in enumerate(inputs):
        attn_scores[i, j] = torch.dot(x_i, x_j)
print(attn_scores)

```

得到的注意力分数如下:

```
tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],
        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],
        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],
        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],
        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],
        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])

```

这个张量中的每个元素代表每对输入之间的注意力分数,如图3.11所示。注意,图3.11中的值是归一化的,这就是为什么它们与上面张量中的原始注意力分数不同。我们稍后会进行归一化。

计算上述注意力分数张量时,我们使用了Python中的for循环。然而,for循环通常很慢,我们可以使用矩阵乘法来更高效地实现相同的结果:

```python
attn_scores = inputs @ inputs.T
print(attn_scores)

```

我们可以直观地确认结果与之前相同:

```
tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],
        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],
        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],
        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],
        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],
        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])

```

在步骤2中,如图3.12所示,我们现在归一化每一行,使得该行中的值总和为1:

```python
attn_weights = torch.softmax(attn_scores, dim=-1)
print(attn_weights)

```

这返回以下注意力权重张量,与图3.10中显示的值匹配:

```
tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],
        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],
        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],
        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],
        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],
        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])

```

在使用PyTorch时,dim参数在像torch.softmax这样的函数中指定了函数将沿着哪个维度计算。通过设置dim=-1,我们指示softmax函数沿着attn_scores张量的最后一个维度应用归一化。如果attn_scores是一个2D张量(例如,形状为[rows, columns]),dim=-1将沿着列归一化,以便每行(假设这是列维度)的值总和为1。

在我们进入步骤3之前,图3.12中显示的最后一步,让我们快速验证一下每一行确实总和为1:

```python
row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])
print("Row 2 sum:", row_2_sum)
print("All row sums:", attn_weights.sum(dim=-1))

```

结果如下:

```
Row 2 sum: 1.0
All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])

```

在第三和最后一步中,我们现在使用这些注意力权重通过矩阵乘法来计算所有上下文向量:

```python
all_context_vecs = attn_weights @ inputs
print(all_context_vecs)

```

在结果输出张量中,每一行包含一个3维上下文向量:

```
tensor([[0.4421, 0.5931, 0.5790],
        [0.4419, 0.6515, 0.5683],
        [0.4431, 0.6496, 0.5671],
        [0.4304, 0.6298, 0.5510],
        [0.4671, 0.5910, 0.5266],
        [0.4177, 0.6503, 0.5645]])

```

我们可以通过比较第2行与我们之前在3.3.1节中计算的上下文向量z(2)来再次检查结果是否正确:

```python
print("Previous 2nd context vector:", context_vec_2)

```

基于结果,我们可以看到之前计算的context_vec_2与上面张量中的第二行完全匹配:

```
Previous 2nd context vector: tensor([0.4419, 0.6515, 0.5683])

```

这结束了简单自注意力机制的步骤说明。在下一节中,我们将添加可训练权重,使LLM能够从数据中学习并改善其在特定任务上的性能。

## 3.4 实现带有可训练权重的自注意力

在本节中,我们将实现原始Transformer架构、GPT模型和其他流行LLM中使用的自注意力机制。这种自注意力机制也被称为缩放点积注意力。图3.13提供了一个心智模型,说明了我们在本节中编码的这种自注意力机制如何适应本书和本章的更广泛背景。

![Untitled](Images/大模型-从零构建一个大模型/第三章/Untitled12.png)

图3.13 一个心智模型,说明我们在本节中编码的自注意力机制如何适应本书和本章的更广泛背景。在上一节中,我们编码了一个简化的注意力机制以理解注意力机制背后的基本机制。在本节中,我们向这个注意力机制添加可训练权重。在接下来的章节中,我们将通过添加因果掩码和多个头来扩展这个自注意力机制。

如图3.13所示,带有可训练权重的自注意力机制建立在之前的概念之上:我们仍然希望计算上下文向量作为输入向量的加权和,特定于某个输入元素。正如我们将看到的,与我们之前编码的基本自注意力机制相比,这里有两个微小但显著的差异。

最显著的区别是引入了在模型训练期间更新的权重矩阵。这些可训练的权重矩阵是至关重要的,因为它们允许模型(特别是,模型内部的注意力模块)学习产生"良好"的上下文向量。(请注意,我们将在第5章训练LLM。)

我们将在两个小节中实现这种自注意力机制。首先,我们将逐步实现它。然后,我们将把它组织成一个简洁的Python类,可以导入到LLM架构中,这是我们将在第4章中做的。

### 3.4.1 逐步计算注意力权重

我们将通过引入三个可训练的权重矩阵Wq、Wk和Wv来实现自注意力机制。这三个矩阵用于从嵌入的输入token,x(j),创建查询、键和值向量,如图3.14所示。

![Untitled](Images/大模型-从零构建一个大模型/第三章/Untitled13.png)

图3.14 在带有可训练权重矩阵的自注意力机制的第一步中,我们为输入元素x计算查询(q)、键(k)和值(v)向量。与前几节类似,我们将第二个输入x(2)指定为查询输入。通过输入x(2)和权重矩阵Wq之间的矩阵乘法获得查询向量q(2)。同样,我们通过涉及权重矩阵Wk和Wv的矩阵乘法获得键和值向量。

类似于3.3.1节,我们将第二个输入元素x(2)定义为查询,当我们计算简化的注意力权重以计算我们的上下文向量z(2)时。然后,在3.3.2节中,我们将其推广为计算所有上下文向量z(1) ... z(T),用于6词输入句子"This journey starts with one step."

同样,我们将从计算单个上下文向量z(2)开始,以便说明。在下一节中,我们将修改这个过程以计算所有上下文向量。

让我们从定义一些变量开始:

```python
x_2 = inputs[1]
d_in = inputs.shape[1]
d_out = 2

```

注意,在GPT类模型中,输入和输出维度通常是相同的,但为了说明目的,更好地跟踪计算,我们选择了不同的输入(d_in=3)和输出(d_out=2)维度。

接下来,我们初始化图3.14中所示的三个权重矩阵Wq、Wk和Wv:

```python
torch.manual_seed(123)
W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)
W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)
W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)

```

注意,我们设置requires_grad=False以减少输出中的混乱以便说明目的,但如果我们想要使用我们的权重矩阵进行模型训练,我们会设置requires_grad=True以在模型训练期间更新这些矩阵。

现在,我们计算图3.14中较早显示的查询、键和值向量:

```python
query_2 = x_2 @ W_query
key_2 = x_2 @ W_key
value_2 = x_2 @ W_value
print(query_2)

```

正如我们可以从查询的输出中看到,这导致了一个2维向量,因为我们将相应权重矩阵的列数,即d_out,设置为2:

```
tensor([0.4306, 1.4551])

```

### 权重参数与注意力权重

注意,在权重矩阵W中,术语"权重"指的是"权重参数",即在训练期间优化的神经网络的值。这不应与注意力权重混淆。正如我们在上一节中已经看到的,注意力权重决定了上下文向量在多大程度上依赖于输入的不同部分,即网络在多大程度上关注输入的不同部分。

总之,权重参数是定义网络连接的基本、可学习的系数,而注意力权重是动态的、上下文特定的值。

虽然临时目标是仅计算一个上下文向量z(2),我们仍然需要所有输入元素的键和值向量,因为它们涉及计算相对于查询q(2)的注意力权重,如图3.14所示。

我们可以通过矩阵乘法获得所有键和值:

```python
keys = inputs @ W_key
values = inputs @ W_value
print("keys.shape:", keys.shape)
print("values.shape:", values.shape)

```

正如我们可以从输出中看到,我们成功地将6个输入token从3D投影到了2D嵌入空间:

```
keys.shape: torch.Size([6, 2])
values.shape: torch.Size([6, 2])

```

下一步是计算注意力分数,如图3.15所示。

![Untitled](Images/大模型-从零构建一个大模型/第三章/Untitled14.png)

图3.15 注意力分数的计算是一个点积计算,类似于我们在3.3节中简化的自注意力机制中使用的。这里的新方面是我们不直接计算输入元素之间的点积,而是使用通过各自的权重矩阵变换输入得到的查询和键。

首先,让我们计算注意力分数ω22:

```python
keys_2 = keys[1]
attn_score_22 = query_2.dot(keys_2)
print(attn_score_22)

```

结果是以下未归一化的注意力分数:

```
tensor(1.8524)

```

同样,我们可以使用矩阵乘法将此计算推广到所有注意力分数:

```python
attn_scores_2 = query_2 @ keys.T # All attention scores for given query
print(attn_scores_2)

```

正如我们所看到的,作为快速检查,输出中的第二个元素匹配我们之前计算的attn_score_22:

```
tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])

```

第三步是从注意力分数到注意力权重,如图3.16所示。

![Untitled](Images/大模型-从零构建一个大模型/第三章/Untitled15.png)

图3.16 在计算注意力分数ω之后,下一步是使用softmax函数对这些分数进行归一化以获得注意力权重α。

现在,如图3.16所示,我们通过取注意力分数并使用我们之前的softmax函数来计算注意力权重。与之前的不同之处在于我们现在通过将它们除以键的嵌入维度的平方根来缩放注意力分数(注意,取平方根在数学上等同于指数0.5):

```python
d_k = keys.shape[-1]
attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)
print(attn_weights_2)

```

得到的注意力权重如下:

```
tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])

```

### 缩放点积注意力的原理

按嵌入维度归一化的原因是为了通过避免小梯度来改善训练性能。例如,当缩放大的点积(这在实际上通常大于数千的GPT类LLM中很常见)时,大点积可能会导致在反向传播期间由于softmax函数应用于它们而产生过小的梯度。随着点积增加,softmax函数表现得更像一个硬函数,导致梯度趋近于零。这些小梯度可能会显著减慢学习速度,导致训练停滞。

通过嵌入维度的平方根进行缩放是这种自注意力机制被称为缩放点积注意力的原因。

现在,最后一步是计算上下文向量,如图3.17所示。

![Untitled](Images/大模型-从零构建一个大模型/第三章/Untitled16.png)

图3.17 在自注意力计算的最后一步,我们通过注意力权重组合所有值向量来计算上下文向量。

与3.3节类似,我们将上下文向量计算为输入向量的加权和,现在我们将上下文向量计算为值向量的加权和。这里,注意力权重作为一个加权因子,权衡每个值向量的相对重要性。与3.3节类似,我们可以使用矩阵乘法一步获得输出:

```python
context_vec_2 = attn_weights_2 @ values
print(context_vec_2)

```

结果向量的内容如下:

```
tensor([0.3061, 0.8210])

```

到目前为止,我们已经计算了单个上下文向量z(2)。在下一节中,我们将推广这些步骤以计算输入序列中的所有上下文向量,z(1)到z(T)。

### 为什么是查询、键和值?

"查询"、"键"和"值"这些术语在注意力机制的上下文中是从信息检索和数据库领域借鉴的,在那里类似的概念用于存储、搜索和检索信息。

"查询"类似于数据库中的搜索查询。它代表当前焦点(例如,句子中的当前单词或token)模型试图理解。查询用于探测其他部分的输入序列,以确定要给予它们多少注意力。

"键"就像数据库键用于索引和搜索。在注意力机制中,输入序列中的每个项(例如,句子中的每个单词)都有一个相关的键。这些键用于与查询匹配。

在这种情况下的"值"类似于键值对中的值。它代表输入项的实际内容或表示。一旦模型确定哪些键(因此哪些输入部分)与查询最相关,它就检索相应的值。

### 3.4.2 实现一个紧凑的自注意力Python类

在前面的部分中,我们经历了许多步骤来计算自注意力输出。这主要是为了说明目的,这样我们可以一步一步地详细了解。在实践中,当在下一章实际实现LLM时,将这些步骤组织成一个Python类是很有帮助的,如下所示:

清单3.1 一个紧凑的自注意力类

```python
import torch.nn as nn
class SelfAttention_v1(nn.Module):
    def __init__(self, d_in, d_out):
        super().__init__()
        self.d_out = d_out
        self.W_query = nn.Parameter(torch.rand(d_in, d_out))
        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))
        self.W_value = nn.Parameter(torch.rand(d_in, d_out))

    def forward(self, x):
        keys = x @ self.W_key
        queries = x @ self.W_query
        values = x @ self.W_value
        attn_scores = queries @ keys.T # omega
        attn_weights = torch.softmax(
            attn_scores / keys.shape[-1]**0.5, dim=-1)
        context_vec = attn_weights @ values
        return context_vec

```

在这个PyTorch类中,SelfAttention_v1是一个从nn.Module派生的类,nn.Module是PyTorch模型的一个基本构建块,为模型层创建和管理提供必要的功能。

__init__方法初始化可训练的权重矩阵(W_query、W_key和W_value)用于查询、键和值,将输入维度d_in转换为输出维度d_out。

在forward步骤中,使用forward方法,我们计算注意力分数(attn_scores)通过乘以查询和键,使用softmax归一化这些分数。最后,我们通过用这些归一化的注意力分数加权值来创建上下文向量。

我们可以使用这个类如下:

```python
torch.manual_seed(123)
sa_v1 = SelfAttention_v1(d_in, d_out)
print(sa_v1(inputs))

```

由于inputs包含六个嵌入向量,这会导致一个矩阵存储六个上下文向量:

```
tensor([[0.2996, 0.8053],
        [0.3061, 0.8210],
        [0.3058, 0.8203],
        [0.2948, 0.7939],
        [0.2927, 0.7891],
        [0.2990, 0.8040]], grad_fn=<MmBackward0>)

```

作为快速检查,注意第二行([0.3061, 0.8210])与上一节中context_vec_2的内容匹配。

图3.18总结了我们刚刚实现的自注意力机制。

![Untitled](Images/大模型-从零构建一个大模型/第三章/Untitled17.png)

图3.18 在自注意力中,我们用三个权重矩阵Wq、Wk和Wv变换输入矩阵X中的输入向量。然后,我们基于得到的查询(Q)和键(K)计算注意力权重矩阵。使用注意力权重和值(V),我们然后计算上下文向量(Z)。(为了视觉清晰,我们在这个图中只关注单个输入文本的n个token,而不是多个输入的批次。因此,3D输入张量在这个上下文中被简化为2D矩阵。这种方法允许更直接的可视化和对所涉及过程的理解。另外,为了与后面的图保持一致,注意力矩阵中的值并不描绘真实的注意力权重。)

如图3.18所示,自注意力涉及三个可训练的权重矩阵Wq、Wk和Wv。这些矩阵将输入转换为查询、键和值,这些是注意力机制的关键组成部分。随着模型暴露于更多数据进行训练,它调整这些可训练的权重,正如我们将在接下来的章节中看到的。

我们可以通过利用PyTorch的nn.Linear层进一步改进SelfAttention_v1实现,这些层有效地执行矩阵乘法并添加偏置。此外,使用nn.Linear而不是手动实现nn.Parameter(torch.rand(...))的一个显著优势是nn.Linear有一个优化的权重初始化方案,有助于更稳定和有效的模型训练。

清单3.2 使用PyTorch的Linear层的自注意力类

```python
class SelfAttention_v2(nn.Module):
    def __init__(self, d_in, d_out, qkv_bias=False):
        super().__init__()
        self.d_out = d_out
        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)

    def forward(self, x):
        keys = self.W_key(x)
        queries = self.W_query(x)
        values = self.W_value(x)
        attn_scores = queries @ keys.T
        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
        context_vec = attn_weights @ values
        return context_vec

```

你可以使用SelfAttention_v2类似于SelfAttention_v1:

```python
torch.manual_seed(789)
sa_v2 = SelfAttention_v2(d_in, d_out)
print(sa_v2(inputs))

```

输出是:

```
tensor([[-0.0739,  0.0713],
        [-0.0748,  0.0703],
        [-0.0749,  0.0702],
        [-0.0760,  0.0685],
        [-0.0763,  0.0679],
        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)

```

注意,SelfAttention_v1和SelfAttention_v2有不同的输出,因为它们对权重矩阵有不同的初始权重。这是因为nn.Linear有一个更复杂的权重初始化方案,而不是nn.Parameter(torch.rand(d_in, d_out))中使用的简单随机初始化,这导致这些机制产生不同的结果。

练习 3.1 比较SELFATTENTION_V1和SELFATTENTION_V2

注意,SelfAttention_v2中的nn.Linear有一个不同的权重初始化方案,而不是SelfAttention_v1中使用的nn.Parameter(torch.rand(d_in, d_out)),这导致这些机制产生不同的结果。要检查这两个实现,SelfAttention_v1和SelfAttention_v2,是否在其他方面相似,我们可以从SelfAttention_v2对象转移权重矩阵到SelfAttention_v1,使得两个对象然后产生相同的结果。

任务是正确地分配权重从一个SelfAttention_v2实例到一个SelfAttention_v1实例。为了做到这一点,你需要理解两个版本之间的关系。(提示:nn.Linear以转置的形式存储权重矩阵。)在分配之后,你应该观察到两个实例产生相同的输出。

在下一节中,我们将对自注意力机制进行增强,特别关注于纳入因果和多头元素。因果方面涉及修改注意力机制以防止模型在序列中访问未来信息,这对于语言建模等任务至关重要,在这些任务中,每个词的预测只应依赖于前面的词。

多头组件涉及将注意力机制分成多个"头"。每个头学习输入的不同方面,允许模型同时关注来自不同表示子空间的信息。这提高了模型在复杂任务中的性能。

## 3.5 用因果注意力隐藏未来词

在本节中,我们修改标准的自注意力机制以创建一个因果注意力机制,这对于在后续章节中开发LLM至关重要。

因果注意力,也称为掩码注意力,是自注意力的一种专门形式。它限制模型在处理和生成任何给定token时只考虑序列中的先前和当前输入。这与标准的自注意力机制形成对比,后者允许访问整个输入序列。

因此,在计算注意力分数时,因果注意力机制确保模型只考虑在序列中出现在当前token之前或等于当前token的token。

要在GPT类LLM中实现这一点,对于每个token处理,我们屏蔽掉未来的token,即在输入行中当前token之后出现的token,如图3.19所示。

![Untitled](Images/大模型-从零构建一个大模型/第三章/Untitled18.png)

图3.19 在因果注意力中,我们屏蔽掉对角线上方的注意力权重,使得对于给定的输入,LLM在使用注意力权重计算上下文向量时无法访问未来的token。例如,对于第二行中的单词"journey",我们只保留之前("Your")和当前位置("journey")的单词的注意力权重。

如图3.19所示,我们屏蔽掉对角线上方的注意力权重,并且我们归一化非屏蔽的注意力权重,使得每行中的注意力权重总和为1。在下一节中,我们将在代码中实现这个屏蔽和归一化过程。

### 3.5.1 应用因果注意力掩码

在本节中,我们在代码中实现因果注意力掩码。我们从图3.20中总结的程序开始。

![Untitled](Images/大模型-从零构建一个大模型/第三章/Untitled19.png)

图3.20 在因果注意力中获得掩码注意力权重矩阵的一种方法是对注意力分数应用softmax函数,将对角线上方的元素置零,然后对结果矩阵进行归一化。

为了实现图3.20中总结的步骤以应用因果注意力掩码获得掩码注意力权重,让我们从上一节计算的注意力分数和权重开始,以展示因果注意力机制。

在图3.20所示的第一步中,我们使用softmax函数计算注意力权重,就像我们在前面的章节中所做的那样:

```python
queries = sa_v2.W_query(inputs)
keys = sa_v2.W_key(inputs)
attn_scores = queries @ keys.T
attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)
print(attn_weights)

```

这导致以下注意力权重:

```
tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],
        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],
        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],
        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],
        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],
        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],
       grad_fn=<SoftmaxBackward0>)

```

我们可以使用PyTorch的tril函数实现图3.20中的步骤2,创建一个掩码,其中对角线上方的值被置零:

```python
context_length = attn_scores.shape[0]
mask_simple = torch.tril(torch.ones(context_length, context_length))
print(mask_simple)

```

得到的掩码如下:

```
tensor([[1., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1.]])

```

现在,我们可以将这个掩码与注意力权重相乘以将对角线上方的值置零:

```python
masked_simple = attn_weights*mask_simple
print(masked_simple)

```

正如我们可以看到,对角线上方的元素已成功置零:

```
tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],
        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],
        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],
        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],
       grad_fn=<MulBackward0>)

```

图3.20中的第三步是重新归一化注意力权重,使每行再次总和为1。我们可以通过将每行中的每个元素除以该行的总和来实现这一点:

```python
row_sums = masked_simple.sum(dim=1, keepdim=True)
masked_simple_norm = masked_simple / row_sums
print(masked_simple_norm)

```

结果是一个注意力权重矩阵,其中对角线上方的注意力权重被置零,每行总和为1:

```
tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],
        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],
        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],
        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],
       grad_fn=<DivBackward0>)

```

### 信息泄漏

当我们应用掩码然后重新归一化注意力权重时,可能最初看起来似乎来自未来token(我们打算掩码)的信息仍然可能影响当前token,因为它们的值是softmax计算的一部分。然而,关键的见解是当我们在掩码后重新归一化注意力权重时,我们本质上是在一个较小的子集(因为掩码位置不贡献softmax值)上重新计算softmax。

softmax的数学魔力是,尽管最初包括所有位置在分母中,在掩码和重新归一化之后,掩码位置的影响是被消除的 - 它们不再对softmax分数有任何有意义的贡献。

用简单的话说,在掩码和重新归一化之后,注意力权重的分布就像是最初只在非掩码位置之间计算的一样。这确保没有来自未来(或其他被掩码)token的信息泄漏,正如我们预期的那样。

虽然我们可以在这一点上理论上停止因果注意力的实现,但我们可以利用softmax函数的一个数学特性,用更少的步骤更有效地实现掩码注意力权重的计算,如图3.21所示。

![Untitled](Images/大模型-从零构建一个大模型/第三章/Untitled20.png)

图3.21 在因果注意力中获得掩码注意力权重矩阵的一种更有效的方法是在应用softmax函数之前用负无穷大值掩码注意力分数。

softmax函数将其输入转换为概率分布。当一行中存在负无穷大值(-∞)时,softmax函数将它们视为零概率。(数学上,这是因为e^-∞接近0。)

我们可以通过创建一个掩码来实现这种更有效的掩码"技巧",其中对角线上方的1被负无穷大(-inf)值替换:

```python
mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)
masked = attn_scores.masked_fill(mask.bool(), -torch.inf)
print(masked)

```

这导致以下掩码:

```python
tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],
        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],
        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],
        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],
        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],
        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],
       grad_fn=<MaskedFillBackward0>)

```

现在,我们所需要做的就是将softmax函数应用于这些掩码结果,我们就得到了:

```python
attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)
print(attn_weights)

```

正如我们可以从输出中看到,每行中的值总和为1,不需要进一步的归一化:

```
tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],
        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],
        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],
        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],
       grad_fn=<SoftmaxBackward0>)

```

我们现在可以使用修改后的注意力权重来计算上下文向量,通过context_vec = attn_weights @ values,就像在3.4节中那样。然而,在下一节中,我们首先要对因果注意力机制进行另一个小调整,这在训练LLM时很有用,以减少过拟合。

### 3.5.2 使用dropout屏蔽额外的注意力权重

Dropout是一种技术,其中随机选择的隐藏层单元在训练期间被忽略,有效地"丢弃"它们。这种方法有助于防止过拟合,确保模型不会变得过度依赖于任何特定的隐藏层单元集。重要的是要强调,dropout仅在训练期间使用,在推理时被禁用。

在Transformer架构(包括GPT等模型)中,注意力机制中的dropout通常应用在两个特定区域:在计算注意力分数之后或在将注意力权重应用于值向量之后。

这里,我们将在计算注意力权重之后应用dropout,如图3.22所示,因为这是实践中更常见的变体。

![Untitled](Images/大模型-从零构建一个大模型/第三章/Untitled21.png)

图3.22 使用因果注意力掩码(左上),我们应用额外的dropout掩码(右上)以将额外的注意力权重置零,以减少训练期间的过拟合。

在下面的代码示例中,我们使用50%的dropout率,这意味着屏蔽一半的注意力权重。(当我们在后面的章节中训练我们的LLM模型时,我们将使用更低的dropout率,如0.1或0.2。)

在以下代码中,我们首先将PyTorch的dropout实现应用于一个6×6张量,该张量由1组成,用于说明目的:

```python
torch.manual_seed(123)
dropout = torch.nn.Dropout(0.5)
example = torch.ones(6, 6)
print(dropout(example))

```

正如我们可以看到,大约一半的值被置零了:

```
tensor([[2., 2., 0., 2., 2., 0.],
        [0., 0., 0., 2., 0., 2.],
        [2., 2., 2., 2., 0., 2.],
        [0., 2., 2., 0., 0., 2.],
        [0., 2., 0., 2., 0., 2.],
        [0., 2., 2., 2., 2., 0.]])

```

当使用50%的率将dropout应用于注意力权重矩阵时,矩阵中的一半元素随机置为零。为了补偿活跃元素的减少,矩阵中剩余元素的值被缩放了一个因子1/0.5 = 2。这种缩放对于维持注意力权重的整体平衡至关重要,确保在训练和推理阶段注意力机制的平均影响保持一致。

现在,让我们将dropout应用到注意力权重矩阵本身:

```python
torch.manual_seed(123)
print(dropout(attn_weights))

```

结果注意力权重矩阵现在有额外的元素置零,剩余值被缩放:

```
tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],
        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],
       grad_fn=<MulBackward0>

```

注意,结果dropout输出可能因你的操作系统而略有不同;你可以阅读更多关于这种非一致性的内容在PyTorch问题跟踪器https://github.com/pytorch/pytorch/issues/121595。

在获得了对因果注意力和dropout掩码的理解后,我们将在下一节中开发一个简洁的Python类。这个类旨在促进这两种技术的有效应用。

### 3.5.3 实现一个紧凑的因果注意力类

在本节中,我们现在将因果注意力和dropout修改纳入到我们在3.4节中开发的SelfAttention Python类中。这个类将然后作为在即将到来的部分中开发多头注意力的模板,这是我们在本章中实现的最终注意力类。

在我们开始之前,一件重要的事情是确保类可以处理由多个输入组成的批次,以便CausalAttention类支持我们在第2章中实现的数据加载器生成的批次输出。

为了模拟批次输入,我们复制输入示例:

```python
batch = torch.stack((inputs, inputs), dim=0)
print(batch.shape)

```

这导致了一个3D张量,由2个输入文本组成,每个文本有6个token,其中每个token是一个3维嵌入向量:

```
torch.Size([2, 6, 3])

```

以下CausalAttention类类似于我们之前实现的SelfAttention类,除了我们现在添加了dropout和因果掩码组件,如下面的代码中突出显示的:

清单3.3 一个紧凑的因果注意力类

```python
class CausalAttention(nn.Module):
    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):
        super().__init__()
        self.d_out = d_out
        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.dropout = nn.Dropout(dropout)
        self.register_buffer(
           'mask',
           torch.triu(torch.ones(context_length, context_length),
           diagonal=1)
        )

    def forward(self, x):
        b, num_tokens, d_in = x.shape
        keys = self.W_key(x)
        queries = self.W_query(x)
        values = self.W_value(x)

        attn_scores = queries @ keys.transpose(1, 2)
        attn_scores.masked_fill_(
            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)
        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
        attn_weights = self.dropout(attn_weights)

        context_vec = attn_weights @ values
        return context_vec

```

虽然大部分代码行应该从之前的部分熟悉,但我们现在在__init__方法中添加了一个self.register_buffer()调用。使用register_buffer在PyTorch中并不是严格必要的,但它为我们使用buffer提供了几个优势。例如,当我们在LLM中使用CausalAttention类时,buffer会自动随模型一起移动到适当的设备(CPU或GPU),这意味着我们不需要手动确保这些张量在与模型参数相同的设备上,避免设备不匹配错误。

我们可以使用CausalAttention类,类似于之前的SelfAttention:

```python
torch.manual_seed(123)
context_length = batch.shape[1]
ca = CausalAttention(d_in, d_out, context_length, 0.0)
context_vecs = ca(batch)
print("context_vecs.shape:", context_vecs.shape)

```

结果上下文向量是一个3D张量,其中每个token现在由2D嵌入表示:

```
context_vecs.shape: torch.Size([2, 6, 2])

```

图3.23提供了一个心智模型,总结了我们到目前为止所完成的工作。

![Untitled](Images/大模型-从零构建一个大模型/第三章/Untitled22.png)

图3.23 一个心智模型,总结了我们在本章中编码的四种不同注意力模块。我们从一个简化的注意力机制开始,添加了可训练权重,然后添加了因果注意力掩码。在本章的剩余部分,我们将扩展因果注意力机制并编码多头注意力,这是我们将在下一章LLM实现中使用的最终模块。

如图3.23所示,在本节中,我们关注了神经网络中因果注意力的概念和实现。在下一节中,我们将扩展这个概念并实现一个多头注意力模块,它并行实现了几个这样的因果注意力机制。

## 3.6 将单头注意力扩展到多头注意力

在本章的最后一节中,我们将把之前实现的因果注意力类扩展为多头。这也被称为多头注意力。

术语"多头"指的是将注意力机制分成多个"头",这些头独立操作。在这种情况下,单个因果注意力模块可以被认为是单头注意力,其中只有一组注意力权重同时处理输入。

在以下小节中,我们将讨论从因果注意力到多头注意力的这种扩展。第一个小节将通过堆叠多个CausalAttention模块来直观地构建多头注意力模块,用于说明目的。第二个小节将实现相同的多头注意力模块,但以更紧凑和计算上更有效的方式。

### 3.6.1 堆叠多个单头注意力层

实际上,实现多头注意力涉及创建自注意力机制的多个实例(在3.4.1节的图3.18中描述),每个实例都有自己的权重,然后组合它们的输出。有多个实例的自注意力机制可能看起来计算昂贵,但它对于LLM生成复杂模式的能力至关重要,这是使transformer-based LLM如此强大的原因。

图3.24说明了多头注意力模块的结构,它由多个单头注意力模块组成,如之前在图3.18中描绘的,堆叠在一起。

![Untitled](Images/大模型-从零构建一个大模型/第三章/Untitled23.png)

图3.24 这个多头注意力模块的图描绘了两个堆叠在一起的单头注意力模块。所以,与其使用单个矩阵Wv来计算值矩阵,在具有两个头的多头注意力模块中,我们现在有两个值权重矩阵:Wv1和Wv2。这同样适用于其他权重矩阵Wq和Wk。我们获得两组上下文向量Z1和Z2,可以将它们组合成单个上下文向量矩阵Z。

如前所述,多头注意力背后的主要思想是让注意力机制多次运行(并行)并具有不同的学习线性投影 - 将输入乘以权重矩阵(用于查询、键和值向量的注意力机制)的结果。

在代码中,我们可以通过实现一个简单的MultiHeadAttentionWrapper类来实现这一点,该类堆叠多个我们之前实现的CausalAttention模块的实例:

清单3.4 一个包装类来实现多头注意力

```python
class MultiHeadAttentionWrapper(nn.Module):
    def __init__(self, d_in, d_out, context_length,
                 dropout, num_heads, qkv_bias=False):
        super().__init__()
        self.heads = nn.ModuleList(
            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)
             for _ in range(num_heads)]
        )

    def forward(self, x):
        return torch.cat([head(x) for head in self.heads], dim=-1)

```

例如,如果我们使用这个MultiHeadAttentionWrapper类与两个注意力头(即num_heads=2)和CausalAttention输出维度d_out=2,这将导致一个4维上下文向量(d_out*num_heads=4),如图3.25所示。

![Untitled](Images/大模型-从零构建一个大模型/第三章/Untitled24.png)

图3.25 使用MultiHeadAttentionWrapper,我们指定了注意力头的数量(num_heads)。如果我们设置num_heads=2,如这个图所示,我们得到一个包含两组上下文向量矩阵的张量。在每个上下文向量矩阵中,行代表对应于token的上下文向量,列对应于通过d_out=4指定的嵌入维度。我们沿着列维度连接这些上下文向量矩阵。由于我们有2个注意力头和2的嵌入维度,最终的嵌入维度是2 × 2 = 4。

为了进一步说明图3.25并给出一个具体的例子,我们可以使用我们的MultiHeadAttentionWrapper类,类似于之前的CausalAttention类:

```python
torch.manual_seed(123)
context_length = batch.shape[1] # This is the number of tokens
d_in, d_out = 3, 2
mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)
context_vecs = mha(batch)

print(context_vecs)
print("context_vecs.shape:", context_vecs.shape)

```

这导致以下张量表示上下文向量:

```
tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],
         [-0.5874,  0.0058,  0.5891,  0.3257],
         [-0.6300, -0.0632,  0.6202,  0.3860],
         [-0.5675, -0.0843,  0.5478,  0.3589],
         [-0.5526, -0.0981,  0.5321,  0.3428],
         [-0.5299, -0.1081,  0.5077,  0.3493]],

        [[-0.4519,  0.2216,  0.4772,  0.1063],
         [-0.5874,  0.0058,  0.5891,  0.3257],
         [-0.6300, -0.0632,  0.6202,  0.3860],
         [-0.5675, -0.0843,  0.5478,  0.3589],
         [-0.5526, -0.0981,  0.5321,  0.3428],
         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)
context_vecs.shape: torch.Size([2, 6, 4])

```

结果context_vecs张量的第一个维度是2,因为我们有两个输入文本(输入文本被复制,这就是为什么我们的上下文向量对两者完全相同)。第二个维度指的是每个输入中的6个token。第三个维度指的是每个token的4维嵌入。

练习 3.2 返回2维嵌入向量

更改MultiHeadAttentionWrapper(..., num_heads=2)的输入参数,使得输出上下文向量是2维而不是4维,同时保持设置num_heads=2。提示:你不需要修改类实现;你只需要更改其他输入参数之一。

在本节中,我们实现了一个MultiHeadAttentionWrapper,它组合了多个单头注意力模块。然而,请注意,这些是按顺序处理的,因为[head(x) for head in self.heads]在forward方法中。我们可以通过并行处理头来改进这个实现。实现这一点的方法是通过矩阵乘法同时计算所有注意力头的输出,我们将在下一节中探讨这一点。

### 3.6.2 使用权重分割实现多头注意力

在上一节中,我们创建了一个MultiHeadAttentionWrapper来通过堆叠多个单头注意力模块来实现多头注意力。这是通过实例化和组合几个CausalAttention对象完成的。

我们可以将这些概念中的许多组合到一个单一的MultiHeadAttention类中,而不是实例化单独的类MultiHeadAttentionWrapper和CausalAttention。此外,除了合并MultiHeadAttentionWrapper和CausalAttention的代码之外,我们将做一些其他修改以更有效地实现多头注意力。

在MultiHeadAttentionWrapper中,多个头是通过创建一个CausalAttention对象列表(self.heads)实现的,每个对象代表一个单独的注意力头。CausalAttention类独立执行注意力机制,结果从每个头被连接起来。相比之下,以下MultiHeadAttention类在单个类内集成了多头功能。它通过拆分投影的查询、键和值张量来将输入分成多个头,然后在计算注意力后组合这些头的结果。

让我们看一下MultiHeadAttention类,然后我们将进一步讨论它:

清单3.5 一个高效的多头注意力类

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_in, d_out,
                 context_length, dropout, num_heads, qkv_bias=False):
        super().__init__()
        assert d_out % num_heads == 0, "d_out must be divisible by num_heads"

        self.d_out = d_out
        self.num_heads = num_heads
        self.head_dim = d_out / num_heads
        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.out_proj = nn.Linear(d_out, d_out)
        self.dropout = nn.Dropout(dropout)
        self.register_buffer(
            'mask',
             torch.triu(torch.ones(context_length, context_length), diagonal=1)
        )

    def forward(self, x):
        b, num_tokens, d_in = x.shape
        keys = self.W_key(x)
        queries = self.W_query(x)
        values = self.W_value(x)

        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)
        values = values.view(b, num_tokens, self.num_heads, self.head_dim)
        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)

        keys = keys.transpose(1, 2)
        queries = queries.transpose(1, 2)
        values = values.transpose(1, 2)

        attn_scores = queries @ keys.transpose(2, 3)
        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]

        attn_scores.masked_fill_(mask_bool, -torch.inf)

        attn_weights = torch.softmax(
            attn_scores / keys.shape[-1]**0.5, dim=-1)
        attn_weights = self.dropout(attn_weights)

        context_vec = (attn_weights @ values).transpose(1, 2)

        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)
        context_vec = self.out_proj(context_vec)
        return context_vec

```

虽然MultiHeadAttention类内的重塑(.view)和转置(.transpose)张量看起来可能很复杂,但从数学上讲,MultiHeadAttention类实现了与之前的MultiHeadAttentionWrapper相同的概念。

在一个高层次上,在之前的MultiHeadAttentionWrapper中,我们堆叠了多个单头注意力层,然后将它们组合成一个多头注意力层。MultiHeadAttention类采取了一种整合的方法。它从一个多头层开始,然后线性地将该层分割成单独的注意力头,如图3.26所示。

![Untitled](Images/大模型-从零构建一个大模型/第三章/Untitled25.png)

图3.26 在具有两个注意力头的MultiheadAttentionWrapper类中,我们初始化了两个权重矩阵Wq1和Wq2,并计算了两个查询矩阵Q1和Q2,如本图顶部所示。在MultiheadAttention类中,我们初始化一个更大的权重矩阵Wq,只执行一次与输入的矩阵乘法以获得查询矩阵Q,然后将查询矩阵分割成Q1和Q2,如本图底部所示。我们对键和值做同样的操作,这里没有显示以减少视觉混乱。

查询、键和值张量的分割,如图3.26所描绘的,是通过PyTorch的.view和.transpose方法使用张量重塑和转置操作实现的。输入首先被变换(通过查询、键和值的线性层),然后被重塑以表示多个头。

关键操作是将d_out维度分割成num_heads和head_dim,其中head_dim = d_out / num_heads。这种分割然后通过.view方法实现:形状为(b, num_tokens, d_out)的张量被重塑为形状(b, num_tokens, num_heads, head_dim)。

然后,张量被转置以将num_heads维度放在num_tokens维度之前,结果形状为(b, num_heads, num_tokens, head_dim)。这种转置对于正确对齐查询、键和值在不同头之间并高效执行批处理矩阵乘法至关重要。

为了说明这种批处理矩阵乘法,假设我们有以下示例张量:

```python
a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],
                    [0.8993, 0.0390, 0.9268, 0.7388],
                    [0.7179, 0.7058, 0.9156, 0.4340]],

                   [[0.0772, 0.3565, 0.1479, 0.5331],
                    [0.4066, 0.2318, 0.4545, 0.9737],
                    [0.4606, 0.5159, 0.4220, 0.5786]]]])

```

现在,我们执行一个批处理矩阵乘法,在张量本身和张量的一个视图之间,我们转置了最后两个维度,num_tokens和head_dim:

```python
print(a @ a.transpose(2, 3))

```

结果如下:

```
tensor([[[[1.3208, 1.1631, 1.2879],
          [1.1631, 2.2150, 1.8424],
          [1.2879, 1.8424, 2.0402]],

         [[0.4391, 0.7003, 0.5903],
          [0.7003, 1.3737, 1.0620],
          [0.5903, 1.0620, 0.9912]]]])

```

在这种情况下,PyTorch中的矩阵乘法实现处理4维输入张量,以便矩阵乘法是在最后2个维度(num_tokens, head_dim)之间进行的,然后对单个头重复。

例如,上面变得一种更简洁的方式来计算每个头的矩阵乘法是分别:

```python
first_head = a[0, 0, :, :]
first_res = first_head @ first_head.T
print("First head:\\n", first_res)

second_head = a[0, 1, :, :]
second_res = second_head @ second_head.T
print("\\nSecond head:\\n", second_res)

```

结果与我们之前使用批处理矩阵乘法print(a @ a.transpose(2, 3))得到的结果完全相同:

```
First head:
 tensor([[1.3208, 1.1631, 1.2879],
        [1.1631, 2.2150, 1.8424],
        [1.2879, 1.8424, 2.0402]])

Second head:
 tensor([[0.4391, 0.7003, 0.5903],
        [0.7003, 1.3737, 1.0620],
        [0.5903, 1.0620, 0.9912]])

```

回到MultiHeadAttention,在计算注意力权重和上下文向量后,来自所有头的上下文向量被转置回形状(b, num_tokens, num_heads, head_dim)。这些向量然后被重塑(展平)回形状(b, num_tokens, d_out),有效地组合了来自所有头的输出。

另外,我们在MultiHeadAttention中添加了一个所谓的输出投影层(self.out_proj),在组合头之后,这在CausalAttention类中不存在。这个输出投影层并不是严格必要的(请参阅附录C中的参考部分了解更多细节),但它在许多LLM架构中常用,这就是我们在这里添加它的原因,以求完整性。

尽管MultiHeadAttention类看起来比MultiHeadAttentionWrapper更复杂,由于额外的重塑和转置张量,但它更加高效。原因是我们只进行一次矩阵乘法来计算,例如,keys = self.W_key(x)(同样适用于查询和值)。在MultiHeadAttentionWrapper中,我们需要对每个注意力头重复这个矩阵乘法,这在计算上是最昂贵的步骤之一。

MultiHeadAttention类可以像我们之前的SelfAttention和CausalAttention类一样使用:

```python
torch.manual_seed(123)
batch_size, context_length, d_in = batch.shape
d_out = 2
mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)
context_vecs = mha(batch)
print(context_vecs)
print("context_vecs.shape:", context_vecs.shape)

```

正如我们可以从结果中看到,我们的输出维度现在直接由d_out参数控制:

```
tensor([[[0.3190, 0.4858],
         [0.2943, 0.3897],
         [0.2856, 0.3593],
         [0.2693, 0.3873],
         [0.2639, 0.3928],
         [0.2575, 0.4028]],

        [[0.3190, 0.4858],
         [0.2943, 0.3897],
         [0.2856, 0.3593],
         [0.2693, 0.3873],
         [0.2639, 0.3928],
         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)
context_vecs.shape: torch.Size([2, 6, 2])

```

在本节中,我们实现了MultiHeadAttention类,我们将在接下来的章节中使用它来实现和训练LLM本身。注意,虽然代码是完全功能的,但我们使用了相对较小的嵌入大小和注意力头数量来使输出可读。

作为比较,最小的GPT-2模型(117百万参数)有12个注意力头和768的上下文向量嵌入大小。最大的GPT-2模型(15亿参数)有25个注意力头和1600的上下文向量嵌入大小。注意,token输入和上下文嵌入的大小在GPT模型中是相同的(d_in = d_out)。

练习 3.3 初始化GPT-2大小的注意力模块

使用MultiHeadAttention类,初始化一个多头注意力模块,使其具有与最小的GPT-2模型相同数量的注意力头(12个注意力头)。还要确保你使用与GPT-2相似的相应输入和输出嵌入大小(768维)。注意,最小的GPT-2模型支持1024个token的上下文长度。

## 3.7 总结

- 注意力机制将输入元素转换为增强的上下文向量表示,这些表示包含了所有输入的信息。
- 自注意力机制将上下文向量表示计算为输入的加权和。
- 在简化的注意力机制中,注意力权重通过点积计算。
- 点积只是一种简洁的方式来按元素乘两个向量然后求和。
- 虽然不是严格要求,但矩阵乘法帮助我们更高效和紧凑地实现计算,替代嵌套的for循环。
- 在LLM中使用的自注意力机制(也称为缩放点积注意力)中,我们包括可训练的权重矩阵来计算输入的中间变换:查询、值和键。
- 在处理从左到右读取和生成文本的LLM时,我们添加因果注意力掩码以防止LLM访问未来的token。
- 除了用于置零注意力权重的因果注意力掩码外,我们还可以添加dropout掩码以减少LLM中的过拟合。
- 基于transformer的LLM中的注意力模块涉及多个因果注意力的实例,这被称为多头注意力。
- 我们可以通过堆叠多个因果注意力模块的实例来创建多头注意力模块。
- 创建多头注意力模块的更有效方法涉及批处理矩阵乘法。

通过这一章,我们深入探讨了注意力机制的实现细节,从简单的自注意力开始,逐步发展到更复杂的多头注意力。这些概念和实现将为我们在接下来的章节中构建完整的LLM奠定基础。注意力机制是现代LLM的核心,理解它们的工作原理对于掌握和改进这些强大的模型至关重要。

在下一章中,我们将把这些注意力模块整合到一个完整的LLM架构中,并探索如何使用这个模型生成文本。这将为我们在后续章节中探索LLM的训练和微调过程做好准备。
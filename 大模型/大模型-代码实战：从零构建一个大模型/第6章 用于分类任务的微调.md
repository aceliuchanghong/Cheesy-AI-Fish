# 第6章 用于分类任务的微调

本章主要内容:

- 介绍不同的LLM微调方法
- 准备用于文本分类的数据集
- 修改预训练LLM以进行微调
- 微调LLM以识别垃圾邮件
- 评估微调后的LLM分类器的准确性
- 使用微调后的LLM进行新数据分类

在前面的章节中,我们已经编码实现了LLM的架构,对其进行了预训练,并学习了如何将外部预训练权重(如OpenAI的权重)导入到我们的模型中。在本章中,我们将通过在特定目标任务(如文本分类)上微调LLM来收获我们之前努力的成果,如图6.1所示。我们将以分类垃圾短信和非垃圾短信为具体示例。

![Untitled](Images/大模型-从零构建一个大模型/第六章/Untitled.png)

图6.1展示了微调LLM的两种主要方式:用于分类的微调(步骤8)和用于遵循指令的微调(步骤9)。在下一节中,我们将详细讨论这两种微调方法。

6.1 不同类别的微调

微调语言模型最常见的方式是指令微调和分类微调。指令微调涉及在一组使用特定指令的任务上训练语言模型,以提高其理解和执行自然语言提示中描述的任务的能力,如图6.2所示。

![Untitled](Images/大模型-从零构建一个大模型/第六章/Untitled1.png)

在分类微调中,模型被训练以识别特定的类别标签,例如"垃圾邮件"或"非垃圾邮件"。分类任务的示例远不止语言模型,还包括从图像识别不同植物物种、对新闻文章进行主题分类(如体育、政治或技术),以及在医学影像中区分良性和恶性肿瘤。

分类微调模型的目标是预测类别,而不是生成下一个token。例如,它可以确定某条信息是"垃圾邮件"还是"非垃圾邮件",如图6.3所示,但不能说出关于输入的其他内容。

![Untitled](Images/大模型-从零构建一个大模型/第六章/Untitled2.png)
与图6.3中描述的分类微调模型相比,指令微调模型通常具有执行更广泛任务的能力。我们可以将分类微调模型视为高度专业化的,而不是通用的,因为开发专门模型比开发适用于各种任务的通用模型更容易。

选择正确的方法

指令微调提高了模型理解和生成基于特定任务说明的响应的能力。指令微调非常适合需要处理基于复杂文本指令的各种任务的模型,提高灵活性和交互质量。分类微调则非常适合需要将文本精确分类到预定义类别的项目,如情感分析或垃圾邮件检测。

虽然指令微调更通用,但它需要更大的数据集和更多的计算资源来开发能够适应各种任务的模型。相比之下,分类微调需要较少的数据和计算能力,但其目标仅限于您训练模型的特定类别。

为了更好地理解这两种微调方法的区别,让我们来看一个具体的例子:

假设我们有一个电子商务网站,需要处理大量的客户反馈。我们可以采用以下两种方法之一:

1. 分类微调方法:
我们可以将LLM微调为一个分类器,将客户反馈分为几个预定义的类别,如"产品质量"、"客户服务"、"配送"和"其他"。
    
    ```python
    def classify_feedback(feedback):
        # 使用微调后的分类模型
        category = model.predict(feedback)
        return category
    
    feedback = "我的包裹迟到了两天,很不满意。"
    result = classify_feedback(feedback)
    print(f"反馈类别: {result}")  # 输出: 反馈类别: 配送
    
    ```
    
2. 指令微调方法:
我们可以将LLM微调为能够理解和执行各种与客户反馈相关的任务的模型。
    
    ```python
    def process_feedback(instruction, feedback):
        # 使用指令微调后的模型
        response = model.generate(instruction + ": " + feedback)
        return response
    
    feedback = "我的包裹迟到了两天,很不满意。"
    
    # 分类任务
    result1 = process_feedback("将以下反馈分类", feedback)
    print(f"反馈类别: {result1}")  # 输出: 反馈类别: 配送
    
    # 情感分析任务
    result2 = process_feedback("分析以下反馈的情感", feedback)
    print(f"情感分析: {result2}")  # 输出: 情感分析: 负面
    
    # 生成回复任务
    result3 = process_feedback("为以下客户反馈生成适当的回复", feedback)
    print(f"生成的回复: {result3}")  # 输出: 生成的回复: 尊敬的客户,非常抱歉给您带来不便。我们将调查延迟原因并采取措施防止再次发生...
    
    ```
    

从这个例子中我们可以看到:

- 分类微调方法更简单,但功能有限,只能将反馈分类到预定义的类别中。
- 指令微调方法更灵活,可以执行多种任务,如分类、情感分析和生成回复,但需要更复杂的模型和更多的训练数据。

选择哪种方法取决于您的具体需求、可用资源和期望的模型复杂度。

6.2 准备数据集

在本章的剩余部分,我们将修改并分类微调我们在前几章中实现和预训练的GPT模型。我们将从下载和准备数据集开始,如图6.4所示。

![Untitled](Images/大模型-从零构建一个大模型/第六章/Untitled3.png)
为了提供一个直观且有用的分类微调示例,我们将使用一个短信数据集,其中包含垃圾短信和非垃圾短信。

这些短信通常来自手机或电子邮件。然而,相同的步骤也适用于电子邮件分类,感兴趣的读者可以在附录C的参考部分找到电子邮件垃圾分类数据集。

我们的第一步是下载数据集,使用以下代码:

```python
import urllib.request
import zipfile
import os
from pathlib import Path

url = "<https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip>"
zip_path = "sms_spam_collection.zip"
extracted_path = "sms_spam_collection"
data_file_path = Path(extracted_path) / "SMSSpamCollection.tsv"

def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):
    if data_file_path.exists():
        print(f"{data_file_path} already exists. Skipping download and extraction.")
        return

    with urllib.request.urlopen(url) as response:
        with open(zip_path, "wb") as out_file:
            out_file.write(response.read())

    with zipfile.ZipFile(zip_path, "r") as zip_ref:
        zip_ref.extractall(extracted_path)

    original_file_path = Path(extracted_path) / "SMSSpamCollection"
    os.rename(original_file_path, data_file_path)
    print(f"File downloaded and saved as {data_file_path}")

download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)

```

执行上述代码后,数据集将被保存为一个制表符分隔的文本文件SMSSpamCollection.tsv,位于sms_spam_collection文件夹中。我们可以将其读入一个pandas DataFrame如下:

```python
import pandas as pd
df = pd.read_csv(data_file_path, sep="\\t", header=None, names=["Label", "Text"])
df

```

结果如图6.5所示。

![Untitled](%E7%AC%AC6%E7%AB%A0%20%E7%94%A8%E4%BA%8E%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E7%9A%84%E5%BE%AE%E8%B0%83%20c06b843dd1024d469232a8284e27f15b/Untitled%204.png)

![https://www.notion.soimages/ch06/fig6.5.png](https://www.notion.soimages/ch06/fig6.5.png)

让我们检查一下类别标签的分布:

```python
print(df["Label"].value_counts())

```

执行上述代码,我们会看到"ham"(即非垃圾邮件)出现的频率远高于"spam":

```
Label
ham     4825
spam     747
Name: count, dtype: int64

```

出于说明目的,并因为我们偏好一个较小的数据集以便快速微调我们的大语言模型,我们选择对数据集进行下采样以包括747个来自每个类别的实例。虽然还有其他处理类别不平衡的方法,但这些超出了关于大型语言模型的书籍的范围。读者如果对探索处理不平衡类别的方法感兴趣可以在附录Y的参考部分找到更多信息。

我们使用以下代码对数据集进行下采样并创建一个平衡数据集:

```python
def create_balanced_dataset(df):
    num_spam = df[df["Label"] == "spam"].shape[0]
    ham_subset = df[df["Label"] == "ham"].sample(num_spam, random_state=123)
    balanced_df = pd.concat([ham_subset, df[df["Label"] == "spam"]])
    return balanced_df

balanced_df = create_balanced_dataset(df)
print(balanced_df["Label"].value_counts())

```

执行上述代码平衡我们的数据集后,我们可以看到我们现在有相等数量的垃圾邮件和非垃圾邮件消息:

```
Label
ham     747
spam    747
Name: count, dtype: int64

```

现在,我们将字符串类别标签"ham"和"spam"转换为整数类别标签0和1:

```python
balanced_df["Label"] = balanced_df["Label"].map({"ham": 0, "spam": 1})

```

这个过程类似于我们之前处理词元ID。但是,不同于使用GPT词汇表(包含超过50,000个单词),我们只处理两个词元ID:0和1。

我们创建一个random_split函数来将数据集分成三部分:70%用于训练,10%用于验证,20%用于测试。(这些比例在机器学习中很常见,用于训练、调整和评估模型。)

```python
def random_split(df, train_frac, validation_frac):
    df = df.sample(frac=1, random_state=123).reset_index(drop=True)

    train_end = int(len(df) * train_frac)
    validation_end = train_end + int(len(df) * validation_frac)

    train_df = df[:train_end]
    validation_df = df[train_end:validation_end]
    test_df = df[validation_end:]

    return train_df, validation_df, test_df

train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)

```

另外,我们将数据集保存为CSV(逗号分隔值)文件,我们稍后可以重用:

```python
train_df.to_csv("train.csv", index=None)
validation_df.to_csv("validation.csv", index=None)
test_df.to_csv("test.csv", index=None)

```

在本节中,我们下载了数据集,平衡了它,并将其分割为训练和评估子集。在下一节中,我们将介绍如何加载我们预先训练好的GPT模型,该模型将用于微调任务。

6.3 创建数据加载器

在本节中,我们将开发PyTorch数据加载器,这些加载器在概念上类似于我们在第2章中实现的数据加载器。

在第2章中,我们使用滑动窗口技术生成固定大小的文本块,然后将这些块分组成批次以进行更高效的模型训练。每个文本块都充当一个单独的训练实例。

然而,在本章中,我们正在处理一个垃圾短信数据集,其中包含长度不同的短信。要像我们在第2章中对文本块那样对这些消息进行批处理,我们有两个主要选项:

1. 截断所有消息至数据集中最短消息的长度以进行批处理。
2. 填充所有消息至数据集中最长消息的长度以进行批处理。

选项1在计算上更高效,但可能会导致信息损失,特别是对于短于平均或最长消息的消息,可能会降低模型性能。因此,我们将使用第二个选项,它保留了所有消息的全部内容。

为了实现选项2,即将所有消息填充到数据集中最长消息的长度,我们将使用填充标记补充所有较短的消息。为此,我们使用"<|endoftext|>"作为填充标记,如第2章中讨论的那样。

然而,我们不是直接将字符串"<|endoftext|>"附加到每个短消息的末尾,而是可以使用标记ID编码将"<|endoftext|>"添加到编码后的文本消息中,如图6.6所示。

![Untitled](Images/大模型-从零构建一个大模型/第六章/Untitled5.png)

图6.6假设50,256是填充标记"<|endoftext|>"的标记ID。我们可以通过使用tiktoken包中的GPT-2分词器对"<|endoftext|>"进行编码来验证这一点,如下所示:

```python
import tiktoken
tokenizer = tiktoken.get_encoding("gpt2")
print(tokenizer.encode("<|endoftext|>", allowed_special={"<|endoftext|>"}))

```

执行上述代码确实返回[50256]。

正如我们在第2章中看到的,我们首先需要实现一个PyTorch Dataset,它指定数据如何加载和处理,然后才能实例化我们的数据加载器。

为此目的,我们定义SpamDataset类,它实现图6.6中描述的概念。这个SpamDataset类处理几个关键任务:它识别训练数据集中最长的序列,编码文本消息,并确保所有其他序列都用填充标记填充以匹配最长序列的长度。

```python
import torch
from torch.utils.data import Dataset

class SpamDataset(Dataset):
    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):
        self.data = pd.read_csv(csv_file)
        self.encoded_texts = [
            tokenizer.encode(text) for text in self.data["Text"]
        ]

        if max_length is None:
            self.max_length = self._longest_encoded_length()
        else:
            self.max_length = max_length
            self.encoded_texts = [
                encoded_text[:self.max_length]
                for encoded_text in self.encoded_texts
            ]

        self.encoded_texts = [
            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))
            for encoded_text in self.encoded_texts
        ]

    def __getitem__(self, index):
        encoded = self.encoded_texts[index]
        label = self.data.iloc[index]["Label"]
        return (
            torch.tensor(encoded, dtype=torch.long),
            torch.tensor(label, dtype=torch.long)
        )

    def __len__(self):
        return len(self.data)

    def _longest_encoded_length(self):
        max_length = 0
        for encoded_text in self.encoded_texts:
            encoded_length = len(encoded_text)
            if encoded_length > max_length:
                max_length = encoded_length
        return max_length

```

SpamDataset类从我们之前创建的CSV文件加载数据,使用GPT-2分词器对文本进行标记化,并允许我们将序列截断到预定义的最大长度或以数据集中最长序列的长度为基准。这确保每个输入张量具有相同的大小,这是创建训练数据加载器所必需的。现在,我们可以如下创建训练数据集:

```python
train_dataset = SpamDataset(
    csv_file="train.csv",
    max_length=None,
    tokenizer=tokenizer
)

```

注意,最长序列长度存储在数据集的max_length属性中。如果你好奇最长序列中有多少个标记,你可以运行以下代码:

```python
print(train_dataset.max_length)

```

你可能会得到120,表示最长序列包含不超过120个标记,这是短信的常见长度。值得注意的是,该模型可以处理长达1,024个标记的序列,考虑到其上下文长度限制。如果你的数据集包含更长的文本,你可以在创建训练数据集时传递max_length=1024,以确保你的输入不超过模型的支持输入(上下文)长度。

现在,我们使用验证和测试数据来匹配最长训练序列的长度。请注意,如果验证和测试示例超过最长训练示例的长度,这些示例会被截断,这是通过我们之前定义的SpamDataset中的encoded_text[:self.max_length]来实现的。这种截断是可选的;你也可以为验证和测试数据使用max_length=None,只要这些数据中没有序列超过1,024个标记。

```python
val_dataset = SpamDataset(
    csv_file="validation.csv",
    max_length=train_dataset.max_length,
    tokenizer=tokenizer
)
test_dataset = SpamDataset(
    csv_file="test.csv",
    max_length=train_dataset.max_length,
    tokenizer=tokenizer
)

```

练习 6.1 增加上下文长度

尝试将输入设置为模型支持的最大标记数,并观察它如何影响感知性能。

有了数据集作为输入,我们现在可以实例化数据加载器,类似于我们在第2章中所做的。然而,在这种情况下,目标表示类别标签而不是下一个标记。例如,假设批次大小为8,每个批次将包含8个长度为120的训练示例和每个示例相应的类别标签,如图6.7所示。

![Untitled](Images/大模型-从零构建一个大模型/第六章/Untitled6.png)
以下代码创建训练、验证和测试数据加载器,这些加载器将文本消息和标签批量加载,每批8个,如图6.7所示:

```python
from torch.utils.data import DataLoader

num_workers = 0
batch_size = 8
torch.manual_seed(123)

train_loader = DataLoader(
    dataset=train_dataset,
    batch_size=batch_size,
    shuffle=True,
    num_workers=num_workers,
    drop_last=True,
)
val_loader = DataLoader(
    dataset=val_dataset,
    batch_size=batch_size,
    num_workers=num_workers,
    drop_last=False,
)
test_loader = DataLoader(
    dataset=test_dataset,
    batch_size=batch_size,
    num_workers=num_workers,
    drop_last=False,
)

```

为确保我们的数据加载器正常工作并产生预期大小的批次,我们迭代训练加载器并打印第一个批次的张量维度:

```python
for input_batch, target_batch in train_loader:
    pass
print("Input batch dimensions:", input_batch.shape)
print("Label batch dimensions", target_batch.shape)

```

输出如下:

```
Input batch dimensions: torch.Size([8, 120])
Label batch dimensions torch.Size([8])

```

正如我们所看到的,输入批次由8个训练示例组成,每个示例有120个标记,如预期的那样。标签张量存储了与8个训练示例对应的类别标签。

最后,为了对我们的数据集有个概览,让我们打印每个数据集中的批次总数:

```python
print(f"{len(train_loader)} training batches")
print(f"{len(val_loader)} validation batches")
print(f"{len(test_loader)} test batches")

```

每个数据集中的批次数如下:

```
130 training batches
19 validation batches
38 test batches

```

这就结束了本章的数据准备部分。接下来,我们将准备我们的模型进行微调。

6.4 用预训练权重初始化模型

在本节中,我们准备我们将用于分类微调以识别垃圾短信的模型。我们从初始化我们在前一章中使用的预训练模型开始,如图6.8所示。

![Untitled](Images/大模型-从零构建一个大模型/第六章/Untitled7.png)
我们从使用第5章的配置开始模型准备过程:

```python
CHOOSE_MODEL = "gpt2-small (124M)"
INPUT_PROMPT = "Every effort moves"
BASE_CONFIG = {
    "vocab_size": 50257,     # 词汇表大小
    "context_length": 1024,  # 上下文长度
    "drop_rate": 0.0,        # Dropout率
    "qkv_bias": True         # 查询-键-值偏置
}
model_configs = {
    "gpt2-small (124M)": {"emb_dim": 768, "n_layers": 12, "n_heads": 12},
    "gpt2-medium (355M)": {"emb_dim": 1024, "n_layers": 24, "n_heads": 16},
    "gpt2-large (774M)": {"emb_dim": 1280, "n_layers": 36, "n_heads": 20},
    "gpt2-xl (1558M)": {"emb_dim": 1600, "n_layers": 48, "n_heads": 25},
}
BASE_CONFIG.update(model_configs[CHOOSE_MODEL])

assert train_dataset.max_length <= BASE_CONFIG["context_length"], (
    f"Dataset length {train_dataset.max_length} exceeds model's context "
    f"length {BASE_CONFIG['context_length']}. Reinitialize data sets with "
    f"`max_length={BASE_CONFIG['context_length']}`"
)

```

这里,我们从第5章下载的gpt_download.py文件中导入download_and_load_gpt2函数。此外,我们还从第5章中复用GPTModel类和load_weights_into_gpt函数,以将下载的权重加载到GPT模型中:

```python
from gpt_download import download_and_load_gpt2
from chapter05 import GPTModel, load_weights_into_gpt

model_size = CHOOSE_MODEL.split(" ")[-1].lstrip("(").rstrip(")")
settings, params = download_and_load_gpt2(model_size=model_size, models_dir="gpt2")

model = GPTModel(BASE_CONFIG)
load_weights_into_gpt(model, params)
model.eval()

```

在将模型权重加载到GPTModel之前,我们可以使用前几章的文本生成实用函数来确保我们的模型生成连贯的文本:

```python
from chapter04 import generate_text_simple
from chapter05 import text_to_token_ids, token_ids_to_text

text_1 = "Every effort moves you"
token_ids = generate_text_simple(
    model=model,
    idx=text_to_token_ids(text_1, tokenizer),
    max_new_tokens=15,
    context_size=BASE_CONFIG["context_length"]
)
print(token_ids_to_text(token_ids, tokenizer))

```

正如我们可以从以下输出看到的,模型生成连贯的文本,这表明模型权重已正确加载:

```
Every effort moves you forward.
The first step is to understand the importance of your work

```

现在,在我们开始将模型微调为垃圾分类器之前,让我们看看模型是否已经可以通过提示它来分类垃圾消息:

```python
text_2 = (
    "Is the following text 'spam'? Answer with 'yes' or 'no':"
    " 'You are a winner you have been specially"
    " selected to receive $1000 cash or a $2000 award.'"
)
token_ids = generate_text_simple(
    model=model,
    idx=text_to_token_ids(text_2, tokenizer),
    max_new_tokens=23,
    context_size=BASE_CONFIG["context_length"]
)
print(token_ids_to_text(token_ids, tokenizer))

```

模型输出如下:

```
Is the following text 'spam'? Answer with 'yes' or 'no': 'You are a winner you have been specially selected to receive $1000 cash or a $2000 award.'
The following text 'spam'? Answer with '
```

基于上一节的输出,很明显模型还不能很好地理解并回答我们的指令。这是可以预料的,因为它只经过了预训练,还没有经过指令微调,这是我们将在下一章探讨的内容。

下一节将为分类微调准备模型。

6.5 添加分类头

在本节中,我们修改预训练的大语言模型以准备它进行分类微调。为此,我们将原始输出层(将隐藏表示映射到50,257个词汇表)替换为一个简单的输出层,该层映射到两个类别:0("非垃圾邮件")和1("垃圾邮件"),如图6.9所示。

![Untitled](Images/大模型-从零构建一个大模型/第六章/Untitled8.png)

如图6.9所示,我们使用与前几章相同的主要模型,只是替换了输出层。

输出层节点数

理论上,我们可以使用单个输出节点,因为我们处理的是二元分类任务。然而,这将需要修改损失函数,如附录T参考部分的一篇文章中所讨论的。因此,我们选择一个更通用的方法,即输出节点的数量与类别数量相匹配。例如,对于一个3类问题,比如将新闻文章分类为"科技"、"体育"或"政治",我们将使用三个输出节点,依此类推。

在尝试图6.9中描述的修改之前,让我们打印模型架构。运行print(model)会打印以下内容:

```
GPTModel(
  (tok_emb): Embedding(50257, 768)
  (pos_emb): Embedding(1024, 768)
  (drop_emb): Dropout(p=0.0, inplace=False)
  (trf_blocks): Sequential(
...
    (11): TransformerBlock(
      (att): MultiHeadAttention(
        (W_query): Linear(in_features=768, out_features=768, bias=True)
        (W_key): Linear(in_features=768, out_features=768, bias=True)
        (W_value): Linear(in_features=768, out_features=768, bias=True)
        (out_proj): Linear(in_features=768, out_features=768, bias=True)
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (ff): FeedForward(
        (layers): Sequential(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU()
          (2): Linear(in_features=3072, out_features=768, bias=True)
        )
      )
      (norm1): LayerNorm()
      (norm2): LayerNorm()
      (drop_resid): Dropout(p=0.0, inplace=False)
    )
  )
  (final_norm): LayerNorm()
  (out_head): Linear(in_features=768, out_features=50257, bias=False)
)

```

上面我们可以看到我们在第4章中实现的架构完全符合。如第4章中讨论的,GPTModel由嵌入层组成,后跟12个相同的Transformer块(为简洁起见,仅显示最后一个块),然后是最终的LayerNorm和输出层out_head。

现在,我们将out_head替换为一个新的输出层,如图6.9所示,我们将对其进行微调。

微调选定层与全部层

由于我们从预训练模型开始,不必微调所有模型层。这是因为,在基于神经网络的语言模型中,较低层通常捕获基本语言结构和语义,这些可以适用于广泛的任务和数据集。因此,微调仅最后几层(接近输出的层),这些层很可能捕获任务特定的模式和更特定的特征,通常就足以适应模型到新任务。实际上,这种方法在计算上更加高效,因为只需要微调少量层。感兴趣的读者可以在本章附录R的参考部分找到更多信息,包括关于微调哪些层的实验。

要为分类微调准备模型,我们首先冻结模型,意味着我们使所有层不可训练:

```python
for param in model.parameters():
    param.requires_grad = False

```

然后,如图6.9所示,我们替换输出层(model.out_head),该层原本将层输入映射到50,257维(词汇表的大小):

```python
torch.manual_seed(123)
num_classes = 2
model.out_head = torch.nn.Linear(
    in_features=BASE_CONFIG["emb_dim"],
    out_features=num_classes
)

```

注意,在上面的代码中,我们使用BASE_CONFIG["emb_dim"],它在"gpt2-small (124M)"模型中等于768,以使代码更加通用。这意味着我们可以使用相同的代码来使用较大的GPT-2模型变体。

这个新model.out_head输出层的requires_grad属性默认设置为True,这意味着它是模型中唯一在训练期间会更新的层。

理论上,训练我们刚刚添加的输出层就足够了。然而,正如我在实验中发现的,微调额外的层可能会显著提高微调模型的预测性能。(有关更多细节,请参阅附录X中的参考文献。)

此外,我们将最后一个Transformer块和最终LayerNorm模块(它连接此块到输出层)设置为可训练,如图6.10所示。

![Untitled](Images/大模型-从零构建一个大模型/第六章/Untitled9.png)

要使最终LayerNorm和最后一个Transformer块可训练,如图6.10所示,我们将它们各自的requires_grad设置为True:

```python
for param in model.trf_blocks[-1].parameters():
    param.requires_grad = True
for param in model.final_norm.parameters():
    param.requires_grad = True

```

练习 6.2 微调整个模型

尝试微调整个模型,而不是只微调最后一个Transformer块,并评估其对预测性能的影响。

尽管我们添加了一个新的输出层并将某些层标记为可训练或不可训练,我们仍然可以以与前几章类似的方式使用模型。例如,我们可以看一个与我们在早期章节中看到的相同的示例。考虑以下示例输入:

```python
inputs = tokenizer.encode("Do you have time")
inputs = torch.tensor(inputs).unsqueeze(0)
print("Inputs:", inputs)
print("Inputs dimensions:", inputs.shape) # shape: (batch_size, num_tokens)

```

如输出所示,上面的代码将输入编码为一个包含4个输入标记的张量:

```
Inputs: tensor([[5211,  345,  423,  640]])
Inputs dimensions: torch.Size([1, 4])

```

现在,我们可以像往常一样将编码的标记ID传递给模型:

```python
with torch.no_grad():
    outputs = model(inputs)
print("Outputs:\\n", outputs)
print("Outputs dimensions:", outputs.shape) # shape: (batch_size, num_tokens, num_classes)

```

输出张量如下所示:

```
Outputs:
 tensor([[[-1.5854,  0.9904],
          [-3.7235,  7.4548],
          [-2.2661,  6.6049],
          [-3.5983,  3.9902]]])
Outputs dimensions: torch.Size([1, 4, 2])

```

在第4和第5章中,类似的输入会产生一个形状为[1, 4, 50257]的输出张量,其中50,257代表词汇表大小。与前几章一样,输出行数对应于输入标记的数量(在本例中为4)。然而,现在输出的嵌入维度(列数)减少到2,而不是50,257,因为我们替换了模型的输出层。

请记住,我们对微调这个模型感兴趣,以便它返回一个类别标签,指示模型输入是垃圾邮件还是非垃圾邮件。为了实现这一点,我们不需要考虑所有4个输出行,而可以专注于单个输出标记。特别是,我们将关注与最后一个输出标记对应的行,如图6.11所示。

![Untitled](Images/大模型-从零构建一个大模型/第六章/Untitled10.png)

要从输出张量中提取最后一个输出标记(如图6.11所示),我们使用以下代码:

```python
print("Last output token:", outputs[:, -1, :])

```

这将打印以下内容:

```
Last output token: tensor([[-3.5983,  3.9902]])

```

在我们继续下一部分之前,让我们暂停讨论。我们将专注于将这些值转换为类别标签预测。首先,让我们理解为什么我们特别对最后一个输出标记感兴趣,而不是第1个、第2个或第3个输出标记。

在第3章中,我们探讨了注意力机制,它建立了每个输入标记与每个其他输入标记之间的关系。随后,我们引入了因果注意力掩码的概念,这在GPT类模型中很常见。这个掩码限制了标记的注意力范围到它的当前位置和之前的位置,确保每个标记只能被自身和前面的标记影响,如图6.12所示。

![Untitled](Images/大模型-从零构建一个大模型/第六章/Untitled11.png)

考虑到图6.12中显示的因果注意力掩码设置,序列中的最后一个标记计算了最多的信息,因为它是唯一一个有权访问所有前面标记的标记。因此,在我们的分类任务中,我们在微调过程中关注这个最后的标记。

现在我们已经修改了模型,下一节将详细介绍将最后一个标记转换为类别标签预测的过程,并计算模型的初始预测准确性。之后,我们将在随后的部分中微调模型进行垃圾邮件分类任务。

练习 6.3 微调第一个与最后一个标记

尝试微调第一个输出标记而不是最后一个输出标记,并在稍后部分微调模型时观察预测性能的变化。

6.6 计算分类损失和准确性

到目前为止在本章中,我们已经准备了数据集,加载了预训练模型,并修改它以进行分类微调。在我们继续进行微调本身之前,还有一个小步骤:实现在微调期间使用的模型评估函数,如图6.13所示。我们将在本节中完成这一步。

![Untitled](Images/大模型-从零构建一个大模型/第六章/Untitled12.png)

在实现评估工具之前,让我们简要讨论一下我们如何将模型输出转换为类别标签预测。

在前一章中,我们通过将50,257个输出转换为概率(使用softmax函数)然后返回最高概率的位置来计算GPT生成的下一个标记的标记ID。在本章中,我们采用相同的方法来计算模型是否输出"垃圾邮件"或"非垃圾邮件"预测给定输入,如图6.14所示,唯一的区别是我们现在处理2维而不是50,257维输出。

![Untitled](Images/大模型-从零构建一个大模型/第六章/Untitled13.png)

为了用一个具体的例子说明图6.14,让我们考虑上一节中最后一个标记的输出:

```python
print("Last output token:", outputs[:, -1, :])

```

对应于最后一个标记的张量值如下所示:

```
Last output token: tensor([[-3.5983,  3.9902]])
```

我们可以通过以下代码获得类别标签:

```python
probas = torch.softmax(outputs[:, -1, :], dim=-1)
label = torch.argmax(probas)
print("Class label:", label.item())
```

在这种情况下,输出会返回1,意味着模型预测输入文本是"垃圾邮件"。请注意,使用softmax函数是可选的,因为最大输出直接对应于最高概率分数,如第5章所述。因此,我们可以简化代码如下:

```python
logits = outputs[:, -1, :]
label = torch.argmax(logits)
print("Class label:", label.item())

```

这个概念可以用来计算所谓的分类准确率,它衡量整个数据集中正确预测的百分比。

为了确定分类准确率,我们将基于argmax的预测规则应用于数据集中的所有示例,并计算正确预测的比例,方法是定义一个calc_accuracy_loader函数:

```python
def calc_accuracy_loader(data_loader, model, device, num_batches=None):
    model.eval()
    correct_predictions, num_examples = 0, 0

    if num_batches is None:
        num_batches = len(data_loader)
    else:
        num_batches = min(num_batches, len(data_loader))
    for i, (input_batch, target_batch) in enumerate(data_loader):
        if i < num_batches:
            input_batch, target_batch = input_batch.to(device), target_batch.to(device)

            with torch.no_grad():
                logits = model(input_batch)[:, -1, :]
            predicted_labels = torch.argmax(logits, dim=-1)

            num_examples += predicted_labels.shape[0]
            correct_predictions += (predicted_labels == target_batch).sum().item()
        else:
            break
    return correct_predictions / num_examples

```

让我们使用这个函数来确定各种数据集的分类准确率,使用10个批次以提高效率:

```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

torch.manual_seed(123)
train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)
val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)
test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)

print(f"Training accuracy: {train_accuracy*100:.2f}%")
print(f"Validation accuracy: {val_accuracy*100:.2f}%")
print(f"Test accuracy: {test_accuracy*100:.2f}%")

```

对于device设置,模型会自动在GPU上运行(如果有CUDA GPU支持)否则在CPU上运行。输出如下:

```
Training accuracy: 46.25%
Validation accuracy: 45.00%
Test accuracy: 48.75%

```

正如我们所看到的,预测准确率仅比随机预测(在这种情况下为50%)略低。为了提高预测准确率,我们需要微调模型。

然而,在我们开始微调模型之前,我们需要定义损失函数,我们将在训练过程中优化它。我们的目标是最大化模型的垃圾分类准确率,这意味着预测应该输出正确的类别标签:0表示非垃圾邮件,1表示垃圾邮件。

然而,分类准确率不是可微分函数,因此我们无法直接使用它作为优化准确率的度量。这就是为什么我们使用在第5章中讨论的交叉熵损失。

相应地,calc_loss_batch函数保持与第5章相同,只有两个调整:我们专注于优化仅最后一个标记,model(input_batch)[:, -1, :],而不是所有标记:

```python
def calc_loss_batch(input_batch, target_batch, model, device):
    input_batch, target_batch = input_batch.to(device), target_batch.to(device)
    logits = model(input_batch)[:, -1, :]  # 最后输出标记的Logits
    loss = torch.nn.functional.cross_entropy(logits, target_batch)
    return loss

```

我们使用calc_loss_batch函数来计算从先前定义的数据加载器获得的单个批次的损失。要计算数据加载器中所有批次的损失,我们定义了calc_loss_loader函数,它与第5章中讨论的函数相同:

```python
def calc_loss_loader(data_loader, model, device, num_batches=None):
    total_loss = 0.
    if len(data_loader) == 0:
        return float("nan")
    elif num_batches is None:
        num_batches = len(data_loader)
    else:
        num_batches = min(num_batches, len(data_loader))
    for i, (input_batch, target_batch) in enumerate(data_loader):
        if i < num_batches:
            loss = calc_loss_batch(input_batch, target_batch, model, device)
            total_loss += loss.item()
        else:
            break
    return total_loss / num_batches

```

类似于计算训练准确率,我们现在计算每个数据集的初始损失:

```python
with torch.no_grad():
    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)
    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)
    test_loss = calc_loss_loader(test_loader, model, device, num_batches=5)

print(f"Training loss: {train_loss:.3f}")
print(f"Validation loss: {val_loss:.3f}")
print(f"Test loss: {test_loss:.3f}")

```

初始损失值如下:

```
Training loss: 3.095
Validation loss: 2.583
Test loss: 2.322

```

在下一节中,我们将实现一个训练函数来微调模型,这意味着调整模型以最小化训练集损失。最小化训练集损失将有助于提高分类准确率,反之亦然。

6.7 在监督数据上微调模型

在本节中,我们定义并应用训练函数来微调我们的预训练GPT并提高其垃圾邮件分类准确率。训练循环(如图6.15所示)是我们在第5章中使用的整体训练循环,唯一的区别是我们计算分类准确率而不是生成一个样本文本来评估模型。

![Untitled](Images/大模型-从零构建一个大模型/第六章/Untitled14.png)

实现图6.15中所示概念的训练函数与我们在第5章中用于预训练模型的train_model_simple函数非常相似。

这里有两个主要区别:我们现在跟踪看到的训练示例数量(examples_seen)而不是标记数,我们在每个周期之后计算准确率而不是打印一个样本文本:

```python
def train_classifier_simple(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, tokenizer):
    # 初始化列表以跟踪损失和看到的示例
    train_losses, val_losses, train_accs, val_accs = [], [], [], []
    examples_seen, global_step = 0, -1

    # 主训练循环
    for epoch in range(num_epochs):
        model.train()

        for input_batch, target_batch in train_loader:
            optimizer.zero_grad()
            loss = calc_loss_batch(input_batch, target_batch, model, device)
            loss.backward()
            optimizer.step()
            examples_seen += input_batch.shape[0]
            global_step += 1

            if global_step % eval_freq == 0:
                train_loss, val_loss = evaluate_model(
                    model, train_loader, val_loader, device, eval_iter)
                train_losses.append(train_loss)
                val_losses.append(val_loss)
                print(f"Ep {epoch+1} (Step {global_step:06d}): "
                      f"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}")

        train_accuracy = calc_accuracy_loader(
            train_loader, model, device, num_batches=eval_iter
        )
        val_accuracy = calc_accuracy_loader(
            val_loader, model, device, num_batches=eval_iter
        )

        print(f"Training accuracy: {train_accuracy*100:.2f}% | ", end="")
        print(f"Validation accuracy: {val_accuracy*100:.2f}%")
        train_accs.append(train_accuracy)
        val_accs.append(val_accuracy)

    return train_losses, val_losses, train_accs, val_accs, examples_seen

```

前面train_classifier_simple中使用的evaluate_model函数与我们在第5章中有的相同:

```python
def evaluate_model(model, train_loader, val_loader, device, eval_iter):
    model.eval()
    with torch.no_grad():
        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)
        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)
    model.train()
    return train_loss, val_loss

```

现在,我们初始化优化器,设置训练周期数,并使用train_classifier_simple函数开始训练。我们将在评估结果后讨论我们对训练周期数的选择。训练在一台W3 ThinkPad X1笔记本电脑上大约需要6分钟,在V100或T100 GPU上需要不到一分钟:

```python
import time

start_time = time.time()
torch.manual_seed(123)
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)
num_epochs = 5

train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(
    model, train_loader, val_loader, optimizer, device,
    num_epochs=num_epochs, eval_freq=50, eval_iter=5,
    tokenizer=tokenizer
)

end_time = time.time()
execution_time_minutes = (end_time - start_time) / 60
print(f"Training completed in {execution_time_minutes:.2f} minutes.")

```

我们在训练过程中看到的输出如下:

```
Ep 1 (Step 000000): Train loss 2.153, Val loss 2.392
Ep 1 (Step 000050): Train loss 0.617, Val loss 0.637
Ep 1 (Step 000100): Train loss 0.523, Val loss 0.557
Training accuracy: 70.00% | Validation accuracy: 72.50%
Ep 2 (Step 000150): Train loss 0.561, Val loss 0.489
Ep 2 (Step 000200): Train loss 0.419, Val loss 0.397
Ep 2 (Step 000250): Train loss 0.409, Val loss 0.353
Training accuracy: 82.50% | Validation accuracy: 85.00%
Ep 3 (Step 000300): Train loss 0.333, Val loss 0.320
Ep 3 (Step 000350): Train loss 0.340, Val loss 0.306
Training accuracy: 90.00% | Validation accuracy: 90.00%
Ep 4 (Step 000400): Train loss 0.136, Val loss 0.200
Ep 4 (Step 000450): Train loss 0.153, Val loss 0.132
Ep 4 (Step 000500): Train loss 0.222, Val loss 0.137
Training accuracy: 100.00% | Validation accuracy: 97.50%
Ep 5 (Step 000550): Train loss 0.207, Val loss 0.143
Ep 5 (Step 000600): Train loss 0.083, Val loss 0.074
Training accuracy: 100.00% | Validation accuracy: 97.50%
Training completed in 5.65 minutes.

```

类似于第5章,我们现在添加绘图功能来可视化训练和验证损失:

```python
import matplotlib.pyplot as plt

def plot_values(epochs_seen, examples_seen, train_values, val_values, label="loss"):
    fig, ax1 = plt.subplots(figsize=(5, 3))

    ax1.plot(epochs_seen, train_values, label=f"Training {label}")
    ax1.plot(epochs_seen, val_values, linestyle="-.", label=f"Validation {label}")
    ax1.set_xlabel("Epochs")
    ax1.set_ylabel(label.capitalize())
    ax1.legend()

    ax2 = ax1.twiny()
    ax2.plot(examples_seen, train_values, alpha=0)  # 不可见的绘图用于对齐刻度
    ax2.set_xlabel("Examples seen")

    fig.tight_layout()
    plt.savefig(f"{label}-plot.pdf")
    plt.show()

epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))
examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))

plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)

```

结果损失曲线如图6.16所示。

![Untitled](Images/大模型-从零构建一个大模型/第六章/Untitled15.png)

![https://www.notion.soimages/ch06/fig6.16.png](https://www.notion.soimages/ch06/fig6.16.png)

正如我们可以从图6.16中看到的急剧下降的损失,模型正在从训练数据中学习,并且几乎没有或没有过拟合的迹象;也就是说,训练和验证损失之间没有明显的分歧)。

选择周期数

早些时候,当我们开始训练时,我们将周期数设置为5。周期数取决于数据集和任务的难度,没有通用的解决方案或建议。5个周期通常是一个很好的起点。如果模型在前几个周期后停止改进,如图6.16所示的损失曲线可能表明,我们可能想要减少周期数。相反,如果趋势表明验证损失可能通过进一步训练而改善,我们应该增加周期数。在这个具体案例中,5个周期似乎是一个合理的数字,因为没有明显的过拟合迹象,并且验证损失接近于0。

使用相同的plot_values函数,让我们现在绘制分类准确率:

```python
epochs_tensor = torch.linspace(0, num_epochs, len(train_accs))
examples_seen_tensor = torch.linspace(0, examples_seen, len(train_accs))

plot_values(epochs_tensor, examples_seen_tensor, train_accs, val_accs, label="accuracy")

```

结果准确率图如图6.17所示。

![Untitled](Images/大模型-从零构建一个大模型/第六章/Untitled16.png)

根据图6.17中的准确率图,模型在第4和第5个周期后达到了相对较高的训练和验证准确率。

然而,重要的是要注意,我们之前在使用train_classifier_simple函数时设置了eval_iter=5,这意味着训练和验证性能的估计是基于每个周期期间仅5个批次的效率。

现在,我们将通过运行以下代码来计算整个数据集的训练、验证和测试性能指标,这次不指定eval_iter值:

```python
train_accuracy = calc_accuracy_loader(train_loader, model, device)
val_accuracy = calc_accuracy_loader(val_loader, model, device)
test_accuracy = calc_accuracy_loader(test_loader, model, device)

print(f"Training accuracy: {train_accuracy*100:.2f}%")
print(f"Validation accuracy: {val_accuracy*100:.2f}%")
print(f"Test accuracy: {test_accuracy*100:.2f}%")

```

结果准确率值如下:

```
Training accuracy: 97.21%
Validation accuracy: 97.32%
Test accuracy: 95.67%

```

训练和测试集的性能几乎相同。

训练和测试集准确率之间的轻微差异表明对训练集的最小过拟合。通常,验证集准确率会略高于测试集准确率,因为模型开发经常涉及调整超参数以在验证集上表现良好,这可能不会同样有效地泛化到测试集。

这种情况很常见,可以通过调整模型的设置来潜在地最小化,例如增加dropout率(drop_rate)或优化器配置中的weight_decay参数。

6.8 使用LLM作为垃圾邮件分类器

在上一节中微调和评估模型后,我们现在处于本章的最后阶段,如图6.18所示:使用模型来分类垃圾邮件。

![Untitled](Images/大模型-从零构建一个大模型/第六章/Untitled17.png)

最后,让我们使用微调后的GPT基垃圾邮件分类模型。以下classify_review函数遵循与我们在本章早期在SpamDataset中实现的类似的预处理步骤。然后,函数使用模型预测一个整数类别标签,类似于我们在6.6节中实现的方式,并返回相应的类别名称:

```python
def classify_review(text, model, tokenizer, device, max_length=None, pad_token_id=50256):
    model.eval()

    input_ids = tokenizer.encode(text)
    supported_context_length = model.pos_emb.weight.shape[1]

    input_ids = input_ids[:min(max_length, supported_context_length)]

    input_ids += [pad_token_id] * (max_length - len(input_ids))
    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0)

    with torch.no_grad():
        logits = model(input_tensor)[:, -1, :]
    predicted_label = torch.argmax(logits, dim=-1).item()

    return "spam" if predicted_label == 1 else "not spam"

```

让我们在一个示例文本上尝试这个classify_review函数:

```python
text_1 = (
    "You are a winner you have been specially"
    " selected to receive $1000 cash or a $2000 award."
)

print(classify_review(
    text_1, model, tokenizer, device, max_length=train_dataset.max_length
))

```

结果模型正确预测"spam"。现在,让我们尝试另一个示例:

```python
text_2 = (
    "Hey, just wanted to check if we're still on"
    " for dinner tonight? Let me know!"
)

print(classify_review(
    text_2, model, tokenizer, device, max_length=train_dataset.max_length
))

```

同样,模型做出正确预测并返回"not spam"标签。

最后,让我们保存模型,以便我们以后可以重用模型而无需再次训练它,使用我们在上一章中介绍的torch.save方法。

```python
torch.save(model.state_dict(), "review_classifier.pth")

```

一旦保存,可以按如下方式加载模型:

```python
model_state_dict = torch.load("review_classifier.pth")
model.load_state_dict(model_state_dict)

```

6.9 总结

- 有不同的策略来微调LLM,包括分类微调(本章)和指令微调(下一章)
- 分类微调涉及通过小型分类层替换LLM的输出层。
- 在将文本消息分类为"垃圾邮件"或"非垃圾邮件"的情况下,新的分类层仅包含2个输出节点;在前几章中,输出节点的数量等于词汇表中唯一标记的数量,即50,256
- 与预训练时预测文本中的下一个标记不同,分类微调训练模型输出正确的类别标签,例如"垃圾邮件"或"非垃圾邮件"。
- 微调的模型输入是转换为标记ID的文本,类似于预训练。
- 在微调LLM之前,我们加载预训练模型作为基础模型。
- 评估分类模型涉及计算分类准确率(正确预测的比例或百分比)。
- 微调分类模型使用与预训练LLM相同的交叉熵损失函数。

在本章中,我们深入探讨了如何将预训练的大型语言模型(LLM)微调为一个专门的文本分类器。我们以垃圾邮件检测为例,展示了从数据准备到模型评估的整个过程。这种方法不仅适用于垃圾邮件检测,还可以扩展到各种其他文本分类任务,如情感分析、主题分类等。

通过这个过程,我们看到了如何利用预训练模型的强大语言理解能力,并将其定向到特定的分类任务。这种方法的优势在于,我们可以利用在大规模语料库上预训练的模型的知识,同时只需要相对较小的标记数据集就能达到高性能。

在下一章中,我们将探讨另一种微调方法 - 指令微调,它将使我们的模型能够理解和执行更广泛的自然语言指令。这将进一步扩展我们的LLM的能力,使其成为更加灵活和通用的工具。
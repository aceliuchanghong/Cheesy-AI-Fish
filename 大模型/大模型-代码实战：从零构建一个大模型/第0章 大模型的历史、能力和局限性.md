![https://cdn.nlark.com/yuque/0/2023/jpeg/406504/1693817685805-fcca58f7-8f27-4fab-bf70-2837848d6c6c.jpeg](https://cdn.nlark.com/yuque/0/2023/jpeg/406504/1693817685805-fcca58f7-8f27-4fab-bf70-2837848d6c6c.jpeg)

自从 ChatGPT 公开亮相以来，大语言模型（或 LLM）已经出现了一个高光时刻。这些巨大的神经网络的潜力让公众既兴奋又害怕。仿佛一夜之间从原始人进化到现代人的恍惚感。但什么是大语言模型？大语言模型是怎么被创造出来的？又是如何工作？如何才能让它们更好地工作？

### 目录

1. 什么是大语言模型 (LLM)？
2. 大语言模型简史
3. 关键术语
4. 大语言模型能做什么？
5. 大型语言模型的新兴能力是什么？
6. 如何提高广义大语言模型的性能
7. 如何使大型语言模型适应特定任务

# 什么是大语言模型 (LLM)？

大型语言模型就是一种生成文本的基础模型 （foundation models）。他们生成的文本可以通过给他们一个起点或“提示”来调节，使他们能够解决用自然语言或代码表达的有用任务。

数据科学家和研究人员通过自监督学习利用大量非结构化数据进行大语言模型（LLM）训练。在训练过程中，模型接受缺少一个或多个单词的单词序列。然后，模型会预测缺失的单词，该过程不仅会为模型生成一组有价值的权重，而且会为输入到模型的每个单词生成一个词嵌入（embedding或称词向量）。

在推理时，用户向 LLM 提供“提示”——模型用作起点的文本片段。首先，模型将提示中的每个标记转换为对应的词向量。然后，它使用这些词向量来预测可能出现的所有标记的相对可能性。然后，它半随机地选择下一个标记，并重复此过程，直到模型选择 STOP 标记。

可以将其想象为一条从零延伸到一的数轴。从左边开始，大语言模型会将标记的概率从大到小堆叠起来。该行的第一块（从 0 到 0.01）可能是“hello”。第二个块，从 0.01 到 0.019 可能是“world”，依此类推。然后，模型在该数轴上选择一个随机点并返回与其关联的标记。在实践中，大语言模型通常仅限于具有相对较高可能性的标记。

# 大语言模型简史

大语言模型源于神经网络的研究和实验，使计算机能够处理自然语言。自然语言处理的根源可以追溯到 20 世纪 50 年代，当时IBM 和乔治城大学的研究人员 开发了一种系统，可以自动将一组短语从俄语翻译成英语。此后的几十年里，研究人员尝试了多种不同的方法，包括概念本体和基于规则的系统，但没有一个产生可靠的结果。

直到20世纪末期，这项研究与当时蓬勃发展的神经网络领域交叉，为第一个大型语言模型奠定了基础。

### BERT，第一个突破性的大型语言模型

2019 年，谷歌的一个研究团队推出了BERT （代表来自 Transformer 的双向编码器表示）。

他们的新模型将多种想法组合成令人惊讶的简单而强大的东西。通过使 BERT 成为双向的，它允许输入和输出考虑彼此的上下文。通过使用一致的神经网络架构，研究人员使模型能够适应各种任务。而且，通过对各种非结构化数据以自我监督的方式对 BERT 进行预训练，研究人员创建了一个能够充分理解单词之间关系的模型。

所有这些都使得研究人员和从业者可以轻松使用 BERT。正如最初的研究人员所解释的那样，“只需一个额外的输出层即可对预训练的 BERT 模型进行微调，从而为各种任务创建最先进的模型。”

BERT 首次亮相就打破了一系列 NLP 基准测试的记录。短时间内，BERT 成为 NLP 任务的标准工具。推出后不到 18 个月，BERT 就为 Google 搜索几乎所有英语查询提供了支持。

### 比 BERT 更大

首次亮相时，BERT 的 3.4 亿个参数使其成为同类中最大的语言模型。这是经过深思熟虑的选择；研究人员希望它具有与 GPT 相同数量的参数，以简化性能比较。

从 2018 年到现在，NLP 研究人员一直在稳步向更大的模型迈进。Hugging Face 的 Julien Simon 将这种稳定增长称为“[新摩尔定律](https://huggingface.co/blog/large-language-models)”。

![https://cdn.nlark.com/yuque/0/2023/png/406504/1693817685126-2df61104-5bfd-4d15-8ecd-7c9a36ee9a57.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1693817685126-2df61104-5bfd-4d15-8ecd-7c9a36ee9a57.png)

随着大语言模型的发展，Bert和GPT也得到了改进。OpenAI 的 GPT-2 于 2019 年最终确定，拥有 15 亿个参数，其生成内容的质量令人惊叹。GPT-2的出色表现让OpenAI犹豫不决；该公司于当年 2 月宣布 ，由于“担心大型语言模型被用来大规模生成欺骗性、偏见或辱骂性语言”，因此不会立即发布该模型的完整版本。相反，他们首先发布了该模型的一个较小的、不太引人注目的版本，随后又推出了几个越来越大的变体。

接下来，OpenAI于 2020 年 6 月发布了 GPT-3。GPT-3 拥有 1750 亿个参数，为大语言模型设定了新的尺寸标准。它很快成为大语言模型研究的焦点，并成为 ChatGPT 的原始基础。

目前OpenAI 已经推出了 GPT-4，但OpenAI 尚未公开说明 GPT-4 的参数数量，但根据与 OpenAI 员工的对话进行的一项估计，其参数数量为 1 万亿个——是 GPT-3 大小的五倍，是 Bert-large 大小的近 3,000倍。 这个庞大的模型允许用户一次处理多达 50 页的文本，并减少了困扰 GPT-3 的“幻觉”的发生率。

### ChatGPT 时刻

尽管研究人员和从业者设计并部署了 BERT、GPT-2、GPT-3 和 T5 的变体，但公众却很少注意到。这些模型更多是在背后默默的提供服务，但却不被公众所感知。就公众而言，大模型最直接的例子是发布全部或部分由 GPT 及其变体编写的各类新闻报道。

随后OpenAI于2022年11月发布了ChatGPT。交互式聊天方式允许非技术用户通过提示与LLM交互并快速收到回复。当用户发送额外的提示时，系统会考虑之前的提示和响应结果，从而提供类似对话的交互连续性。

ChatGPT引起了巨大轰动。各类关于ChatGPT的报道层出不穷，且大部分都透露所报道的新闻使用了ChatGPT进行辅助撰写。

在这股热潮之后，谷歌于与 OpenAI 合作，构建了由 ChatGPT 提供支持的谷歌搜索引擎版本。与此同时，资本市场突然对这项技术如何获取更大的资本利润产生了兴趣。

### 竞相追逐ChatGPT

研究人员和科技公司通过展示自己的大型语言模型能力来响应 ChatGPT 时刻。

2023 年 2 月，Cohere推出了 其摘要产品的测试版。新端点建立在专门为摘要定制的大型语言模型之上，允许用户输入最多 18-20 页的文本进行摘要，这远远超过用户通过 ChatGPT 或直接通过 GPT-3 进行摘要的数量。

一周后，谷歌推出了 Bard，由谷歌自研的LLM支持的聊天机器人。随后宣布基于Bard构建新型 Bing 搜索引擎。

Meta 推出了 LLaMA（大型语言模型 Meta AI），为这个月画上了圆满的句号。LLaMA 并不是 GPT-3 的直接复制品（Meta AI 于 2020 年 5 月推出了他们的直接 GPT-3 克隆OPT-175B）。相反，LLaMA 项目旨在为研究社区提供规模可管理的强大大型语言模型。LLaMA 有四种大小的变体，其中最大的有 650 亿个参数，这仍然略高于 GPT-3 大小的三分之一。

4 月，DataBricks 发布了Dolly 2.0。Databricks 首席执行官 Ali Ghodsi告诉彭博社 ，开源 LLM 复制了“这些现有其他模型”的许多功能。在同一次采访中，Ghodsi 指出，他的公司选择“Dolly”这个名字是为了纪念克隆羊，因为它听起来有点像 OpenAI 的另一个著名模型 Dall-E。

### 超级大模型的时代已经结束了吗？

OpenAI 发布 GPT-4 后不久，OpenAI 首席执行官 Sam Altman在麻省理工学院告诉人群 ，他认为超级大模型的时代已经结束。使用更多数据训练更大规模参数的网络的策略已经达到了收益递减的地步。他表示，除其他挑战外，OpenAI 还面临着公司拥有或可以建造的数据中心数量的物理限制。

Cohere 联合创始人 Nick Frosst 一直认为，更大的语言模型并不是万能的。我们的研究人员的开创性工作表明 ，通过专业化，较小的大型语言模型可以比它们的庞然大物更有效。

# 关键术语

作为一个新领域，大语言模型已经产生了自己的专业术语集合。本节将定义几个最重要的内容。

### 注意力

注意力是一种数学机制，用于决定如何计算的每个token的影响。研究人员设计这种方法是为了解决输入长序列时梯度消失或爆炸的问题。在实践中，早期的自然语言处理模型会“遗忘”距离目标token超过 20 步的标记，这限制了它们的有效性。

研究人员已经制定了几种计算注意力的方法，但它通常是通过一个小模块来处理的，该模块本身就是一个神经网络。

### 嵌入（词向量|embedding）

嵌入是指捕获单词语义和上下文关系的数字表示。这些高维向量表示允许单词更接近具有相似含义的单词，并且含义在不同的上下文中具有灵活性。

例如，“bank”一词可以指银行或河流与陆地交汇的地方。高维表示允许嵌入捕获所有这些含义，并使模型能够识别在当前上下文中应该使用哪个含义。

嵌入还用于表示更大的文本单元，例如句子、段落或整个文档。这些嵌入旨在捕获句子或文档的整体含义或上下文。

### Transformer

Transformer 是一种神经网络架构，构成大多数大型语言模型的核心。与每次处理一个元素的输入数据的循环神经网络 (RNN) 不同，Transformer可以一次处理整个输入。

Transformer 由两个主要组件组成：编码器和解码器。编码器将输入序列转换为称为编码的向量表示。解码器接受编码并生成输出序列。语言模型可以是解码器（例如，GPT）、编码器（例如，BERT），或由两者组成（例如，T5）。

编码器和解码器组件都由多个注意力层组成。每个注意力层使用一个函数来计算输入或输出序列中每个元素的“自注意力”分数。这个“自我注意力”分数衡量每个元素彼此之间的相似程度。

然后，注意力层使用这些分数来创建“上下文向量”，即输入或输出元素的加权平均值。然后，注意力层将上下文向量传递给前馈网络，该网络应用非线性变换来产生该层的最终输出。

### 提示

提示是指用户向大语言模型提供的输入或指令，以生成响应或执行特定任务。它作为模型理解用户期望并指导其后续语言生成的起点。根据应用程序或任务，提示可以采取多种形式。它可以是一个完整的句子，一个问题，一个部分短语，甚至几个关键词。

### 指令调优

最近，许多大语言模型被进一步训练以遵循提示形式的指令。指令微调是预训练后进一步增强模型能力的一次训练。这个想法是训练语言模型来回答各种各样的提示，以便学习如何在测试时回答新的、未见过的提示。这项技术是由两个团队同时提出的，一个来自谷歌 ，一个来自BigScience 研究项目，其中包括来自 Snorkel 的研究人员。

### RLHF（带有人类反馈的强化学习）

带人类反馈的强化学习（更广为人知的名称是“RLHF”）是一种根据人类偏好对模型进行指令调整的技术。例如，寻求通过 RLHF 改进大型语言模型的研究人员会要求模型对给定提示生成多个响应，然后要求一个或多个人将响应从最好到最差进行排名。这为模型创建了一个用于优化的奖励函数，而奖励函数可能很难被生成任务定义。

# 大语言模型能做什么？

最流行的大型语言模型以其生成文本的能力而闻名，但LLM的功能并不限于用喜欢的名人的声音撰写文本。无论是通过巧妙的提示还是在预先训练的基础模型之上构建额外的神经层，LLM都可以为机器学习从业者执行许多有用的任务。

### 情感分析

可以使用大语言模型通过在带有情感标签的文本数据集上对其进行微调来执行情感分析。例如，可以在具有相关评级的电影评论数据集上微调大型语言模型。然后，可以使用微调后的模型来预测新电影评论的评分。

还可以使用大语言模型通过上下文学习来执行情感分析。这意味着模型生成答案之前提供一些任务示例。例如：

```
I love this movie. It was so funny and entertaining: Positive
This movie was terrible. The plot was boring and the acting was awful: Negative
This movie was okay. It had some good moments but also some flaws: Neutral
The movie was great. I enjoyed it a lot:
```

### 文本分类

大语言模型可以帮助机器学习从业者以两种主要方式对文本进行分类：通过对标记数据集进行微调，或通过巧妙的提示工程。如果对一小段文本进行分类，类似于上面情感分析部分中显示的上下文学习示例的方法可能会起作用。对于较长的文本，可以向LLM提供要分类的文本，然后提出诸如“以下哪个类别与上述文本最相似：商业|体育|新闻”之类的问题。

### 文字编辑

大语言模型可以编辑文本或对现有文章提出改进建议。例如，可以使用大语言模型来检查拼写和语法或重写句子以使其更清晰或更简洁。这可以通过多种方式来完成，一些 LLM API 还具有“编辑”模式，或者通过针对特定领域或任务（例如学术写作或技术文档）微调预先训练的模型来构建自己的系统。

### 语言翻译

语言翻译是大语言模型的最初用途之一，而最新一代的LLM只会使翻译变得更加容易。许多公开提供的LLM将通过简单的提示提供还过得去的翻译——尽管一些巧妙的提示技术可以提高翻译质量。

### 信息提取

使用大语言模型进行信息提取主要有两种方式：微调和检索。

在微调方法中，使用带有要提取的信息类型的标记的数据集，例如，具有用 [PER] 标签注释的人名的数据集。然后，使用微调的模型来预测新文本的标签。

检索方法可以无需微调即可提取信息。只需将提取任务表述为自然语言提示，例如“以下名人叫什么？” 精心设计的提示将带来更好的结果，但即使是简单的命令在大多数情况下也能发挥作用。

微调更加准确和一致，但需要更多的数据和计算资源。检索更加灵活和通用，但可能会产生不正确或不完整的答案。需要根据任务和可用资源，选择任一方法或两者的组合。

### 摘要

摘要是自然语言处理的早期用例，研究人员专门为此目的构建了多个模型。然而，现代大语言模型使其变得更加容易。

与此处列出的其他功能一样，使用 LLM 进行摘要的一种方法是在摘要数据集（在较短的段落中摘要的一对较长文本）上进行微调。然而，最流行的大语言模型的界面将让您通过简单的提示获得“现成的”摘要。可以直接向LLM提供产品评论，并询问其产品的主要优点和缺点，向其提供一大段文本并要求其列出该文本中最重要的五个要点，或者简单地说“总结一下这。”

基于个人需求，进行提示的书写，但一般少量的修改就可以产生有效的结果，但少量的修改应该会产生可用的结果。

# 大型语言模型的涌现能力是什么？

大语言模型相对一般模型的一个明显特征是：存在涌现能力。这些功能在针对类似功能的较小模型中不存在，并且不一定会因规模增加而出现。

### 情境学习

上下文学习，也称为少样本提示，是一种技术，其中用户在提示模型完成同一任务之前包含给定任务的一个或多个输入和输出示例。下图给出了一个例子。

![https://cdn.nlark.com/yuque/0/2023/png/406504/1693817685686-61887509-2a13-4a5f-9a93-4bb5a593888b.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1693817685686-61887509-2a13-4a5f-9a93-4bb5a593888b.png)

### 增强提示策略

除了ICL提示之外，研究人员和从业者还发现，当在足够大小的大语言模型上使用时，一套提示策略会产生更准确和有用的输出。我们将在下一节中更详细地讨论这些策略。

# 如何提高广义大语言模型的性能

广义大语言模型玩起来很有趣，但是当尝试构建具有可预测结果的 LLM 支持的应用程序时，它们不稳定的行为可能会成为一个严重的问题。研究人员发现了几种基于提示的技巧来缩小和锐化大型语言模型的反应。这通常称为“提示编程”或“提示工程”。根据论文《[Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm》](https://arxiv.org/pdf/2102.07350.pdf)可知，成功的提示编程通常旨在通过冗余来强化所需的行为。其中一些可能是微妙和挑剔的；在本文提出的一个示例中，作者通过将提示的开头从“Translate French to English:”更改为“Translate this French sentence to english”，发现性能更可靠。他说，前者增加了LLM继续用法语而不是翻译该段落的可能性。

但研究人员设计的一些方法不那么微妙且更加离散，包括下面列出的方法。

![https://cdn.nlark.com/yuque/0/2023/png/406504/1693817684755-70eefb81-7271-435c-9e20-a1cb53d27577.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1693817684755-70eefb81-7271-435c-9e20-a1cb53d27577.png)

### 多步推理

推理任务，尤其是那些具有多个步骤的任务，对语言模型提出了挑战。然而，思维链提示会有所帮助。这种方法引导大型语言模型在提供最终答案之前生成一系列中间步骤。与标准提示相比，思维链提示通常可以提高效果。此外，在几次提示中的最终答案后添加解释也显示出性能的提高。

![https://cdn.nlark.com/yuque/0/2023/png/406504/1693817684986-7b3f60bb-887a-4767-adf7-c683a9057fae.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1693817684986-7b3f60bb-887a-4767-adf7-c683a9057fae.png)

### 遵循指令

“指令遵循”是一种通过使用指定用户意图和所需输出格式的提示来引导大型语言模型更可预测地执行的方法。例如，要使语言模型编写文本摘要，可以使用如下提示：

```
Translation:
Translate the following sentence from English to Spanish.
Sentence:
I like to read books and play games.
Output:
Me gusta leer libros y jugar juegos.
```

遵循提示策略的指令可以通过使语言模型更符合用户的目标、更真实且毒性更小来提高语言模型的性能。它还可以减少对微调或特定于任务的数据的需求。虽然它需要人类的专业知识和创造力来为不同的任务和领域设计有效的提示，但该框架可以通过字符串替换技术轻松实现自动化。

### 模型标定（Model calibration）

在这种提示策略中，用户要求大型语言模型产生问题的答案，同时也评估模型对该答案的信心。这可以通过两种方式实现：第一种是简单地询问模型其答案为真的概率。第二种是询问模型知道指定问题答案的概率，如果概率高则只询问指定问题。

The [memetic proxy](https://arxiv.org/pdf/2102.07350.pdf) approach helps narrow the large language model’s execution by putting it in the semantic space of a particular character. To use memetic proxy, you prompt the model to answer as a particular persona. This could be a well-known public figure, such as a celebrity or politician, or a well-known archetype, like a teacher. For example: “Explain foundation models to me like a teacher would explain them to a third grader.”

Other examples can get more elaborate, such as telling the model that the user entered their question into an “expert generator” and asking the model to simulate the output of the expert generator.

### 角色扮演（Memetic proxy）

角色扮演方法通过将大语言模型置于特定字符的语义空间中来帮助缩小大型语言模型的执行范围。要使用角色扮演，需要提示模型以特定角色的身份进行回答。这可以是著名的公众人物，例如名人或政治家，也可以是众所周知的原型，例如老师。例如：“向我解释基础模型，就像老师向三年级学生解释它们一样。”

其他示例可以变得更详细，例如告诉模型用户将他们的问题输入“专家生成器”并要求模型模拟专家生成器的输出。

### 程序执行

“程序执行”策略通过强制模型使用“便笺本”来提高涉及执行程序的大语言模型任务的性能。例如，用户可以要求 LLM 显示执行跟踪，而不是向 LLM 询问给定特定输入的 Python 函数的输出。这会导致模型为函数将执行的每个中间步骤生成预测，并增加 LLM 生成的最终行正确的可能性。

![https://cdn.nlark.com/yuque/0/2023/png/406504/1693817685579-7e9156e2-cb91-4911-a974-bf0d0c6d391c.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1693817685579-7e9156e2-cb91-4911-a974-bf0d0c6d391c.png)

# 如何使大语言模型适应特定任务

大语言模型可以构成项目的基础，但它们不太可能帮助您一路实现目标。现成的LLM接受过跨多个领域的大量数据的训练，因此往往在特定领域的任务上表现不佳。但有几种方法可以从他们那里得到你所需要的东西。

### 自监督微调

在这种方法中，机器学习从业者向预先训练的大语言模型提供大量未标记的特定领域数据来微调其权重。例如，如果想要一个能够高精度回答法律问题的 LLaMA 版本，可以根据法庭文件语料库微调基本模型。

微调能够在增强模型在特定任务的能力，但会普遍降低其通用理解能力。例如，根据法庭文件进行微调的 LLaMA 不太可能将“西装”一词解释为正装。

虽然自监督微调可以提高性能，但它也可能会传播来自预训练模型的错误或偏差。它还可能抵消在基本模型上进行指令调整的一些好处。

### 监督微调

在这种方法中，机器学习实践者在较小的、特定于任务的标记数据集上重新训练预训练的语言模型。

为此，从业者准备一个带标签的数据集，其中包含所需输入和输出的示例。例如，如果我们想使用语言模型进行情感分析，我们可以准备一个包含句子及其相应情感标签（积极、消极或中性）的数据集。如果我们想使用语言模型进行摘要，我们可以准备一个包含文档及其相应摘要的数据集。

从那里，实践者定义一个损失函数，并通过向模型提供批量的训练输入和输出来重新训练模型。这将更新模型的权重，并使其输出更接近训练数据的输出。

### 蒸馏

蒸馏是将知识从大型模型转移到较小模型的过程。虽然大模型（例如大语言模型）具有很高的知识容量，但这种容量可能没有得到充分利用或与我们的任务完全相关。我们可以将该模型中的相关知识压缩为更高效、更快的更小的模型，同时保留其大部分性能。

为此，我们首先选择一个要提取的大模型。该模型将充当较小模型的老师。其次，我们选择一个较小的模型来充当学生。第三，我们使用大模型的输出作为“软”目标来训练较小的模型。这意味着我们不使用硬标签（例如正确的类或标记）作为基本事实，而是使用大模型生成的所有可能标签或标记的概率分布。这样，较小的模型不仅可以从最可能的输出中学习，还可以从大模型的不确定性和置信度中学习。
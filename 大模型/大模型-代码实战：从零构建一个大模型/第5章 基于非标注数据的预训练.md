# 第5章 基于非标注数据的预训练

在本章中,我们将学习:

1. 计算训练集和验证集损失,以评估LLM生成文本的质量
2. 实现训练函数并预训练LLM
3. 保存和加载模型权重以继续训练LLM
4. 从OpenAI加载预训练权重

在前几章中,我们已经实现了数据采样、注意力机制,并编写了LLM的架构。本章的核心重点是实现一个训练函数并预训练LLM,如图5.1所示。

![Untitled](Images/大模型-从零构建一个大模型/第五章/Untitled.png)

![https://www.notion.soimages/llm_training_flow.png](https://www.notion.soimages/llm_training_flow.png)

图5.1 LLM编码、预训练和微调的三个主要阶段示意图。本章聚焦于LLM的预训练,包括实现训练代码、评估性能以及保存和加载模型权重。

如图5.1所示,我们还将学习基本的模型评估技术来衡量生成文本的质量,这是在训练过程中优化LLM的必要条件。此外,我们将讨论如何加载预训练权重,为后续章节中的微调任务奠定基础。

在LLM和其他神经网络模型的语境中,"权重"指的是在训练过程中调整的可学习参数。这些权重通常被称为权重参数或简称参数。在PyTorch中,这些权重存储在线性层等结构中,例如我们在第3章实现多头自注意力模型和第4章的GPTModel时使用的那些。初始化一个层后(如new_layer = torch.nn.Linear(...)),我们可以通过.weight属性访问其权重,即new_layer.weight。此外,为了方便起见,PyTorch允许通过model.parameters()方法直接访问模型的所有可学习参数,包括权重和偏置,这在后面实现模型训练时会用到。

5.1 评估生成式文本模型

在开始本章的主要内容之前,让我们快速回顾一下上一章的文本生成过程,并讨论一些基本方法来评估生成文本的质量。本节涵盖的内容和本章其余部分的概述如图5.2所示。

![Untitled](Images/大模型-从零构建一个大模型/第五章/Untitled1.png)

图5.2 本章涵盖的主题概览。我们首先回顾上一章的文本生成,然后实现基本的模型评估技术,这些技术将在预训练阶段使用。

如图5.2所示,我们的第一个子部分将回顾上一章末尾的文本生成过程,然后我们将介绍文本评估,并在随后的小节中计算训练和验证损失。

5.1.1 使用GPT生成文本

在本节中,我们将使用GPT模型简要复习上一章实现的文本生成过程。我们首先初始化GPT模型,该模型将在本章中进行训练,使用第4章中的GPTModel类和GPT_CONFIG_124M字典:

```python
import torch
from chapter04 import GPTModel

GPT_CONFIG_124M = {
    "vocab_size": 50257,
    "context_length": 256,
    "emb_dim": 768,
    "n_heads": 12,
    "n_layers": 12,
    "drop_rate": 0.1,
    "qkv_bias": False
}

torch.manual_seed(123)
model = GPTModel(GPT_CONFIG_124M)
model.eval()

```

注意GPT_CONFIG_124M字典中,我们做了一些调整:与上一章相比,我们将上下文长度(context_length)减少到256个token。这一修改减少了训练模型的计算需求,使得在标准笔记本电脑上进行训练成为可能。

原始的GPT-2模型配置为处理长达1,024个token的序列。在训练过程结束时,我们会更新上下文长度设置,并加载预训练权重以匹配配置为1,024 token上下文长度的模型。

接下来,我们采用上一章介绍的generate_text_simple函数,并回顾两个有用的函数:text_to_token_ids和token_ids_to_text。这些函数促进了文本和token表示之间的转换,这是我们将在整个章节中使用的技术。为了提供更清晰的理解,图5.3展示了这个过程,然后我们将深入代码细节。

![Untitled](Images/大模型-从零构建一个大模型/第五章/Untitled2.png)

图5.3 生成文本涉及将文本编码为LLM处理的token ID,然后将其转换为logit向量。logit向量随后被转换回token ID,再被解码为文本表示。

图5.3说明了使用GPT模型的三步文本生成过程。首先,分词器将输入文本转换为一系列token ID,如第2章所讨论。其次,模型接收这些token ID并生成相应的logits,这些logits是表示词汇表中每个token的概率分布的未归一化分数,如第4章所讨论。最后,这些logits被转换回token ID,分词器将其解码为人类可读的文本,完成从文本输入到文本输出的循环。

在代码中,我们按如下方式实现文本生成过程:

```python
import tiktoken
from chapter04 import generate_text_simple

def text_to_token_ids(text, tokenizer):
    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})
    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # 添加批次维度
    return encoded_tensor

def token_ids_to_text(token_ids, tokenizer):
    flat = token_ids.squeeze(0) # 移除批次维度
    return tokenizer.decode(flat.tolist())

start_context = "Every effort moves you"
tokenizer = tiktoken.get_encoding("gpt2")

token_ids = generate_text_simple(
    model=model,
    idx=text_to_token_ids(start_context, tokenizer),
    max_new_tokens=10,
    context_size=GPT_CONFIG_124M["context_length"]
)
print("Output text:\\n", token_ids_to_text(token_ids, tokenizer))

```

使用上述代码,模型生成以下文本:

```
Output text:
 Every effort moves you rentingetic wasnم refres RexMeCHicular stren

```

很明显,模型当前并没有产生连贯的文本,因为它还没有经过训练。要定义什么构成"高质量"或"连贯"的文本,我们需要实现一种数值方法来评估生成的内容。这种方法将使我们能够监控和增强模型的性能,贯穿整个训练过程。

下一节考虑我们如何计算生成输出的损失度量。这个损失作为训练过程的进度和成功指标。此外,在后续章节关于微调LLM时,我们将探讨评估模型质量的其他方法。

5.1.2 计算文本生成损失

本节探讨用于数值评估训练期间生成文本质量的技术,通过计算所谓的文本生成损失。我们将逐步介绍这个主题,使用一个实际示例来使概念清晰和可应用,从一个简短回顾我们如何使用在第2章引入的数据加载器和第4章的generate_text_simple函数生成的文本开始。

图5.4说明了从输入文本到LLM生成文本的整体流程,使用一个逐令牌的过程。

![Untitled](Images/大模型-从零构建一个大模型/第五章/Untitled3.png)
图5.4 对于左侧所示的3个输入token,我们计算一个向量,包含对应于词汇表中每个token的概率分数。每个向量中最高概率分数的索引位置代表最可能的下一个token ID。选择与最高概率分数相关的这些token ID,并将其映射回表示模型生成的文本。

图5.4中概述的文本生成过程与第4章的generate_text_simple函数工作方式类似。我们需要执行这些相同的初始步骤,然后我们才能计算一个损失来衡量生成文本质量,这将在本节后面讨论。

为了简化说明,图5.4展示了文本生成过程,使用一个小型7-token词汇表在单个页面上。然而,我们的GPTModel使用一个包含50,257个单词的更大词汇表;因此,下面代码中的token ID将范围从0到50,256,而不是0到6。

此外,图5.4仅为简单起见显示了一个文本示例("every effort moves")。在以下动手实践示例中实现图5.4中的步骤时,我们将使用两个输入示例("every effort moves"和"I really like")作为GPT模型的输入:

```python
# 定义我们的两个输入示例,已经映射为token ID,对应图5.4中的步骤1:
inputs = torch.tensor([[16833, 3626, 6100],   # ["every effort moves",
                       [40,    1107, 588]])   #  "I really like"]

# 匹配这些输入,targets包含我们希望模型生成的token ID:
targets = torch.tensor([[3626, 6100, 345  ],  # [" effort moves you",
                        [107,  588, 11311]]) #  " really like chocolate"]

```

注意targets比inputs向前移动了一个位置,这是我们在第2章实现数据加载器时介绍的概念。这种移位策略对于训练模型预测序列中的下一个token至关重要。

现在我们将inputs输入模型以计算两个输入示例的logit向量,每个包含三个token,并应用softmax函数将这些logit值转换为概率分数,这对应图5.4中的步骤2:

```python
with torch.no_grad():
    logits = model(inputs)
probas = torch.softmax(logits, dim=-1) # 词汇表中每个token的概率
print(probas.shape)

```

结果概率分数(probas)张量的维度如下:

```
torch.Size([2, 3, 50257])

```

第一个数字2对应inputs中的两个示例(批次),也称为批次大小。第二个数字3对应每个输入中的token数(序列长度)。最后一个数字对应嵌入的维度大小,由词汇表大小决定,如前几章讨论的。

在使用softmax函数将logits转换为概率后,generate_text_simple函数从第4章然后将最高概率分数转换回token,如图5.4中的步骤3-5所示。

我们可以通过对概率分数应用argmax函数来实现步骤3和4,以获得相应的token ID:

```python
token_ids = torch.argmax(probas, dim=-1, keepdim=True)
print("Token IDs:\\n", token_ids)

```

由于我们有2个输入批次,每个包含3个token,对概率分数应用argmax函数(图5.4中的步骤3)产生2组输出,每组有3个预测的token ID:

```
Token IDs:
 tensor([[[16657], # 第一个批次
         [  339],
         [42826]],
        [[49906], # 第二个批次
         [29669],
         [41751]]])

```

最后,步骤5将token ID转换回文本:

```python
print(f"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}")
print(f"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}")

```

当我们解码这些token时,我们发现这些输出token与我们希望模型生成的目标token完全不同:

```
Targets batch 1:  effort moves you
Outputs batch 1:  Armed heNetflix

```

模型产生随机文本,与目标文本不同,因为它还没有经过训练。我们现在到了需要评估模型生成文本性能的部分,使用所谓的损失,如图5.4所示。这个损失对于衡量生成文本的质量非常有用,它是实现训练函数的构建块,我们稍后用它来更新模型权重以改善生成的文本。

![Untitled](Images/大模型-从零构建一个大模型/第五章/Untitled4.png)
图5.5 我们现在在本节剩余部分实现文本评估函数。在下一节中,我们将这个评估函数应用于用于模型训练的整个数据集。

文本评估过程的核心,如图5.5所示,是衡量生成的token与正确预测(目标)之间的"距离"。我们稍后在本章实现的训练函数将使用这个信息来调整模型权重,以生成更接近(理想情况下匹配)目标文本的文本。

你可能想象,我们的目标是增加softmax概率在对应正确目标token ID的索引位置,如图5.6所示。这个softmax概率也用在我们在本节剩余部分实现的评估指标中,以数值地评估模型生成的输出:概率越高,越好。

![Untitled](Images/大模型-从零构建一个大模型/第五章/Untitled5.png)
图5.6 训练前,模型产生随机的下一个token概率向量。模型训练的目标是确保对应高亮目标token ID的概率值被最大化。

请记住,图5.6显示了一个紧凑的7-token词汇表的softmax概率,以便将所有内容放入一个图中。这意味着初始随机值将围绕1/7,即约0.14。

然而,我们用于GPT-2模型的词汇表有50,257个token,因此任何初始概率都将围绕0.00002或1/50,257。

对于两个输入文本,我们可以打印初始softmax概率分数,对应目标token的概率如下:

```python
text_idx = 0
target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]
print("Text 1:", target_probas_1)

text_idx = 1
target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]
print("Text 2:", target_probas_2)

```

每个批次的3个目标token ID概率如下:

```
Text 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])
Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])

```

训练LLM的目标是最大化这些值,使它们尽可能接近概率1。这样,我们确保LLM一致地预测目标token - 本质上是句子中的下一个单词 - 作为它生成的下一个token。

那么,我们如何最大化对应目标token的softmax概率值呢?答案是我们更新模型权重,使模型输出对各自token ID我们想要生成的更高值。权重更新是通过一个称为反向传播的过程完成的,这是训练深度神经网络的标准技术(有关反向传播和模型训练的更多细节,请参见附录A中的A.3至A.7节)。

反向传播需要一个损失函数,它计算模型预测输出(即,对应目标token ID的概率)与实际期望输出之间的差异。这个损失函数衡量模型的预测有多远离目标值。

在本节的剩余部分,我们计算两个示例批次的target_probas_1和target_probas_2的概率分数的损失。接下来的步骤如图5.7所示。

![Untitled](Images/大模型-从零构建一个大模型/第五章/Untitled6.png)
图5.7 计算损失涉及几个步骤。步骤1到3计算对应目标张量的token概率。这些概率然后通过对数函数转换并在步骤4-6中平均。

由于我们已经应用了图5.7中所示的步骤1-3来获得target_probas_1和target_probas_2,我们继续步骤4,对概率分数应用对数:

```python
log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))
print(log_probas)

```

这产生以下值:

```
tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])

```

使用概率分数的对数在数学优化中比直接处理分数更有优势。这个主题超出了本文的范围,但我在附录A的参考部分中详细讨论了它。

现在,我们通过计算平均值将这些对数概率合并为单个分数(图5.7中的步骤5):

```python
avg_log_probas = torch.mean(log_probas)
print(avg_log_probas)

```

结果平均对数概率分数如下:

```
tensor(-10.7940)

```

目标是通过调整模型权重作为训练过程的一部分,使平均对数概率尽可能接近0,这我们将在5.2节实现。

然而,在更一般的情况下,常见做法不是将平均对数概率加到0,而是将负平均对数概率降到0。负平均对数概率只是平均对数概率乘以-1,这对应于图5.7中的步骤6:

```python
neg_avg_log_probas = avg_log_probas * -1
print(neg_avg_log_probas)

```

这打印出tensor(-10.7940)。

术语上,这个负值,-10.7940变为10.7940,被称为交叉熵损失。

幸运的是,在实践中,PyTorch已经有一个内置的cross_entropy函数,它处理图5.7中的所有6个步骤。

交叉熵损失是机器学习和深度学习中衡量两个概率分布之间差异的自然度量 - 通常是真实分布(例如,数据集中的token)和模型预测的分布(例如,LLM生成的token概率)。

在机器学习的上下文中,特别是在PyTorch等框架中,cross_entropy函数计算这个度量作为预测输出,这与目标token给定模型生成的token概率的负平均对数概率相关且通常是可互换的。

在应用交叉熵函数之前,让我们简要回顾一下logits和targets张量的形状:

```python
print("Logits shape:", logits.shape)
print("Targets shape:", targets.shape)

```

结果形状如下:

```
Logits shape: torch.Size([2, 3, 50257])
Targets shape: torch.Size([2, 3])

```

我们可以看到,logits张量有三个维度:批次大小、token数和词汇表大小。targets张量有两个维度:批次大小和token数。

对于PyTorch的cross_entropy损失函数,我们需要展平这些张量,将它们合并到批次维度中:

```python
logits_flat = logits.flatten(0, 1)
targets_flat = targets.flatten()
print("Flattened logits:", logits_flat.shape)
print("Flattened targets:", targets_flat.shape)

```

结果张量维度如下:

```
Flattened logits: torch.Size([6, 50257])
Flattened targets: torch.Size([6])

```

记住,targets是我们希望LLM生成的token ID,而logits包含未缩放的模型输出,在通过softmax函数获得概率分数之前。

之前,我们应用了softmax函数,选择了对应目标ID的概率分数,并计算了负平均对数概率。PyTorch的cross_entropy函数将为我们处理所有这些步骤:

```python
loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)
print(loss)

```

结果损失与我们之前手动应用图5.7中所示的单独步骤时获得的相同:

```
tensor(10.7940)

```

困惑度

困惑度是一个经常用于评估语言建模任务中模型性能的指标,它是从交叉熵损失派生出来的。它可以提供一种更直观的方式来理解模型在预测序列中下一个token时的不确定性。

困惑度衡量模型预测的概率分布与数据集中单词的实际分布匹配得有多好。与损失类似,较低的困惑度表示模型预测更接近实际分布。

困惑度可以计算为perplexity = torch.exp(loss),当应用于之前计算的损失时,返回tensor(48725.8203)。

困惑度通常被认为比原始损失值更具解释性,因为它量化了模型在每个时间步不确定的有效词汇表大小。在给定示例中,这将转化为模型在词汇表中约47,678个单词或token中不确定要生成哪个作为下一个token。

在本节中,我们计算了两个小文本输入的损失以作说明。在下一节中,我们将损失计算应用于整个训练和验证数据集。

5.1.3 计算训练集和验证集损失

在本节中,我们首先准备稍后在本章用于训练LLM的训练和验证数据集。然后,我们计算训练和验证数据的交叉熵损失,如图5.8所示,这是模型训练过程的重要组成部分。

![Untitled](Images/大模型-从零构建一个大模型/第五章/Untitled7.png)
图5.8 在上一节计算交叉熵损失后,我们现在将这个损失计算应用于我们将用于模型训练的整个文本数据集。

为了计算训练和验证数据集的损失,如图5.8所示,我们使用一个非常小的文本数据集,"The Verdict"短篇小说,我们在第2章已经使用过。通过选择一个来自公共领域的文本,我们避免了与使用权相关的问题。此外,我们使用这样一个小数据集的原因是它允许在普通笔记本电脑上在几分钟内执行我们的示例,即使没有高端GPU,这对于教育目的特别有利。

感兴趣的读者可以使用我们补充代码来准备一个更大规模的数据集,包含来自Project Gutenberg的60,000多本公共领域书籍,并在这些数据上训练LLM(见附录B获取详细信息)。

预训练LLM的成本

为了将项目规模纳入视角,考虑训练7亿参数的GPT-2模型,这是一个相对较小的公开可用LLM。这个模型需要184,320 GPU小时的昂贵V100 GPU,处理2万亿个token。按照写作时的价格,在8xV100云服务器上运行的AWS成本约为每小时$30。粗略估计,训练这样一个LLM的总成本约为$690,000(计算为184,320小时除以8,然后乘以$30)。

以下代码加载我们在第2章使用的"The Verdict"短篇小说:

```python
file_path = "the-verdict.txt"
with open(file_path, "r", encoding="utf-8") as file:
    text_data = file.read()

```

在加载数据集后,我们可以检查数据集中的字符数和token数:

```python
total_characters = len(text_data)
total_tokens = len(tokenizer.encode(text_data))
print("Characters:", total_characters)
print("Tokens:", total_tokens)

```

输出如下:

```
Characters: 20479
Tokens: 5145

```

只有5,145个token,这个文本可能看起来太小以至于无法训练LLM,但正如前面提到的,出于教育目的,我们可以在几分钟内而不是几周内运行代码。此外,我们将在本章末尾从OpenAI加载预训练权重到GPTModel中。

现在,我们将数据集分为训练集和验证集,并使用第2章的数据加载器来准备LLM训练的批次。这个过程如图5.9所示。

![Untitled](Images/大模型-从零构建一个大模型/第五章/Untitled8.png)

图5.9 准备数据加载器时,我们将输入文本分为训练集和验证集部分。然后,我们对文本进行分词(为简单起见,只显示训练集部分),并将分词后的文本分成用户指定长度的块(这里是6)。最后,我们打乱行并将分块文本组织成批次(这里,批次大小为2),这些批次可以用于模型训练。

为了可视化目的,图5.9使用max_length=6显示了工作流程。然而,对于我们实际实现的数据加载器,我们将max_length设置为等于LLM支持的256-token上下文长度,以便LLM可以学习更长的文本。

可变长度训练

我们使用相似大小的块训练模型是为了简单性和效率。然而,在实践中,用可变长度输入训练LLM也可能是有益的,以帮助LLM更好地泛化到不同类型的输入,尤其是在被部署时。

为了实现图5.9中所示的数据分割和加载,我们首先定义一个train_ratio来使用90%的数据进行训练,剩余10%作为验证数据用于训练期间的模型评估:

```python
train_ratio = 0.90
split_idx = int(train_ratio * len(text_data))
train_data = text_data[:split_idx]
val_data = text_data[split_idx:]

```

使用train_data和val_data子集,我们现在可以创建各自的数据加载器,使用第2章的create_dataloader_v1函数:

```python
from chapter02 import create_dataloader_v1
torch.manual_seed(123)

train_loader = create_dataloader_v1(
    train_data,
    batch_size=2,
    max_length=GPT_CONFIG_124M["context_length"],
    stride=GPT_CONFIG_124M["context_length"],
    drop_last=True,
    shuffle=True,
    num_workers=0
)
val_loader = create_dataloader_v1(
    val_data,
    batch_size=2,
    max_length=GPT_CONFIG_124M["context_length"],
    stride=GPT_CONFIG_124M["context_length"],
    drop_last=False,
    shuffle=False,
    num_workers=0
)

```

我们使用相对较小的批次大小在前面的代码中,以减少计算资源需求,因为我们正在处理一个非常小的数据集。在实践中,使用1,024或更大的批次大小训练LLM并不罕见。

作为重要检查,我们可以迭代数据加载器以确保它们创建正确:

```python
print("Train loader:")
for x, y in train_loader:
    print(x.shape, y.shape)

print("\\nValidation loader:")
for x, y in val_loader:
    print(x.shape, y.shape)

```

我们应该看到以下输出:

```
Train loader:
torch.Size([2, 256]) torch.Size([2, 256])
torch.Size([2, 256]) torch.Size([2, 256])
torch.Size([2, 256]) torch.Size([2, 256])
torch.Size([2, 256]) torch.Size([2, 256])
torch.Size([2, 256]) torch.Size([2, 256])
torch.Size([2, 256]) torch.Size([2, 256])
torch.Size([2, 256]) torch.Size([2, 256])
torch.Size([2, 256]) torch.Size([2, 256])
torch.Size([2, 256]) torch.Size([2, 256])

Validation loader:
torch.Size([2, 256]) torch.Size([2, 256])

```

基于生成的打印输出,我们有9个训练数据批次,每个批次有2个样本和256个token。由于我们分配了10%的数据用于验证,只有一个验证批次,包含2个输入示例。

如预期,输入数据(x)和目标数据(y)具有相同的形状(批次大小乘以每个批次中的token数),因为目标只是输入向前移动一个位置,如第2章所讨论。

现在,我们实现一个实用函数来计算给定批次的交叉熵损失,由训练和验证加载器返回:

```python
def calc_loss_batch(input_batch, target_batch, model, device):
    input_batch, target_batch = input_batch.to(device), target_batch.to(device)
    logits = model(input_batch)
    loss = torch.nn.functional.cross_entropy(
        logits.flatten(0, 1), target_batch.flatten()
    )
    return loss

```

我们现在可以使用这个calc_loss_batch实用函数,它计算单个批次的损失,来实现以下calc_loss_loader函数,该函数计算由给定数据加载器采样的所有批次的平均损失:

```python
def calc_loss_loader(data_loader, model, device, num_batches=None):
    total_loss = 0.
    if len(data_loader) == 0:
        return float("nan")
    elif num_batches is None:
        num_batches = len(data_loader)
    else:
        num_batches = min(num_batches, len(data_loader))
    for i, (input_batch, target_batch) in enumerate(data_loader):
        if i < num_batches:
            loss = calc_loss_batch(input_batch, target_batch, model, device)
            total_loss += loss.item()
        else:
            break
    return total_loss / num_batches

```

默认情况下,calc_loss_batch函数迭代给定数据加载器中的所有批次,累积total_loss变量中的损失,然后计算所有批次的平均损失。或者,我们可以指定较小数量的批次通过num_batches来加速模型训练期间的评估。

让我们使用这个calc_loss_batch函数,将其应用于训练和验证数据加载器:

```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
with torch.no_grad():
    train_loss = calc_loss_loader(train_loader, model, device)
    val_loss = calc_loss_loader(val_loader, model, device)
print("Training loss:", train_loss)
print("Validation loss:", val_loss)

```

结果损失值如下:

```
Training loss: 10.98758347829183
Validation loss: 10.98110580444336

```

损失值相当高,因为模型还没有经过任何训练。作为比较,如果模型学会生成训练和验证数据中出现的下一个token,损失会接近0。

现在我们有了一种衡量生成文本质量的方法,在下一节中,我们将训练LLM以减少这个损失,使其变得更擅长生成文本,如图5.10所示。

![Untitled](Images/大模型-从零构建一个大模型/第五章/Untitled9.png)
图5.10 我们已经回顾了文本生成过程并实现了基本的模型评估技术来计算训练和验证集损失。接下来,我们将转向训练函数并预训练LLM。

如图5.10所示,下一节重点关注训练LLM。在模型训练之后,我们将实现替代文本生成策略并保存和加载经训练的模型权重。

5.2 训练LLM

在本节中,我们终于实现了用于训练LLM(我们的GPTModel)的代码。为此,我们专注于一个简单的训练循环,如图5.11所示,以保持本节简单和可读。然而,感兴趣的读者可以在附录C中学习更高级的技术,包括学习率预热、梯度裁剪和梯度累积,这些技术在"进阶技巧"中有详细介绍。

![Untitled](Images/大模型-从零构建一个大模型/第五章/Untitled10.png)
图5.11 PyTorch中训练深度神经网络的典型训练循环包括几个步骤,在多个epoch中迭代训练集中的批次。在每个循环中,我们计算每个训练集批次的损失以确定损失梯度,我们使用这些梯度来更新模型权重,以最小化训练集损失。

图5.11中的流程图描绘了一个典型的PyTorch神经网络训练工作流程,我们用它来训练LLM。它概述了八个步骤,从迭代多个epoch开始,包括处理批次、计算梯度、更新权重,并以监控步骤结束,如打印损失和生成文本样本。如果你相对不熟悉用PyTorch训练深度神经网络,并且这些步骤中的任何一个不熟悉,请考虑阅读附录A中的A.5至A.8节,"PyTorch简介"。

在代码中,我们可以将这个训练流程实现为以下train_model_simple函数:

```python
def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,
                       eval_freq, eval_iter, start_context, tokenizer):
    train_losses, val_losses, track_tokens_seen = [], [], []
    tokens_seen, global_step = 0, -1

    for epoch in range(num_epochs):
        model.train()
        for input_batch, target_batch in train_loader:
            optimizer.zero_grad()
            loss = calc_loss_batch(input_batch, target_batch, model, device)
            loss.backward()
            optimizer.step()
            tokens_seen += input_batch.numel()
            global_step += 1

            if global_step % eval_freq == 0:
                train_loss, val_loss = evaluate_model(
                    model, train_loader, val_loader, device, eval_iter)
                train_losses.append(train_loss)
                val_losses.append(val_loss)
                track_tokens_seen.append(tokens_seen)
                print(f"Ep {epoch+1} (Step {global_step:06d}): "
                      f"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}")

        generate_and_print_sample(
            model, tokenizer, device, start_context
        )
    return train_losses, val_losses, track_tokens_seen

```

注意train_model_simple函数中我们还没有定义两个函数:evaluate_model和generate_and_print_sample。

evaluate_model函数对应图5.11中的步骤7。它打印训练和验证数据损失,在每次模型更新后,以便我们可以评估训练是否改进了模型。

更具体地说,evaluate_model函数计算训练和验证集的损失,同时将模型置于评估模式,禁用训练特定操作(如dropout),并在计算损失时禁用梯度计算:

```python
def evaluate_model(model, train_loader, val_loader, device, eval_iter):
    model.eval()
    with torch.no_grad():
        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)
        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)
    model.train()
    return train_loss, val_loss

```

与evaluate_model类似,generate_and_print_sample函数是一个便利函数,我们可以用它来追踪模型在训练期间是否改进。特别是,generate_and_print_sample函数接受一个文本提示(start_context)作为输入,将其转换为token ID,并将其馈送到LLM以使用我们之前的generate_text_simple函数生成文本样本:

```python
def generate_and_print_sample(model, tokenizer, device, start_context):
    model.eval()
    context_size = model.pos_emb.weight.shape[0]
    encoded = text_to_token_ids(start_context, tokenizer).to(device)
    with torch.no_grad():
        token_ids = generate_text_simple(
            model=model, idx=encoded,
            max_new_tokens=50, context_size=context_size
        )
        decoded_text = token_ids_to_text(token_ids, tokenizer)
        print(decoded_text.replace("\\n", " "))  # 紧凑打印格式
    model.train()

```

虽然evaluate_model函数提供了模型训练进展的数值估计,但这个generate_and_print_sample文本函数提供了由模型生成的具体文本示例,以判断其在训练期间的能力。

AdamW

Adam优化器是训练深度神经网络的流行选择。然而,在训练循环中,我们使用AdamW优化器。AdamW是Adam的一个变体,它改进了权重衰减方法,这有助于最小化模型复杂性并防止过拟合,通过惩罚大权重。这种调整允许AdamW实现更有效的正则化和更好的泛化,并且在训练LLM时经常使用。

让我们将所有这些付诸实践,通过使用AdamW优化器和我们之前定义的train_model_simple函数训练GPTModel实例10个epoch。

```python
torch.manual_seed(123)
model = GPTModel(GPT_CONFIG_124M)
model.to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)
num_epochs = 10
train_losses, val_losses, tokens_seen = train_model_simple(
    model, train_loader, val_loader, optimizer, device,
    num_epochs=num_epochs, eval_freq=5, eval_iter=1,
    start_context="Every effort moves you", tokenizer=tokenizer
)

```

执行training_model_simple函数启动训练过程,在类似笔记本电脑上大约需要5分钟完成。在执行期间打印的输出如下:

```
Ep 1 (Step 000000): Train loss 9.781, Val loss 9.933
Ep 1 (Step 000005): Train loss 8.111, Val loss 8.339
Every effort moves you,,,,,,,,,,,,.
Ep 2 (Step 000010): Train loss 6.661, Val loss 7.048
Ep 2 (Step 000015): Train loss 5.961, Val loss 6.616
Every effort moves you, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and,, and, and,
[...] 结果被截断以节省空间
Ep 9 (Step 000080): Train loss 0.541, Val loss 6.393
Every effort moves you?"  "Yes--quite insensible to the irony. She wanted him vindicated--and by me!"  He laughed again, and threw back the window-curtains, I had the donkey. "There were days when I
Ep 10 (Step 000085): Train loss 0.391, Val loss 6.452
Every effort moves you know," was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed luncheon-table, when, on a later day, I had again run over from Monte Carlo; and Mrs. Gis

```

正如我们可以看到,根据训练期间打印的结果,训练损失显著改善,从9.558的值开始降到0.762。模型的语言技能也有了很大改进。在开始时,模型只能添加逗号到起始上下文("Every effort moves you,,,,,,,,,,,,")或重复单词"and"。到训练结束时,它可以生成语法正确的文本。

与训练数据损失类似,我们可以看到验证损失从高值(9.856)开始并在训练期间下降。然而,它从未变得像训练数据损失那样小,并在第10个epoch后保持在6.372。

在更详细地讨论验证损失之前,让我们创建一个简单的图表,显示训练和验证数据损失随时间的变化:

```python
import matplotlib.pyplot as plt

def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):
    fig, ax1 = plt.subplots(figsize=(5, 3))
    ax1.plot(epochs_seen, train_losses, label="Training loss")
    ax1.plot(epochs_seen, val_losses, linestyle="-.", label="Validation loss")
    ax1.set_xlabel("Epochs")
    ax1.set_ylabel("Loss")
    ax1.legend(loc="upper right")
    ax2 = ax1.twiny()
    ax2.plot(tokens_seen, train_losses, alpha=0)
    ax2.set_xlabel("Tokens seen")
    fig.tight_layout()
    plt.show()

epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))
plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)

```

结果训练和验证损失图如图5.12所示。

![Untitled](Images/大模型-从零构建一个大模型/第五章/Untitled11.png)

图5.12 在训练开始时,我们观察到训练和验证集损失都急剧下降,这是模型正在学习的迹象。然而,训练集损失在第二个epoch之后继续下降,而验证损失停滞不前。这表明模型仍在学习,但在第2个epoch之后开始过度拟合训练集。

如图5.12所示,训练和验证损失在第一个epoch开始时都有所改善。然而,从第二个epoch开始,损失开始diverge。这种diverge和验证损失明显高于训练损失表明模型正在过度拟合训练数据。我们可以确认模型确实通过增加其生成的文本片段来记忆训练数据,如"quite insensible to the irony"在"The Verdict"文本中出现。

这种行为是预期的,因为我们正在使用一个非常小的训练数据集并训练模型多个epoch。通常,最好在更大、更多样化的数据集上训练模型一个epoch。

如前所述,感兴趣的读者可以尝试在60,000本来自Project Gutenberg的公共领域书籍上训练模型,在这种情况下过拟合不太可能发生;见附录B获取详细信息。

在接下来的部分中,如图5.13所示,我们将探讨LLM采用的采样方法,以缓解记忆效应,从而产生更新颖的生成文本。

![Untitled](Images/大模型-从零构建一个大模型/第五章/Untitled12.png)

图5.13 我们的模型在实现训练函数后可以生成连贯的文本。然而,它经常一字不差地记忆训练集中的段落。下一节涵盖了生成更多样化输出文本的策略。

如图5.13所示,下一节将涵盖LLM的文本生成策略,以减少训练集记忆并增加LLM生成文本的原创性,然后我们将讨论权重加载和保存,以及从OpenAI的GPT模型加载预训练权重。

5.3 控制随机性的解码策略

在本节中,我们将介绍文本生成策略(也称为解码策略)以生成更原创的文本。首先,我们简要回顾上一章的generate_text_simple函数,我们在本章前面的generate_and_print_sample中使用了它。然后,我们将涵盖两种技术,温度缩放和top-k采样,来改进这个函数。

我们首先将模型移回CPU,因为在相对较小的模型上推理不需要GPU。然后,在训练后,我们将模型置于评估模式以关闭随机组件,如dropout:

```python
model.to("cpu")
model.eval()

```

现在,我们将GPTModel实例(model)传递给generate_text_simple函数,该函数让LLM一次生成一个token:

```python
tokenizer = tiktoken.get_encoding("gpt2")
token_ids = generate_text_simple(
    model=model,
    idx=text_to_token_ids("Every effort moves you", tokenizer),
    max_new_tokens=25,
    context_size=GPT_CONFIG_124M["context_length"]
)
print("Output text:\\n", token_ids_to_text(token_ids, tokenizer))

```

生成的文本如下:

```
Output text:
Every effort moves you know," was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed lun

```

如5.1.2节所解释,生成的token是通过选择对应于词汇表中最高概率分数的token来选择的。

以下小节介绍两个概念来控制生成文本的随机性和多样性:温度缩放和top-k采样。

5.3.1 温度缩放

本节介绍温度缩放,这是一种将概率选择过程应用于下一个token生成任务的技术。

之前,在generate_text_simple函数内,我们总是使用torch.argmax采样具有最高概率的token作为下一个token,也称为贪婪解码。为了生成具有更多变化的文本,我们可以用一个从概率分布采样的函数(即,LLM为每个词汇表条目在每个token生成步骤生成的概率分数)替换argmax。

为了说明概率采样与一个具体示例,让我们简要讨论使用一个非常小的词汇表进行下一个token生成过程,以便说明:

```python
vocab = {
    "closer": 0,
    "every": 1,
    "effort": 2,
    "forward": 3,
    "inches": 4,
    "moves": 5,
    "pizza": 6,
    "toward": 7,
    "you": 8,
}
inverse_vocab = {v: k for k, v in vocab.items()}

```

现在,假设LLM给定起始上下文"every effort moves you"并生成以下下一个token的logits:

```python
next_token_logits = torch.tensor(
    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]
)

```

如前一章所讨论,在generate_text_simple内,我们将logits转换为概率通过softmax函数,并使用argmax函数获得对应生成token的token ID,我们可以使用反向词汇表转换回文本:

```python
probas = torch.softmax(next_token_logits, dim=0)
next_token_id = torch.argmax(probas).item()
print(inverse_vocab[next_token_id])

```

由于最大logit值,因此对应最大softmax概率分数,位于第四个位置(索引位置3,因为Python使用0-索引),生成的单词是"forward"。

要实现概率采样过程,我们现在可以用PyTorch的multinomial函数替换argmax:

```python
torch.manual_seed(123)
next_token_id = torch.multinomial(probas, num_samples=1).item()
print(inverse_vocab[next_token_id])

```

打印的输出仍然是"forward",就像之前一样。发生了什么?multinomial函数根据其概率分数采样下一个token。换句话说,"forward"仍然是最可能的token,并且会被multinomial最频繁地选择。为了说明这一点,让我们实现一个重复这个采样1000次的函数:

```python
def print_sampled_tokens(probas):
    torch.manual_seed(123)
    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]
    sampled_ids = torch.bincount(torch.tensor(sample))
    for i, freq in enumerate(sampled_ids):
        print(f"{freq} x {inverse_vocab[i]}")

print_sampled_tokens(probas)

```

采样输出如下:

```
73 x closer
0 x every
0 x effort
582 x forward
2 x inches
0 x moves
0 x pizza
343 x toward

```

正如我们从输出中看到的,单词"forward"被采样最频繁(1000次中的582次),但其他token如"closer"、"inches"和"toward"也会被偶尔采样。这意味着如果我们用multinomial函数替换generate_and_print_sample函数内的argmax函数,LLM有时会生成像"every effort moves you toward"、"every effort moves you inches"和"every effort moves you closer"这样的文本,而不是"every effort moves you forward"。

我们可以通过一个称为温度缩放的概念进一步控制分布和选择过程,其中温度缩放只是一个花哨的表达方式,表示将logits除以大于0的数:

```python
def softmax_with_temperature(logits, temperature):
    scaled_logits = logits / temperature
    return torch.softmax(scaled_logits, dim=0)

```

大于1的温度会导致更均匀分布的token概率,而小于1的温度会导致更自信(更尖锐或更峰值)的分布。让我们通过绘制原始概率与不同温度值缩放的概率来说明这一点:

```python
temperatures = [1, 0.1, 5]
scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]
x = torch.arange(len(vocab))
bar_width = 0.15
fig, ax = plt.subplots(figsize=(5, 3))
for i, T in enumerate(temperatures):
    rects = ax.bar(x + i * bar_width, scaled_probas[i],
                   bar_width, label=f'Temperature = {T}')
ax.set_ylabel('Probability')
ax.set_xticks(x)
ax.set_xticklabels(vocab.keys(), rotation=90)
ax.legend()
plt.tight_layout()
plt.show()

```

结果图如图5.14所示。

![Untitled](Images/大模型-从零构建一个大模型/第五章/Untitled13.png)

图5.14 温度为1表示词汇表中每个token的未缩放概率分数。将温度降低到0.1会使分布变得更尖锐,因此最可能的token(这里是"forward")将具有更高的概率分数。反之,将温度增加到5会使分布更加均匀。

温度为1将logits除以1,然后将它们传递给softmax函数以计算概率分数。换句话说,使用温度1与不使用任何温度缩放相同。在这种情况下,token被选择的概率等于原始softmax概率分数,当使用PyTorch的multinomial采样函数时。

然而,如我们在图5.14中看到的,应用非常小的温度(如0.1)会导致更尖锐的分布,使得multinomial函数的行为选择最可能的token(这里:"forward")几乎100%的时间,接近argmax函数的行为。相反,温度为5会导致更均匀的分布,其中其他token被更频繁地选择。这可以为生成的文本添加更多变化,但太高的温度经常会导致无意义的文本。例如,使用温度5会导致文本如"every effort moves you pizza"约4%的时间。

练习5.1
使用print_sampled_tokens函数打印图5.13中所示温度的softmax概率的采样频率。在这些情况下,"pizza"一词被采样的频率如何?你能想到一种更快更准确的方法来确定"pizza"一词被采样的频率吗?

5.3.2 Top-k采样

在上一节中,我们实现了一个概率采样方法,结合温度缩放来增加输出的多样性。我们看到较高的温度值会导致更均匀分布的下一个token概率,这导致更多样化的输出,因为它降低了模型重复选择相同可能token的可能性。这种方法允许探索较不可能但潜在更有趣和创造性的路径在生成过程中。然而,这种方法的一个缺点是它有时会导致语法不正确或完全无意义的输出,如"every effort moves you pizza"。

在本节中,我们介绍另一个称为top-k采样的概念,当与概率采样和温度缩放结合使用时,可以改善文本生成结果。

在top-k采样中,我们可以将采样的token限制为top-k个最可能的token,并通过将其他token的概率分数设为零来排除所有其他token,如图5.15所示。

![Untitled](Images/大模型-从零构建一个大模型/第五章/Untitled14.png)

图5.15 使用top-k采样(k=3),我们专注于与最高logits相关的3个token,并在应用softmax函数之前将所有其他token掩码为负无穷(-inf)。这导致概率分布中为所有非top-k token分配概率值0。

图5.15中概述的方法将所有非选定logits替换为负无穷值(-inf),这样当计算softmax值时,非top-k token的概率分数为0,剩余概率总和为1。(细心的读者可能记得我们在第3章3.5.1节"应用因果注意力掩码"中实现的类似技巧。)

在代码中,我们可以按如下方式实现图5.15中概述的top-k过程,从选择具有最大logit值的token开始:

```python
top_k = 3
top_logits, top_pos = torch.topk(next_token_logits, top_k)
print("Top logits:", top_logits)
print("Top positions:", top_pos)

```

```
Top logits: tensor([6.7500, 6.2800, 4.5100])
Top positions: tensor([3, 7, 0])

```

随后,我们应用PyTorch的where函数来将低于top-3选择中最低logit值的logit值替换为负无穷(-inf)。

```python
new_logits = torch.where(
    condition=next_token_logits < top_logits[-1],
    input=torch.tensor(float('-inf')),
    other=next_token_logits
)
print(new_logits)

```

下一个token在我们9-token词汇表中的结果logits如下:

```
tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])

```

最后,让我们应用softmax函数将这些转换为下一个token概率:

```python
topk_probas = torch.softmax(new_logits, dim=0)
print(topk_probas)

```

正如我们可以看到,这个top-3方法的结果是3个非零概率分数:

```
tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])
```

我们现在可以应用温度缩放和multinomial函数进行概率采样,如上一节所介绍,从这3个非零概率分数中选择下一个token来生成下一个token。我们在下一节通过修改我们的文本生成函数来做到这一点。

5.3.3 修改文本生成函数

前两小节介绍了两个概念来增加LLM生成文本的多样性:温度采样和top-k采样。在本节中,我们结合并使用这些概念来修改我们之前用于让LLM生成文本的generate_simple函数,创建一个新的generate函数:

```
def generate(model, idx, max_new_tokens, context_size,
             temperature=1.0, top_k=None, eos_id=None):
    for _ in range(max_new_tokens):
        idx_cond = idx[:, -context_size:]
        with torch.no_grad():
            logits = model(idx_cond)
        logits = logits[:, -1, :]
        if top_k is not None:
            top_logits, _ = torch.topk(logits, top_k)
            min_val = top_logits[:, -1]
            logits = torch.where(
                logits < min_val,
                torch.tensor(float('-inf')).to(logits.device),
                logits
            )

        if temperature > 0.0:
            logits = logits / temperature
            probs = torch.softmax(logits, dim=-1)
            idx_next = torch.multinomial(probs, num_samples=1)
        else:
            idx_next = torch.argmax(logits, dim=-1, keepdim=True)
        if idx_next == eos_id:
            break
        idx = torch.cat((idx, idx_next), dim=1)
    return idx

```

让我们现在看看这个新的generate函数的运行情况:

```python
torch.manual_seed(123)
token_ids = generate(
    model=model,
    idx=text_to_token_ids("Every effort moves you", tokenizer).to(device),
    max_new_tokens=15,
    context_size=GPT_CONFIG_124M["context_length"],
    top_k=25,
    temperature=1.4
)
print("Output text:\\n", token_ids_to_text(token_ids, tokenizer))

```

生成的文本如下:

```
Output text:
 Every effort moves you stand to work on surprise, a one of us had gone with random-

```

正如我们可以看到,生成的文本与我们之前使用generate_simple函数在5.3节开始时生成的文本("Every effort moves you know," was one of the axioms he laid...!")非常不同,后者是从训练数据中记忆的段落。

练习5.2
尝试不同的温度和top-k设置。基于你的观察,你能想到低温和top-k设置可能更合适的应用吗?反之,你能想到更高温度和top-k设置可能更适合的应用吗?(建议在章节末尾加载预训练权重后再次尝试这个练习。)

练习5.3
你能找到generate函数的不同设置组合来强制确定性行为,也就是说,禁用随机采样使得它总是产生类似generate_simple函数的输出吗?

到目前为止,我们已经讨论了如何训练LLM和如何使用它们生成文本。本章的最后两节将讨论如何保存和加载训练好的LLM以及如何加载OpenAI的预训练权重。

5.4 在PyTorch中加载和保存模型权重

在本章中,我们已经讨论了如何数值评估训练进度并从头开始训练LLM。即使我们的LLM和数据集相对较小,这个过程表明预训练LLM在计算上是昂贵的。因此,能够保存LLM很重要,这样我们就不需要每次想要使用它时都重新运行整个训练过程。

![Untitled](Images/大模型-从零构建一个大模型/第五章/Untitled15.png)

如章节概述中图5.16所示,我们在本节中介绍如何保存和加载训练好的模型。然后,在接下来的部分中,我们将把更强大的预训练OpenAI GPT模型加载到我们的GPTModel实例中。

图5.16 训练和检查模型后,通常有帮助的是保存模型,以便我们以后可以使用或继续训练它,这是本节的主题,之后我们将在本章的最后一节从OpenAI加载预训练模型权重。

幸运的是,保存PyTorch模型相对简单。推荐的方法是保存模型的所谓state_dict,这是一个将每个层映射到其参数的字典,使用torch.save函数如下:

```python
torch.save(model.state_dict(), "model.pth")

```

在前面的代码中,"model.pth"是保存state_dict的文件名。.pth扩展名是PyTorch文件的约定,尽管我们技术上可以使用任何文件扩展名。

然后,在保存模型权重到state_dict后,我们可以将模型权重加载到新的GPTModel实例中如下:

```python
model = GPTModel(GPT_CONFIG_124M)
model.load_state_dict(torch.load("model.pth"))
model.eval()

```

如第4章所讨论,eval()帮助防止模型通过随机"丢弃"一些用户的神经元在训练期间过度拟合。然而,在推理期间,我们不想随机丢弃任何网络已经学到的信息。使用model.eval()将模型切换到评估模式以进行推理,禁用dropout层。

如果我们想稍后继续训练模型,例如,使用我们之前在本章定义的train_model_simple函数,保存优化器状态也是推荐的。

像AdamW这样的自适应优化器为每个模型权重存储额外的参数。AdamW还存储历史数据以动态调整每个模型参数的学习率。没有它,优化器重置,模型可能学习次优或甚至无法正确收敛,这意味着它将失去生成连贯文本的能力。使用torch.save,我们可以同时保存模型和优化器state_dict内容如下:

```python
torch.save({
    "model_state_dict": model.state_dict(),
    "optimizer_state_dict": optimizer.state_dict(),
    },
    "model_and_optimizer.pth"
)

```

然后,我们可以恢复模型和优化器状态如下,首先使用torch.load加载保存的数据,然后使用load_state_dict方法:

```python
checkpoint = torch.load("model_and_optimizer.pth")
model = GPTModel(GPT_CONFIG_124M)
model.load_state_dict(checkpoint["model_state_dict"])
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)
optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
model.train();

```

练习5.4
在保存权重后,加载模型和优化器到新的Python会话或Jupyter笔记本单元格中,并使用train_model_simple函数继续训练它1个额外的epoch。

5.5 从OpenAI加载预训练权重

之前,出于教育目的,我们使用包含短篇小说文本的有限数据集训练了一个小型GPT-2模型。这种方法允许我们专注于基本原理,而无需昂贵的时间和计算资源。

幸运的是,OpenAI公开共享了他们的GPT-2模型权重,让我们避免了需要投入数百甚至数千小时来自己训练模型。

在本节的剩余部分,我们将这些权重加载到GPTModel类中,并使用该模型进行文本生成。这里,权重指的是存储在PyTorch的Linear和Embedding层的.weight属性中的权重参数,例如。我们之前通过model.parameters()在训练模型时访问了这些权重。

在接下来的章节中,我们将使用这些预训练权重来微调模型用于文本分类任务,并遵循类似于ChatGPT的指令。

注意OpenAI最初使用TensorFlow保存了GPT-2权重,我们将使用它来加载Python中的权重。此外,以下代码将使用一个名为tqdm的进度条来跟踪下载过程,我们需要安装它。

我们可以通过在终端中执行以下命令来安装这些库:

```
pip install tensorflow>=2.15.0  tqdm>=4.66

```

下载代码相对较长,大部分不太有趣,也不太相关。因此,不在本章中花费宝贵空间来讨论Python代码用于从互联网获取文件,我们直接从本章的在线存储库下载gpt_download.py Python模块:

```python
import urllib.request
url = (
    "<https://raw.githubusercontent.com/rasbt/>"
    "LLMs-from-scratch/main/ch05/"
    "01_main-chapter-code/gpt_download.py"
)
filename = url.split('/')[-1]
urllib.request.urlretrieve(url, filename)

```

现在,在将此文件下载到当前Python会话的本地目录后,读者应该简要检查该文件的内容,以确保它已正确保存并包含有效的Python代码。

我们现在可以从gpt_download.py文件中导入download_and_load_gpt2函数如下,这将GPT-2架构设置(settings)和权重参数(params)加载到当前Python会话中:

```python
from gpt_download import download_and_load_gpt2
settings, params = download_and_load_gpt2(model_size="124M", models_dir="gpt2")

```

执行前面的代码下载以下7个与124M参数GPT-2模型相关的文件:

```
checkpoint: 100%|███████████████████████████| 77.0/77.0 [00:00<00:00, 63.9kiB/s]
encoder.json: 100%|█████████████████████████| 1.04M/1.04M [00:00<00:00, 2.20MiB/s]
hprams.json: 100%|██████████████████████████| 90.0/90.0 [00:00<00:00, 78.3kiB/s]
model.ckpt.data-00000-of-00001: 100%|███████| 498M/498M [01:09<00:00, 7.16MiB/s]
model.ckpt.index: 100%|█████████████████████| 5.21k/5.21k [00:00<00:00, 3.24MiB/s]
model.ckpt.meta: 100%|██████████████████████| 471k/471k [00:00<00:00, 2.46MiB/s]
vocab.bpe: 100%|████████████████████████████| 456k/456k [00:00<00:00, 1.70MiB/s]

```

更新的下载说明

如果下载代码由于某种原因不工作,这可能是由于间歇性的互联网连接、服务器问题或OpenAI如何共享开源GPT-2模型权重的变化。在这种情况下,请访问本章的在线代码存储库https://github.com/rasbt/LLMs-from-scratch 获取替代和更新的说明,并请查看问答部分获取进一步指导。

前面代码的执行完成后,让我们检查settings和params的内容:

```python
print("Settings:", settings)
print("Parameter dictionary keys:", params.keys())

```

内容如下:

```
Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}
Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])

```

这些settings和params是Python字典。settings字典存储LLM架构设置,类似于我们手动定义的GPT_CONFIG_124M设置。params字典包含实际的权重张量。注意我们避免打印权重内容,因为打印权重会占用太多屏幕空间,但是我们可以通过打印整个字典print(params)或通过选择各自字典键的单个张量来检查这些权重张量,例如嵌入层权重:

```python
print(params["wte"])
print("Token embedding weight tensor dimensions:", params["wte"].shape)

```

token嵌入层的权重如下:

```
[[-0.11010301 ... -0.1363697   0.01506208   0.04531523]
 [ 0.04034033 ...  0.08605453  0.00253983   0.04318958]
 [-0.12746179  ...  0.08991534 -0.12972379 -0.08785918]
 ...
 [-0.04453601 ...   0.10435229  0.09783269 -0.06952604]
 [ 0.1860082  ...  -0.09625227  0.07847701 -0.02245961]
 [ 0.05135201 ...   0.00704835  0.15519823  0.12067825]]
Token embedding weight tensor dimensions: (50257, 768)

```

我们下载并加载了最小的GPT-2模型权重,使用download_and_load_gpt2(model_size="124M", ...)设置。然而,请注意OpenAI也共享了更大模型的权重:"355M"、"774M"和"1558M"。这些不同大小的GPT模型的整体架构是相同的,如图5.17所示。

![Untitled](Images/大模型-从零构建一个大模型/第五章/Untitled17.png)

图5.17 GPT-2 LLM有几种不同的模型大小,范围从1.24亿到15.58亿参数。核心架构是相同的,唯一的区别是嵌入大小和单个组件(如注意力头和transformer块)重复的次数。

如图5.17所示,不同大小的GPT-2模型的整体架构保持相同,除了不同的架构元素重复不同次数,嵌入大小也不同。本章剩余部分的代码与这些更大的模型兼容。

将GPT-2模型权重加载到Python后,我们仍然需要将它们从settings和params字典转移到GPTModel实例中。

首先,我们创建一个字典,列出不同GPT模型大小之间的差异,如图5.17所解释:

```python
model_configs = {
    "gpt2-small (124M)": {"emb_dim": 768, "n_layers": 12, "n_heads": 12},
    "gpt2-medium (355M)": {"emb_dim": 1024, "n_layers": 24, "n_heads": 16},
    "gpt2-large (774M)": {"emb_dim": 1280, "n_layers": 36, "n_heads": 20},
    "gpt2-xl (1558M)": {"emb_dim": 1600, "n_layers": 48, "n_heads": 25},
}

```

假设我们对加载最小的模型"gpt2-small (124M)"感兴趣。我们可以使用model_configs表中的相应设置来更新我们之前在本章中定义和使用的完整GPT_CONFIG_124M,如下所示:

```python
model_name = "gpt2-small (124M)"
NEW_CONFIG = GPT_CONFIG_124M.copy()
NEW_CONFIG.update(model_configs[model_name])

```

细心的读者可能记得我们之前使用了256-token长度,但原始GPT-2模型是用1,024-token长度训练的,所以我们需要相应地更新NEW_CONFIG:

```python
NEW_CONFIG.update({"context_length": 1024})

```

此外,OpenAI使用了存储在多头注意力模块的线性层中的bias向量来实现查询、键和值矩阵计算。这些向量在LLM中并不常用,因为它们不改善建模性能并且是不必要的。然而,由于我们正在使用预训练权重,我们需要匹配设置以保持一致性并启用这些bias向量:

```python
NEW_CONFIG.update({"qkv_bias": True})

```

我们现在可以使用更新后的NEW_CONFIG字典来初始化新的GPTModel实例:

```python
gpt = GPTModel(NEW_CONFIG)
gpt.eval()

```

默认情况下,GPTModel实例是用随机权重初始化用于训练的。使用OpenAI模型权重的关键是用我们加载到params字典中的权重替换这些随机权重。

为此,我们首先定义一个小的assign实用函数,检查我们的张量或数组(left和right)是否具有相同的维度或形状,并返回right张量作为PyTorch参数:

```python
def assign(left, right):
    if left.shape != right.shape:
        raise ValueError(f"Shape mismatch. Left: {left.shape}, Right: {right.shape}")
    return torch.nn.Parameter(torch.tensor(right))

```

然后,我们定义一个load_weights_into_gpt函数,该函数将权重从params字典加载到GPTModel实例gpt中:

```python
import numpy as np

def load_weights_into_gpt(gpt, params):
    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])
    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])

    for b in range(len(params["blocks"])):
        q_w, k_w, v_w = np.split(
            (params["blocks"][b]["attn"]["c_attn"])["w"], 3, axis=-1)
        gpt.trf_blocks[b].att.W_query.weight = assign(
            gpt.trf_blocks[b].att.W_query.weight, q_w.T)
        gpt.trf_blocks[b].att.W_key.weight = assign(
            gpt.trf_blocks[b].att.W_key.weight, k_w.T)
        gpt.trf_blocks[b].att.W_value.weight = assign(
            gpt.trf_blocks[b].att.W_value.weight, v_w.T)

        q_b, k_b, v_b = np.split(
            (params["blocks"][b]["attn"]["c_attn"])["b"], 3, axis=-1)
        gpt.trf_blocks[b].att.W_query.bias = assign(
            gpt.trf_blocks[b].att.W_query.bias, q_b)
        gpt.trf_blocks[b].att.W_key.bias = assign(
            gpt.trf_blocks[b].att.W_key.bias, k_b)
        gpt.trf_blocks[b].att.W_value.bias = assign(
            gpt.trf_blocks[b].att.W_value.bias, v_b)

        gpt.trf_blocks[b].att.out_proj.weight = assign(
            gpt.trf_blocks[b].att.out_proj.weight,
            params["blocks"][b]["attn"]["c_proj"]["w"].T)
        gpt.trf_blocks[b].att.out_proj.bias = assign(
            gpt.trf_blocks[b].att.out_proj.bias,
            params["blocks"][b]["attn"]["c_proj"]["b"])

        gpt.trf_blocks[b].ff.layers[0].weight = assign(
            gpt.trf_blocks[b].ff.layers[0].weight,
            params["blocks"][b]["mlp"]["c_fc"]["w"].T)
        gpt.trf_blocks[b].ff.layers[0].bias = assign(
            gpt.trf_blocks[b].ff.layers[0].bias,
            params["blocks"][b]["mlp"]["c_fc"]["b"])
        gpt.trf_blocks[b].ff.layers[2].weight = assign(
            gpt.trf_blocks[b].ff.layers[2].weight,
            params["blocks"][b]["mlp"]["c_proj"]["w"].T)
        gpt.trf_blocks[b].ff.layers[2].bias = assign(
            gpt.trf_blocks[b].ff.layers[2].bias,
            params["blocks"][b]["mlp"]["c_proj"]["b"])

        gpt.trf_blocks[b].norm1.scale = assign(
            gpt.trf_blocks[b].norm1.scale,
            params["blocks"][b]["ln_1"]["g"])
        gpt.trf_blocks[b].norm1.shift = assign(
            gpt.trf_blocks[b].norm1.shift,
            params["blocks"][b]["ln_1"]["b"])
        gpt.trf_blocks[b].norm2.scale = assign(
            gpt.trf_blocks[b].norm2.scale,
            params["blocks"][b]["ln_2"]["g"])
        gpt.trf_blocks[b].norm2.shift = assign(
            gpt.trf_blocks[b].norm2.shift,
            params["blocks"][b]["ln_2"]["b"])

    gpt.final_norm.scale = assign(gpt.final_norm.scale, params["g"])
    gpt.final_norm.shift = assign(gpt.final_norm.shift, params["b"])
    gpt.out_head.weight = assign(gpt.out_head.weight, params["wte"])

```

在load_weights_into_gpt函数中,我们仔细地将OpenAI的实现中的权重与GPTModel实现匹配。作为一个具体的例子,OpenAI将第一个transformer块的输出投影层的权重张量存储为params["blocks"][0]["attn"]["c_proj"]["w"]。在我们的实现中,这个权重张量对应于gpt.trf_blocks[b].att.out_proj.weight,其中gpt是GPTModel实例。

实现load_weights_into_gpt函数需要一些猜测,因为OpenAI使用略微不同的命名约定。然而,assign函数会警告我们是否试图匹配具有不同维度的张量。此外,如果我们在这个函数中犯了错误,我们会注意到这一点,因为结果GPT模型将无法产生连贯的文本。

让我们现在使用load_weights_into_gpt函数,并将OpenAI模型权重加载到GPTModel实例gpt中:

```python
load_weights_into_gpt(gpt, params)
gpt.to(device)

```

如果模型加载正确,我们现在可以使用它来使用我们之前的generate函数生成新文本:

```python
torch.manual_seed(123)
token_ids = generate(
    model=gpt,
    idx=text_to_token_ids("Every effort moves you", tokenizer),
    max_new_tokens=25,
    context_size=NEW_CONFIG["context_length"],
    top_k=50,
    temperature=1.5
)
print("Output text:\\n", token_ids_to_text(token_ids, tokenizer))

```

结果文本如下:

```
Output text:
 Every effort moves you toward finding an ideal new way to practice something!
What makes us want to be on top of that?

```

我们可以确认我们正确加载了模型权重,因为模型可以产生连贯的文本。这个过程中的任何错误都会导致模型失败。

在接下来的章节中,我们将进一步使用这个预训练模型,并微调它以分类文本和遵循指令。

练习5.5
计算GPTModel与从OpenAI预训练权重在"The Verdict"数据集上的训练和验证集损失。

练习5.6
读者被鼓励尝试不同大小的GPT-2模型,例如,最大的1558M参数模型,并比较生成的文本与我们在本章加载的124M模型。

5.6 总结

- 当LLM生成文本时,它们一次输出一个token。
- 默认情况下,下一个token是通过将模型输出转换为概率分数并从词汇表中选择对应最高概率分数的token来生成的,这被称为"贪婪解码"。
- 使用概率采样和温度缩放,我们可以影响生成文本的多样性和连贯性。
- 训练集和验证集损失可以用来衡量LLM在训练期间生成文本的质量。
- 预训练LLM涉及改变其权重以最小化训练损失。
- LLM的训练循环本身是深度学习中的标准程序,使用常规的交叉熵损失和AdamW优化器。
- 在大型文本语料库上预训练LLM是耗时和资源密集的,所以我们可以加载OpenAI公开可用的权重,作为自己在大型数据集上预训练模型的替代方案。
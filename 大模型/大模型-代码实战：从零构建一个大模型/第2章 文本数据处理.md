# 第2章 文本数据处理

在本章中,我们将深入探讨如何为大型语言模型(LLM)的训练准备文本数据。主要内容包括:

- 文本预处理及分词技术
- 词和子词的编码方法
- 基于字节对编码(BPE)的高级分词算法
- 使用滑动窗口采样训练样本
- 将分词结果转换为向量表示,作为LLM的输入

在第1章中,我们概述了大型语言模型的基本架构,并了解到它们是在海量文本数据上进行预训练的。我们重点关注了基于Transformer架构的仅解码器LLM,这也是ChatGPT等流行GPT类模型的基础。

在预训练阶段,LLM会逐词处理文本。通过在包含数百万到数十亿参数的模型上进行下一个词预测任务的训练,可以得到具有令人印象深刻能力的模型。这些模型随后可以进一步微调,以遵循通用指令或执行特定的目标任务。但在实现和训练LLM之前,我们需要准备训练数据集,这正是本章的重点,如图2.1所示。

![Untitled](Images/大模型-从零构建一个大模型/第二章/Untitled.png)

图2.1 LLM开发的三个主要阶段:编码实现、在通用文本数据集上预训练,以及在标记数据集上微调。本章将解释并实现用于LLM预训练的数据准备和采样流程。

在本章中,我们将学习如何为训练LLM准备输入文本。这涉及将文本拆分为单独的词和子词标记,然后将其编码为LLM的向量表示。我们还将学习高级分词方案,如字节对编码(BPE),这在GPT等流行的LLM中得到了广泛应用。最后,我们将实现一种采样和数据加载策略,以生成用于在后续章节中训练LLM的输入-输出对。

## 2.1 理解词嵌入

像LLM这样的神经网络模型无法直接处理原始文本。由于文本是分类的,与用于实现和训练神经网络的数学运算不兼容。因此,我们需要一种方法将词表示为连续值向量。(关于向量和张量在计算上下文中的更多基础知识,可以参考附录C中的C2.2节"理解张量"。)

将词转换为向量格式的概念通常被称为嵌入。通过神经网络层或其他预训练神经网络模型,我们可以嵌入不同的数据类型,例如视频、音频和文本,如图2.2所示。

![Untitled](Images/大模型-从零构建一个大模型/第二章/Untitled1.png)

图2.2 深度学习模型无法以原始形式处理视频、音频和文本等数据格式。因此,我们使用嵌入模型将这些原始数据转换为深度学习架构可以轻松理解和处理的密集向量表示。具体来说,该图说明了将原始数据转换为三维数值向量的过程。

如图2.2所示,我们可以使用嵌入模型处理各种不同的原始格式。然而,重要的是要知道不同的数据格式需要不同的嵌入模型。例如,为文本设计的嵌入模型不适合嵌入音频或视频数据。

简而言之,嵌入是将离散对象(如词、图像或整个文档)映射到连续向量空间中的点的过程 - 嵌入的主要目的是将非数值数据转换为神经网络可以处理的格式。

虽然词嵌入是最常见的文本嵌入类型,但也存在句子、段落或整个文档的嵌入。句子或段落嵌入是检索增强生成的流行选择。检索增强生成将生成(如产生文本)与检索(如搜索外部知识库)相结合,以在生成文本时找到相关信息,这是一种超出本书范围的技术。由于我们的目标是训练GPT类型的LLM,它们学习一次生成一个词的文本,本章将重点关注词嵌入。

存在几种算法和框架,它们被开发用来生成词嵌入。其中一个早期且非常流行的例子是Word2Vec方法。Word2Vec训练神经网络架构,通过预测给定目标词的上下文或给定上下文的目标词来生成词嵌入。Word2Vec背后的主要思想是,在相似上下文中出现的词往往具有相似含义。因此,当投影到2维词嵌入空间进行可视化时,我们可以看到相似的词聚集在一起,如图2.3所示。

![Untitled](Images/大模型-从零构建一个大模型/第二章/Untitled2.png)

图2.3 如果词嵌入是二维的,我们可以将它们绘制在二维散点图中进行可视化,如图所示。当使用词嵌入技术(如Word2Vec)时,对应于相似概念的词通常在嵌入空间中彼此接近出现。例如,不同类型的鸟类在嵌入空间中比国家和城市更接近。

词嵌入可以有不同的维度,从几十到几千不等。如图2.3所示,我们可以选择二维词嵌入进行可视化目的。更高的维度可能会捕捉更多细微的关系,但以计算效率为代价。

虽然我们可以使用预训练模型(如Word2Vec)为机器学习模型生成嵌入,但LLM通常会在其输入层产生自己的嵌入,并在训练过程中更新它们。优化我们自己的嵌入作为LLM训练的一部分而不是使用Word2Vec的优势在于,嵌入可以针对特定任务和域进行优化。我们将在本章后面实现这样的嵌入层。此外,LLM可以创建上下文化的输出嵌入,我们将在第3章中讨论。

不幸的是,高维嵌入对可视化提出了挑战,因为人类感知和常见图形表示固有地限于三维或更少,这就是为什么图2.3显示了二维嵌入在二维散点图中。然而,在使用LLM时,我们通常使用比图2.3所示维度更高得多的嵌入。对于GPT-2和GPT-3,嵌入大小(通常称为模型的隐藏状态)因特定模型变体而异。这是性能和效率之间的权衡。最小的GPT-2模型(117M和125M参数)使用768维的嵌入大小作为具体示例。最大的GPT-3模型(175B参数)使用12,288维的嵌入大小。

本章剩余部分将介绍为LLM准备嵌入所需的步骤,包括将文本分割成词、将词转换为标记,以及将标记转换为嵌入向量。

## 2.2 文本分词

本节介绍如何将输入文本拆分为单独的标记,这是为LLM生成嵌入的必要预处理步骤。这些标记可以是单独的词或特殊字符,包括标点符号,如图2.4所示。

![Untitled](Images/大模型-从零构建一个大模型/第二章/Untitled3.png)

图2.4 本节涵盖的文本处理步骤在LLM上下文中的视图。在这里,我们将输入文本拆分为单独的标记,这些标记要么是词,要么是特殊字符,如标点符号。在接下来的部分中,我们将文本转换为标记ID并创建标记嵌入。

我们将用于LLM训练的文本是一个名为《判决》的短篇小说,作者是亨利·詹姆斯。这篇小说已进入公共领域,可以自由用于LLM训练任务。该文本可在Wikisource上获取,链接为https://en.wikisource.org/wiki/The_Verdict。你可以复制并粘贴到文本文件中,我将其保存为名为"the-verdict.txt"的文件,以便使用Python的标准文件读取实用程序:

```python
# 读取短篇小说作为文本示例到Python中
with open("the-verdict.txt", "r", encoding="utf-8") as f:
    raw_text = f.read()
print("总字符数:", len(raw_text))
print(raw_text[:99])

```

输出:

```
总字符数: 20479
I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no

```

现在我们的任务是将这个20,479字符的短篇小说分割成单独的词和特殊字符,以便我们可以将它们转换为嵌入,用于后续章节的LLM训练。

> 文本样本大小
> 
> 请注意,我们通常会处理数百万甚至数十亿个单词 - 数千兆字节的文本 - 当使用LLM时。然而,出于教育目的,使用较小的文本样本(如单个短篇小说)就足够了,以说明文本处理步骤背后的主要思想,并使其尽可能简单,以便在合理的时间内在普通硬件上运行。

那么我们如何将这段文本拆分为标记列表呢?首先,让我们看一个小例句,并使用Python的正则表达式库进行说明。(请注意,你不需要学习或记忆任何正则表达式模式,因为我们稍后将过渡到一个现成的分词器。)

从这个简单示例开始,我们可以使用re.split命令和以下语法来将文本按空白字符拆分:

```python
import re
text = "Hello, world. This, is a test."
result = re.split(r'(\\s)', text)
print(result)

```

结果是一个包含单独词、空格和标点符号的列表:

```
['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']

```

请注意,上面的简单分词方案主要适用于将示例文本分割成单独的词,但是一些词仍然连接着标点符号,我们希望将其作为单独的项目。我们保留了标点符号,因为它们帮助LLM区分专有名词和普通名词、理解句子结构,并学会生成具有适当大小写的文本。

我已经修改了正则表达式拆分以包括空格(\s)和逗号、句号([,.]):

```python
result = re.split(r'([,.]|\\s)', text)
print(result)

```

我们可以看到,词和标点字符现在被分成了我们想要的单独项目:

```
['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']

```

一个小问题是,列表中仍然包含空白字符。我们可以通过以下方式轻松移除这些多余的字符:

```python
result = [item for item in result if item.strip()]
print(result)

```

去除空白后的输出如下:

```
['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']

```

> 是否移除空格?
> 
> 
> 在开发简单的分词器时,是将空格编码为单独的字符还是直接移除取决于应用和其要求。移除空格可以减少内存和计算需求。但是,如果我们训练的模型对文本的确切结构敏感(例如,Python代码对缩进和空格敏感),那么保留空格可能会很有用。这里,我们移除空格以简化和缩短我们的标记化输出。之后,我们将切换到一种包含空格的分词方案。
> 

我们设计的分词方案在简单的示例文本上运行良好。让我们稍微修改一下,使其能够处理其他类型的标点符号,如问号、引号和破折号,这些在亨利·詹姆斯短篇小说的前100个字符中出现,以及额外的特殊字符:

```python
text = "Hello, world. Is this-- a test?"
result = re.split(r'([,.:;?_!"()\\']|--|\\s)', text)
result = [item.strip() for item in result if item.strip()]
print(result)

```

输出结果如下:

```
['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']

```

正如我们从图2.5中总结的结果可以看到,分词方案现在可以成功处理文本中的各种特殊字符。

![Untitled](Images/大模型-从零构建一个大模型/第二章/Untitled4.png)

图2.5 我们实现的分词方案将文本拆分为单独的词和标点符号。在此图中显示的特定示例中,样本文本被拆分为10个独立的标记。

现在我们有了一个基本的分词器工作,让我们将它应用到亨利·詹姆斯的整个短篇小说:

```python
preprocessed = re.split(r'([,.:;?_!"()\\']|--|\\s)', raw_text)
preprocessed = [item.strip() for item in preprocessed if item.strip()]
print(len(preprocessed))

```

上面的print语句输出4690,这是文本中的标记数量(不包括空白)。

让我们打印前30个标记进行快速视觉检查:

```python
print(preprocessed[:30])

```

结果输出显示分词器似乎正在正确处理文本,因为词和特殊字符被很好地分开了:

```
['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']

```

## 2.3 将标记转换为标记ID

在上一节中,我们将亨利·詹姆斯的短篇小说分词成了单独的标记。在本节中,我们将这些标记从Python字符串转换为整数表示,以产生所谓的标记ID。这种转换是在将标记ID转换为嵌入向量之前的一个中间步骤。

要将我们之前生成的标记转换为标记ID,我们首先需要构建一个所谓的词汇表。这个词汇表定义了如何将每个唯一的词和特殊字符映射到一个唯一的整数,如图2.6所示。

![Untitled](Images/大模型-从零构建一个大模型/第二章/Untitled5.png)

图2.6 我们通过将训练数据集中的整个文本分词成单独的标记来构建词汇表。然后将这些单独的标记按字母顺序排序,并删除重复的标记。然后将唯一的标记聚合成一个词汇表,该词汇表定义了从每个唯一标记到唯一整数值的映射。为了说明目的,所描述的词汇表故意很小,并且为了简单起见不包含标点符号或特殊字符。

在上一节中,我们将亨利·詹姆斯的短篇小说分词并将其分配给一个名为preprocessed的Python变量。让我们现在创建一个所有唯一标记的列表并按字母顺序排序,以确定词汇表大小:

```python
all_words = sorted(set(preprocessed))
vocab_size = len(all_words)
print(vocab_size)

```

在确定词汇表大小为1,130后,让我们创建词汇表并打印其前51个条目以供说明:

```python
vocab = {token:integer for integer,token in enumerate(all_words)}
for i, item in enumerate(vocab.items()):
    print(item)
    if i > 50:
        break

```

输出如下:

```
('!', 0)
('"', 1)
("'", 2)
...
('Her', 49)
('Hermia', 50)

```

正如我们从上面的输出中看到的,字典包含与唯一整数标签相关联的单独标记。下一步是应用这个词汇表将文本转换为标记ID,如图2.7所示。

![Untitled](Images/大模型-从零构建一个大模型/第二章/Untitled5.png)

图2.7 从一个新的文本样本开始,我们对文本进行分词,并使用词汇表将文本标记转换为标记ID。词汇表是从整个训练集构建的,可以应用于训练集本身和任何新的文本样本。为了简单起见,所描述的词汇表不包含标点符号或特殊字符。

记住在这一点上,当我们需要将LLM的输出从数字转回文本时,我们只需要一种将标记ID转回相应文本标记的方法。为此,我们可以创建词汇表的反向版本,将标记ID映射回相应的文本标记。

让我们在Python中实现一个完整的分词器类,其中包含一个encode方法,该方法将文本分割成标记并应用字符串到整数映射来生成标记ID和词汇表。此外,我们实现了一个decode方法,该方法应用相反的整数到字符串映射将标记ID转回文本。

这个分词器实现的代码如下所示:

```python
class SimpleTokenizerV1:
    def __init__(self, vocab):
        self.str_to_int = vocab
        self.int_to_str = {i:s for s,i in vocab.items()}

    def encode(self, text):
        preprocessed = re.split(r'([,.?_!"()\\']|--|\\s)', text)
        preprocessed = [item.strip() for item in preprocessed if item.strip()]
        ids = [self.str_to_int[s] for s in preprocessed]
        return ids

    def decode(self, ids):
        text = " ".join([self.int_to_str[i] for i in ids])

        text = re.sub(r'\\s+([,.?!"()\\'])', r'\\1', text)
        return text

```

使用上面的SimpleTokenizerV1 Python类,我们现在可以实例化一个分词器对象并使用一个预定义的词汇表,我们可以用它来编码和解码文本,如图2.8所示。

![Untitled](Images/大模型-从零构建一个大模型/第二章/Untitled7.png)

图2.8 分词器实现共享两个常见方法:encode方法和decode方法。encode方法接收样本文本,将其分割成单独的标记,并通过词汇表将标记转换为标记ID。decode方法接收标记ID,将它们转回文本标记,并将文本标记连接成自然文本。

让我们实例化一个新的分词器对象并从亨利·詹姆斯的短篇小说中分词一段文字来尝试一下:

```python
tokenizer = SimpleTokenizerV1(vocab)
text = """"It's the last he painted, you know," Mrs. Gisburn said with pardonable pride."""
ids = tokenizer.encode(text)
print(ids)

```

上面的代码打印以下标记ID:

```
[1, 2, 13, 2, 4, 349, 47, 6, 252, 5, 345, 588, 5, 2, 48, 7, 37, 261, 484, 1163, 670, 1265, 667, 7]

```

现在,让我们看看是否可以使用decode方法将这些标记ID转回文本:

```python
print(tokenizer.decode(ids))

```

这会输出以下文本:

```
'" It\\' s the last he painted, you know," Mrs. Gisburn said with pardonable pride.'

```

基于上面的输出,我们可以看到decode方法成功地将标记ID转换回原始文本。

到目前为止,很好。我们实现了一个能够标记化和重新标记化文本的分词器,基于从训练文本中提取的模式。让我们现在应用它到一个不是来自训练文本的新文本样本:

```python
text = "Hello, do you like tea?"
print(tokenizer.encode(text))

```

执行上面的代码将导致以下错误:

```
...
KeyError: 'Hello'

```

问题在于单词"Hello"没有出现在《判决》短篇小说中。因此,它不在词汇表中。这突显了需要考虑未登录词并修订训练数据以扩展我们的词汇表,当使用LLM时。

在下一节中,我们将进一步改进分词器以处理包含未知词的文本,我们还将讨论可以用来为LLM提供更多上下文的其他特殊标记。

## 2.4 添加特殊上下文标记

在上一节中,我们实现了一个简单的分词器并将其应用于来自训练文本的一段文字。在本节中,我们将修改该分词器以处理未知词。

具体来说,我们将修改在上一节中实现的词汇表和分词器SimpleTokenizerV2,以支持两个新标记,<|unk|>和<|endoftext|>,如图2.9所示。
![Untitled](Images/大模型-从零构建一个大模型/第二章/Untitled8.png)

图2.9 我们向词汇表添加特殊标记以处理某些上下文。例如,我们添加一个<|unk|>标记来表示不是训练数据一部分且因此不在现有词汇表中的新的和未知的词。此外,我们添加一个<|endoftext|>标记,我们可以用它来分隔两个不相关的文本源。

如图2.9所示,我们可以修改分词器以添加一个<|unk|>标记,如果它遇到一个不在词汇表中的词。此外,我们添加一个在不相关文本之间的标记。例如,当训练GPT类型的LLM时,在多个独立的文档或书籍上,通常会在每个文档前插入一个标记,以确保模型知道后面跟着一个新的文本源,如图2.10所示。这有助于我们的LLM理解,尽管这些文本源为了训练而被连接在一起,但它们实际上是不相关的。

![Untitled](Images/大模型-从零构建一个大模型/第二章/Untitled9.png)

图2.10 在处理多个独立的文本源时,我们在这些文本之间添加<|endoftext|>标记。这些<|endoftext|>标记充当标记,表示特定段落的开始或结束,允许LLM更有效地处理和理解。

让我们现在修改词汇表以包括这两个新的特殊标记,<|unk|>和<|endoftext|>,方法是将它们添加到我们在上一节中创建的所有唯一词的列表中:

```python
all_tokens = sorted(list(set(preprocessed)))
all_tokens.extend(["<|endoftext|>", "<|unk|>"])
vocab = {token:integer for integer,token in enumerate(all_tokens)}

print(len(vocab.items()))

```

根据上面print语句的输出,新词汇表大小为1161(上一节中的词汇表大小为1159)。

作为额外的快速检查,让我们打印更新后词汇表的最后5个条目:

```python
for i, item in enumerate(list(vocab.items())[-5:]):
    print(item)

```

上面的代码打印以下内容:

```
('younger', 1156)
('your', 1157)
('yourself', 1158)
('<|endoftext|>', 1159)
('<|unk|>', 1160)

```

根据上面的输出,我们可以确认两个新的特殊标记已成功合并到词汇表中。现在,让我们相应地调整从清单2.3的分词器,如清单2.4所示:

```python
class SimpleTokenizerV2:
    def __init__(self, vocab):
        self.str_to_int = vocab
        self.int_to_str = { i:s for s,i in vocab.items()}

    def encode(self, text):
        preprocessed = re.split(r'([,.?_!"()\\']|--|\\s)', text)
        preprocessed = [item.strip() for item in preprocessed if item.strip()]
        preprocessed = [item if item in self.str_to_int
                        else "<|unk|>" for item in preprocessed]

        ids = [self.str_to_int[s] for s in preprocessed]
        return ids

		def decode(self, ids):
        text = " ".join([self.int_to_str[i] for i in ids])

        text = re.sub(r'\\s+([,.?!"()\\'])', r'\\1', text)
        return text
```

与我们在清单2.3中实现的SimpleTokenizerV1相比,新的SimpleTokenizerV2将未知词替换为<|unk|>标记。

让我们创建两个包含未知词的文本样本,并用<|endoftext|>标记将它们连接起来:

```python
text1 = "Hello, do you like tea?"
text2 = "In the sunlit terraces of the palace."
text = " <|endoftext|> ".join((text1, text2))
print(text)

```

输出如下:

```
'Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.'

```

现在,让我们使用我们之前在清单2.2中创建的SimpleTokenizerV2对样本文本进行标记化:

```python
tokenizer = SimpleTokenizerV2(vocab)
print(tokenizer.encode(text))

```

这会打印以下标记ID:

```
[1160, 5, 362, 1155, 642, 1000, 10, 1159, 57, 1013, 981, 1009, 738, 1013, 1160, 7]

```

我们可以看到,标记ID列表包含1159作为<|endoftext|>分隔符标记以及两个1160标记,这些被用于未知词。

```python
print(tokenizer.decode(tokenizer.encode(text)))

```

输出如下:

```
'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'

```

通过比较重新标记化的文本和原始输入文本,我们知道训练数据集(亨利·詹姆斯的短篇小说《判决》)不包含单词"Hello"和"palace"。

到目前为止,我们讨论了标记化作为处理文本作为LLM输入的重要步骤。根据LLM,可能会考虑额外的特殊标记,例如以下:

- [BOS] (序列开始): 这个标记标记文本的开始。它向LLM发出一段内容开始的信号。
- [EOS] (序列结束): 这个标记位于文本的末尾,当连接多个不相关的文本时特别有用,类似于<|endoftext|>。例如,当组合两个不同的维基百科文章或书籍时,EOS标记表示一篇文章何时结束,下一篇何时开始。
- [PAD] (填充): 当用大于1的批次大小训练LLM时,批次可能包含不同长度的文本。为确保所有文本具有相同的长度,较短的文本使用[PAD]标记"填充"到批次中最长文本的长度。

请注意,我们为GPT模型开发的分词器没有所有这些上面提到的标记,只有一个<|endoftext|>标记用于简化。<|endoftext|>类似于上面提到的[EOS]标记。此外,<|endoftext|>也用于填充。但是,正如我们将在后续章节中探讨的,当批量训练时,我们通常使用掩码,这意味着我们不需要填充标记。因此,特殊标记的选择成为设计决策。

此外,我们为GPT模型开发的分词器没有用于词汇外词的<|unk|>标记。相反,GPT模型使用一种更高级的编码分词器,它将未知词分解为子词单元,我们将在下一节中讨论。

## 2.5 字节对编码

我们在前几节中实现了一个简单的标记化方案用于说明目的。本节介绍一种更复杂的标记化方案,基于一个称为字节对编码(BPE)的概念。本节介绍的BPE分词器用于训练如GPT-2、GPT-3和原始模型用于ChatGPT的LLM。

由于实现BPE可能相对复杂,我们将使用一个现有的Python第三方库称为tiktoken ([https://github.com/openai/tiktoken](https://github.com/openai/tiktoken)),它基于Rust中的源代码非常高效地实现了BPE算法。与其他Python库类似,我们可以通过Python的pip安装器从终端安装tiktoken库:

```
pip install tiktoken

```

本章中的代码基于tiktoken 0.5.1。你可以使用以下代码检查版本并确保正确安装:

```python
from importlib.metadata import version
import tiktoken
print("tiktoken version:", version("tiktoken"))

```

安装后,我们可以如下实例化BPE分词器:

```python
tokenizer = tiktoken.get_encoding("gpt2")

```

这个分词器的使用类似于我们之前实现的SimpleTokenizerV2,有一个encode方法:

```python
text = "Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace."
integers = tokenizer.encode(text, allowed_special={"<|endoftext|>"})
print(integers)

```

上面的代码打印以下标记ID:

```
[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]

```

我们可以使用decode方法将标记ID转回文本,类似于我们的SimpleTokenizerV2:

```python
strings = tokenizer.decode(integers)
print(strings)

```

上面的代码打印以下内容:

```
'Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.'

```

我们可以根据上面的标记ID和解码的文本做出几个重要观察。首先,<|endoftext|>标记被分配了一个相对较大的标记ID,即50256。实际上,GPT分词器(用于训练如GPT-2、GPT-3和ChatGPT使用的原始模型)有一个总词汇表大小为50,257,<|endoftext|>被分配最大的标记ID。

第二,BPE分词器能够编码和解码未知词,如"someunknownPlace"。BPE分词器如何在不使用<|unk|>标记的情况下处理任何未知词?

BPE算法的基本思想是将不在其预定义词汇表中的词分解成更小的子词单位或单个字符,使其能够处理词汇外的词。因此,归功于BPE算法,如果分词器遇到未知词,它可以将其解释为一系列子词标记或字符,如图2.11所示。

![Untitled](Images/大模型-从零构建一个大模型/第二章/Untitled10.png)

图2.11 BPE分词器将未知词分解为子词和单个字符。这样,BPE分词器可以解析任何词,不需要用特殊标记替换未知词,如<|unk|>。

如图2.11所示,将未知词分解为单个字符的能力确保了分词器,因此也是用它训练的LLM,可以处理任何文本,即使它包含在其训练数据中从未见过的词。

> 练习 2.1 未知词的字节对编码
> 
> 
> 使用tiktoken库中的BPE分词器对未知词"Birkbeck"和"tkj"进行标记,并打印单独的标记ID。然后,使用decode函数将这个列表转换为结果字符串,以重现图2.11中所示的映射。最后,使用decode方法对标记ID进行解码,检查它是否能重构原始输入"Birkbeck tkj"。
> 

BPE的详细讨论和实现超出了本书的范围,但简而言之,它通过迭代地合并频繁出现的字符对来构建词汇表。例如,BPE开始时将所有单个ASCII字符添加到其词汇表中("a", "b", ...)。在下一步,它合并频繁一起出现的字符组合成子词。例如,"p"和"o"可以合并成子词"po",这在许多英语词中很常见,如"pencil"、"appoint"、"poem"和"opinion"。合并过程由频率因子决定。

## 2.6 使用滑动窗口进行数据采样

前面的部分详细介绍了标记化步骤和从字符串标记到整数标记ID的转换。在我们最终创建LLM的嵌入之前的下一步是生成训练LLM所需的输入-目标对。

这些输入-目标对是什么样的?正如我们在第1章中了解到的,LLM通过预测文本中的下一个词来训练,如图2.12所示。

![Untitled](Images/大模型-从零构建一个大模型/第二章/Untitled11.png)

图2.12 给定一个文本样本,提取输入块作为子样本,作为LLM的输入,LLM在训练期间的预测任务是预测跟在输入块之后的下一个词。在训练期间,我们掩盖了目标之后的所有词。请注意,此图中显示的文本在LLM处理之前会经过标记化;但是,为了清晰起见,本图省略了标记化步骤。

在本节中,我们将实现一个数据加载器,使用滑动窗口方法从训练数据集中提取图2.12中描述的输入-目标对。

作为开始,我们将首先使用我们在前面部分引入的BPE分词器对整个《判决》短篇小说进行标记化:

```python
with open("the-verdict.txt", "r", encoding="utf-8") as f:
    raw_text = f.read()

enc_text = tokenizer.encode(raw_text)
print(len(enc_text))

```

执行上述代码将返回5145,这是应用BPE分词器后训练数据中的标记总数。

现在,我们从数据集中移除前50个标记用于演示目的,因为它导致了一个稍微更有趣的文本段落在后续步骤中:

```python
enc_sample = enc_text[50:]

```

创建用于下一词预测任务的输入-目标对的最简单和最直观的方法是创建两个变量,x和y,其中x包含输入标记,y包含目标,即输入向前移动1:

```python
context_size = 4
x = enc_sample[:context_size]
y = enc_sample[1:context_size+1]
print(f"x: {x}")
print(f"y:      {y}")

```

运行上面的代码打印以下输出:

```
x: [290, 4920, 2241, 287]
y:      [4920, 2241, 287, 257]

```

将输入与目标(即输入向前移动一个位置)一起考虑,我们现在可以创建图2.12中描述的下一词预测任务,如下所示:

```python
for i in range(1, context_size+1):
    context = enc_sample[:i]
    desired = enc_sample[i]
    print(context, "---->", desired)

```

上面的代码打印以下内容:

```
[290] ----> 4920
[290, 4920] ----> 2241
[290, 4920, 2241] ----> 287
[290, 4920, 2241, 287] ----> 257

```

箭头(---->)左侧表示LLM将接收的输入,箭头右侧的标记ID表示LLM应该预测的目标标记ID。

为了说明目的,让我们重复上述步骤,但将标记ID转换回文本:

```python
for i in range(1, context_size+1):
    context = enc_sample[:i]
    desired = enc_sample[i]
    print(tokenizer.decode(context), "---->", tokenizer.decode([desired]))

```

以下输出显示了输入和输出现在以文本格式:

```
and ---->  established
and established ---->  himself
and established himself ---->  in
and established himself in ---->  a

```

我们现在创建了输入-目标对,我们可以在后续章节中用于LLM训练。

这里还有一个步骤,在我们可以将标记转换为嵌入之前:实现一个高效的数据加载器,它迭代整个输入数据集并将输入和目标返回为PyTorch张量,可以被认为是多维数组。

特别是,我们对生成两个张量感兴趣:一个输入张量,包含LLM看到的文本,和一个目标张量,包含LLM要预测的目标,如图2.13所示。

![Untitled](Images/大模型-从零构建一个大模型/第二章/Untitled12.png)

图2.13 为了实现高效的数据加载器,我们在一个张量x中收集输入,其中每一行表示一个输入上下文。第二个张量y包含相应的预测目标(下一个词),它们是通过将输入向前移动一个位置创建的。

虽然图2.13显示了字符串格式的标记以便说明,但实际实现将直接在标记ID上操作,因为BPE分词器的encode方法在单个步骤中执行标记化和转换为标记ID。

对于高效的数据加载器实现,我们将使用PyTorch的内置Dataset和DataLoader类。有关安装PyTorch的额外信息和指导,请参见附录B中的B.1.3节"安装PyTorch"。

数据集类的代码如清单2.5所示:

```python
import torch
from torch.utils.data import Dataset, DataLoader

class GPTDatasetV1(Dataset):
    def __init__(self, txt, tokenizer, max_length, stride):
        self.input_ids = []
        self.target_ids = []

        token_ids = tokenizer.encode(txt)

        for i in range(0, len(token_ids) - max_length, stride):
            input_chunk = token_ids[i:i + max_length]
            target_chunk = token_ids[i + 1: i + max_length + 1]
            self.input_ids.append(torch.tensor(input_chunk))
            self.target_ids.append(torch.tensor(target_chunk))

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return self.input_ids[idx], self.target_ids[idx]

```

清单2.5中的GPTDatasetV1类基于PyTorch的Dataset类,并定义了如何从数据集中提取单个项目,其中每个项目由一定数量的标记ID(基于max_length)组成的input_chunk张量组成。target_chunk张量包含相应的目标。我建议你仔细阅读一下它是如何工作的 - 当我们将数据集与PyTorch DataLoader结合使用时,这将带来额外的直觉和清晰度。

如果你不熟悉PyTorch Dataset类的结构,如清单2.5所示,请查看附录C中的C.6节"设置高效数据加载器",其中解释了PyTorch Dataset和DataLoader类的一般结构和用法。

以下代码将使用GPTDatasetV1来创建批量输入-目标对的PyTorch DataLoader:

```python
def create_dataloader_v1(txt, batch_size=4, max_length=256,
        stride=128, shuffle=True, drop_last=True, num_workers=0):
    tokenizer = tiktoken.get_encoding("gpt2")
    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        drop_last=drop_last,
        num_workers=0
    )

    return dataloader

```

让我们使用批次大小为1的dataloader来处理一个上下文大小为4的LLM,以便理解清单2.5中的GPTDatasetV1类和清单2.6中的create_dataloader_v1函数如何协同工作:

```python
with open("the-verdict.txt", "r", encoding="utf-8") as f:
    raw_text = f.read()

dataloader = create_dataloader_v1(
    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)
data_iter = iter(dataloader)
first_batch = next(data_iter)
print(first_batch)

```

执行前面的代码打印以下内容:

```
[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]

```

first_batch变量包含两个张量:第一个张量存储输入标记ID,第二个张量存储目标标记ID。请注意,max_length被设置为4,因此两个张量中的每一个都包含4个标记ID。请注意,输入大小为4是任意小的,仅为说明目的而选择。通常使用至少256的输入大小来训练LLM。

为了说明stride=1的含义,让我们从数据集中获取另一个批次:

```python
second_batch = next(data_iter)
print(second_batch)

```

第二个批次的内容如下:

```
[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]

```

如果我们比较第一个和第二个批次,我们可以看到第二个批次的标记ID向前移动了一个位置(例如,第一个批次输入的第二个ID是367,它是第二个批次输入的第一个ID)。stride设置表示输入在批次之间移动的位置数,实现了滑动窗口方法,如图2.14所示。

![Untitled](Images/大模型-从零构建一个大模型/第二章/Untitled13.png)

图2.14 当从输入数据集创建多个批次时,我们在文本上滑动输入窗口。如果stride设置为1,我们在创建下一个批次时将输入窗口移动1个位置。如果我们将stride设置为等于输入窗口大小,我们可以防止批次之间的重叠。

> 练习 2.2 使用不同步长和上下文大小的数据加载器
> 
> 
> 为了更好地理解数据加载器是如何工作的,尝试使用不同的设置运行它,如max_length=12和stride=2以及max_length=8和stride=2。
> 

批次大小为1,就像我们在上面的示例中所做的那样,对于说明目的很有用。如果你有以前的深度学习经验,你可能知道小批次大小在训练期间需要更多的内存但导致更嘈杂的模型更新。批次大小是一个权衡,建议在训练LLM时进行实验。

在我们继续本章的最后两节,它们专注于从标记ID创建嵌入向量之前,让我们快速看一下如何使用数据加载器进行大于1的批次大小的采样:

```python
dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4)

data_iter = iter(dataloader)
inputs, targets = next(data_iter)
print("Inputs:\\n", inputs)
print("\\nTargets:\\n", targets)

```

这打印以下内容:

```
Inputs:
 tensor([[   40,   367,  2885,  1464],
        [ 1807,  3619,   402,   271],
        [10899,  2138,   257,  7026],
        [15632,   438,  2016,   257],
        [  922,  5891,  1576,   438],
        [  568,   340,   373,   645],
        [ 1049,  5975,   284,   502],
        [  284,  3285,   326,    11]])

Targets:
 tensor([[  367,  2885,  1464,  1807],
        [ 3619,   402,   271, 10899],
        [ 2138,   257,  7026, 15632],
        [  438,  2016,   257,   922],
        [ 5891,  1576,   438,   568],
        [  340,   373,   645,  1049],
        [ 5975,   284,   502,   284],
        [ 3285,   326,    11,   287]])

```

注意我们将stride增加到4。这是为了说明我们可以避免完全重叠(我们不想有重复的行)但也避免批次之间的间隙,因为这种重叠可能导致增加过拟合。

在本章的最后两节中,我们将实现嵌入层,将标记ID转换为连续向量表示,作为LLM的输入数据格式。

## 2.7 创建标记嵌入

准备LLM训练的输入文本的最后一步是将标记ID转换为嵌入向量,如图2.15所示,这将是这两个剩余部分的重点。
![Untitled](Images/大模型-从零构建一个大模型/第二章/Untitled14.png)

图2.15 为LLM准备输入文本涉及将文本标记化、将文本标记转换为标记ID,以及将标记ID转换为向量嵌入向量。在本节中,我们考虑在前面部分中创建的标记ID来创建标记嵌入向量。

除了图2.15中概述的过程外,重要的是要注意我们将这些嵌入权重初始化为随机值作为初始起点。这种初始化作为LLM学习过程的起点。我们将在第5章作为LLM训练的一部分优化嵌入权重。

连续向量表示或嵌入是必要的,因为GPT类型的LLM是使用反向传播算法训练的神经网络。如果你不熟悉神经网络如何通过反向传播训练,请查看附录B中的B.4节"通过反向传播训练"。

让我们用一个实践示例来说明标记ID到嵌入向量的转换是如何工作的。假设我们有以下标记ID输入,2、3、5和1:

```python
input_ids = torch.tensor([2, 3, 5, 1])

```

为了简单起见和说明目的,假设我们有一个只有6个词的小词汇表(而不是GPT分词器词汇表中的50,257个词),我们想要创建大小为3的嵌入(在GPT-3中,嵌入大小是12,288维):

```python
vocab_size = 6
output_dim = 3

```

给定vocab_size和output_dim,我们可以在PyTorch中实例化一个嵌入层,将随机种子设置为123以实现可重复性:

```python
torch.manual_seed(123)
embedding_layer = torch.nn.Embedding(vocab_size, output_dim)
print(embedding_layer.weight)

```

上面示例中的print语句打印嵌入层的底层权重矩阵:

```
Parameter containing:
tensor([[ 0.3374, -0.1778, -0.1690],
        [ 0.9178,  1.5810,  1.3010],
        [ 1.2753, -0.2010, -0.1606],
        [-0.4015,  0.9666, -1.1481],
        [-1.1589,  0.3255, -0.6315],
        [-2.8400, -0.7849, -1.4096]], requires_grad=True)

```

我们可以看到嵌入层的权重矩阵包含小的随机值。这些值在LLM训练期间被优化作为LLM优化本身的一部分,我们将在即将到来的章节中看到。然而,我们可以看到权重矩阵有六行和三列。有六行用于词汇表中可能的六个标记。三列用于三个嵌入维度中的每一个。

在实例化嵌入层后,让我们现在将其应用于标记ID以获得嵌入向量:

```python
print(embedding_layer(torch.tensor([3])))

```

返回的嵌入向量如下:

```
tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)

```

如果我们将标记ID 3的嵌入向量与之前的嵌入矩阵进行比较,我们可以看到它与第4行相同(Python从零索引开始,所以它是对应索引3的行)。换句话说,嵌入层本质上是一个查找操作,检索嵌入层权重矩阵中标记ID的行。

之前,我们看到了如何将单个标记ID转换为三维嵌入向量。让我们现在应用这个到我们之前定义的所有输入ID (torch.tensor([2, 3, 5, 1])):

```python
print(embedding_layer(input_ids))

```

打印输出显示这导致了一个4x3矩阵:

```
tensor([[ 1.2753, -0.2010, -0.1606],
        [-0.4015,  0.9666, -1.1481],
        [-2.8400, -0.7849, -1.4096],
        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)

```

输出矩阵中的每一行都是通过从嵌入权重矩阵中查找而获得的,如图2.16所示。

![Untitled](Images/大模型-从零构建一个大模型/第二章/Untitled15.png)

图2.16 嵌入层执行查找操作,从嵌入层的权重矩阵中检索对应于标记ID的嵌入向量。例如,标记ID 5的嵌入向量是嵌入层权重矩阵的第六行(它是第六行而不是第五行,因为Python从0开始计数)。为了说明目的,我们假设标记ID是由我们在2.3节中使用的小词汇表产生的。

本节介绍了我们如何从标记ID创建嵌入向量。本章的下一个也是最后一个部分将对这些嵌入向量进行小的修改,以编码有关标记在文本中位置的信息。

## 2.8 编码词位置

在上一节中,我们将标记ID转换为连续的向量表示,即所谓的标记嵌入。原则上,这是LLM的适当输入。然而,LLM的一个小缺点是它们的自注意力机制(我们将在第3章详细介绍)本身并不了解序列中标记的位置或顺序。

之前介绍的嵌入层的工作方式是,相同的标记ID总是映射到相同的向量表示,无论该标记ID在输入序列中的位置如何,如图2.17所示。

![Untitled](Images/大模型-从零构建一个大模型/第二章/Untitled16.png)

图2.17 嵌入层将标记ID转换为相同的向量表示,无论它在输入序列中的位置如何。例如,标记ID 5,无论它在标记ID输入向量中是第一个还是第三个位置,都会得到相同的嵌入向量。

原则上,这种无差别、位置无关的标记ID嵌入对于可解释性目的是有用的。然而,由于LLM的自注意力机制本身是位置无关的,向LLM注入额外的位置信息是有帮助的。

绝对位置嵌入直接与序列中的特定位置相关联。对于输入序列中的每个位置,都会添加一个唯一的嵌入到标记的嵌入中,以编码其确切位置。例如,第一个标记将有一个特定的位置嵌入,第二个标记另一个不同的嵌入,依此类推,如图2.18所示。

![Untitled](Images/大模型-从零构建一个大模型/第二章/Untitled17.png)

图2.18 位置嵌入被添加到标记嵌入向量中,以创建LLM的输入嵌入。位置向量与原始标记嵌入具有相同的维度。为了简单起见,标记嵌入显示为值1。

相对位置嵌入的重点不是标记的绝对位置,而是标记之间的相对位置或距离。这意味着模型学习关系的术语是"什么是相对的"而不是"在哪个确切位置"。这种方法的优点是模型可以更好地泛化到不同长度的序列,即使它在训练期间没有见过这种长度。

这两种类型的位置嵌入都旨在增强LLM理解标记之间的顺序和关系的能力,从而实现更准确和上下文感知的预测。你的选择通常取决于特定的应用和正在处理的任务的性质。

OpenAI的GPT模型使用绝对位置嵌入,这些嵌入在训练过程中被优化,而不是像原始Transformer模型中那样被固定或预定义。这个优化过程是模型训练本身的一部分,我们将在本书后面实现。现在,让我们创建初始位置嵌入来创建LLM输入以供后续章节使用。

之前,我们专注于非常小的嵌入大小,用于本章的说明目的。现在让我们考虑更现实和有用的嵌入大小,并将输入标记编码为256维向量表示。这比原始GPT-3模型使用的大小小得多(在GPT-3中,嵌入大小为12,288维),但对于实验来说仍然是合理的。此外,我们假设标记ID是由我们之前实现的BPE分词器创建的,它有50,257的词汇表大小:

```python
vocab_size = 50257
output_dim = 256
token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)

```

使用上面的token_embedding_layer,如果我们从数据加载器中采样,我们将每个批次中的每个标记嵌入到256维向量中。如果我们使用批次大小为8,每个4个标记长,结果将是一个8 x 4 x 256的张量。

让我们实例化我们在2.6节"使用滑动窗口进行数据采样"中的数据加载器:

```python
max_length = 4
dataloader = create_dataloader_v1(
    raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)
data_iter = iter(dataloader)
inputs, targets = next(data_iter)
print("Token IDs:\\n", inputs)
print("\\nInputs shape:\\n", inputs.shape)

```

前面的代码打印以下输出:

```
Token IDs:
 tensor([[   40,   367,  2885,  1464],
        [ 1807,  3619,   402,   271],
        [10899,  2138,   257,  7026],
        [15632,   438,  2016,   257],
        [  922,  5891,  1576,   438],
        [  568,   340,   373,   645],
        [ 1049,  5975,   284,   502],
        [  284,  3285,   326,    11]])

Inputs shape:
 torch.Size([8, 4])

```

如我们所见,标记ID张量是8x4维的,意味着批次包含8个文本样本,每个有4个标记。

现在让我们使用嵌入层将这些标记ID嵌入到256维向量中:

```python
token_embeddings = token_embedding_layer(inputs)
print(token_embeddings.shape)

```

前面的print函数调用返回以下内容:

```
torch.Size([8, 4, 256])

```

如我们可以从8x4x256维张量输出看到的,每个标记ID现在被嵌入为256维向量。

对于GPT模型的绝对嵌入方法,我们现在需要创建另一个嵌入层,它具有与token_embedding_layer相同的维度:

```python
context_length = max_length
pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)
pos_embeddings = pos_embedding_layer(torch.arange(context_length))
print(pos_embeddings.shape)

```

如前面的代码示例所示,pos_embeddings的输入通常是一个等差向量torch.arange(context_length),它包含一个从0, 1, ...,到最大输入长度 − 1的数字序列。context_length是一个表示LLM支持的输入大小的变量。这里,我们选择它等于输入文本的最大长度。在实践中,输入文本可能长于支持的上下文长度,在这种情况下我们需要截断文本。

print语句的输出如下:

```
torch.Size([4, 256])

```

如我们所见,位置嵌入张量由四个256维向量组成。我们现在可以将这些应用到标记嵌入,其中PyTorch会将4x256维pos_embeddings张量广播到每个8x4x256维标记嵌入张量的批次:

```python
input_embeddings = token_embeddings + pos_embeddings
print(input_embeddings.shape)

```

```
torch.Size([8, 4, 256])

```

我们创建的input_embeddings,如图2.19所总结的,是嵌入的输入示例,现在可以被主要的LLM模块处理,我们将在第3章开始实现。

![Untitled](Images/大模型-从零构建一个大模型/第二章/Untitled18.png)

图2.19 作为输入处理管道的一部分,输入文本首先被分解成单独的标记。然后这些标记使用词汇表转换为标记ID。标记ID被转换为嵌入向量,添加类似大小的位置嵌入,结果是用作主要LLM层输入的输入嵌入。

## 2.9 总结

- LLM需要将文本数据转换为数值向量(称为嵌入),因为它们无法处理原始文本。嵌入将离散数据(如词或图像)转换为连续向量空间,使其与神经网络操作兼容。
- 作为第一步,原始文本被分解成标记,可以是词或字符。然后,标记被转换为整数表示,称为标记ID。
- 可以添加特殊标记,如<|unk|>和<|endoftext|>,以增强模型的理解并处理各种上下文,例如未知词或标记不相关文本之间的边界。
- GPT-2和GPT-3等LLM使用的字节对编码(BPE)分词器可以通过将未知词分解为子词单元或单个字符来有效处理未知词。
- 我们在标记化数据上使用滑动窗口方法生成LLM训练的输入-目标对。
- PyTorch中的嵌入层作为查找操作,检索对应于标记ID的向量。结果嵌入向量为标记提供连续表示,这对于训练LLM等深度学习模型至关重要。
- 虽然标记嵌入为每个标记提供一致的向量表示,但它们缺乏标记在序列中位置的感觉。为了纠正这一点,存在两种主要类型的位置嵌入:绝对和相对。OpenAI的GPT模型利用绝对位置嵌入,这些嵌入被添加到标记嵌入向量中,并在模型训练期间进行优化。
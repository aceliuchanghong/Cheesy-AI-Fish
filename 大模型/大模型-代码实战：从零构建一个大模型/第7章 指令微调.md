# 第7章 指令微调

本章将介绍:

- 大语言模型指令微调过程的介绍
- 为监督指令微调准备数据集
- 组织指令数据为训练批次
- 加载预训练大语言模型并微调以遵循人类指令
- 提取大语言模型生成的指令响应用于评估
- 评估经过指令微调的大语言模型

在前几章中,我们实现了大语言模型的架构,进行了预训练,并将外部预训练权重导入到我们的模型中。在上一章,我们专注于将大语言模型微调用于特定的分类任务:区分垃圾短信和非垃圾短信。在本章中,我们将实现微调大语言模型以遵循人类指令的过程,如图7.1所示,这是开发用于聊天机器人应用、个人助理和其他对话任务的大语言模型的主要技术之一。

![Untitled](Images/大模型-从零构建一个大模型/第七章/Untitled.png)

图7.1显示了微调大语言模型的两种主要方式:微调用于分类(步骤8)和微调大语言模型以遵循指令(步骤9)。我们在上一章实现了步骤8。本章将重点关注使用指令数据集微调大语言模型,这个过程将在下一节中进一步解释。

## 7.1 指令微调简介

在第5章中,我们看到了生成式大语言模型的训练过程,其中它学会了预测给定文本的下一个单词。这个过程产生的大语言模型能够完成更复杂的任务,意味着它可以完成句子或写出整个段落,给定一个起始提示。

然而,预训练的大语言模型往往难以处理特定指令,比如"总结以下文本"或"将此文本翻译成西班牙语"。我们将在7.5节中看到一个具体例子,我们使用预训练的大语言模型作为指令微调的起点。

在本章中,我们专注于提高大语言模型遵循人类指令并生成所需响应的能力,如图7.2所示。

![Untitled](Images/大模型-从零构建一个大模型/第七章/Untitled1.png)

在本章的剩余部分,我们将分几个步骤实现指令微调过程,从数据集准备开始,如图7.3所示。

![Untitled](Images/大模型-从零构建一个大模型/第七章/Untitled2.png)

准备数据集是指令微调的一个关键步骤,我们将在本章的大部分内容中讨论。下一节实现了下载和格式化数据集的步骤,这是数据集准备过程中的第一个任务。

## 7.2 准备用于监督指令微调的数据集

在本节中,我们下载并格式化本章用于指令微调预训练大语言模型的指令数据集。该数据集包含1100个指令-响应对,类似于图7.2所示的那些。这个数据集是为本书特别创建的,但感兴趣的读者可以在附录A中找到其他公开可用的指令数据集。

以下代码实现并执行一个函数来下载该数据集,这是一个相对较小的文件,只有204 KB,采用JSON格式。JSON,即JavaScript对象表示法,类似于Python字典,提供了一种简单的结构,既便于人类阅读又便于机器解析。

```python
import json
import os
import urllib

def download_and_load_file(file_path, url):
    if not os.path.exists(file_path):
        with urllib.request.urlopen(url) as response:
            text_data = response.read().decode("utf-8")
        with open(file_path, "w", encoding="utf-8") as file:
            file.write(text_data)
    else:
        with open(file_path, "r", encoding="utf-8") as file:
            text_data = file.read()
    with open(file_path, "r") as file:
        data = json.load(file)
    return data

file_path = "instruction-data.json"
url = "<https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/instruction-data.json>"

data = download_and_load_file(file_path, url)
print("条目数量:", len(data))

```

执行上述代码的输出如下:

```
条目数量: 1100

```

data变量,我们从JSON文件加载的,包含1100个指令数据集的条目。让我们打印其中一个条目来看看每个条目是如何结构化的:

```python
print("示例条目:\\n", data[50])

```

示例条目的内容如下:

```
示例条目:
 {'instruction': '识别以下单词的正确拼写。', 'input': 'Ocassion', 'output': "正确的拼写是'Occasion'。"}

```

如我们所见,示例条目是Python字典对象,包含'instruction'、'input'和'output'。让我们看另一个例子:

```python
print("另一个示例条目:\\n", data[999])

```

基于这个条目的内容,我们可以看到'input'字段可以选择性为空:

```
另一个示例条目:
 {'instruction': "'complicated'的反义词是什么?", 'input': '', 'output': "'complicated'的反义词是'simple'。"}

```

指令微调,也称为监督指令微调,涉及在一个数据集上训练模型,其中输入-输出对,如我们从JSON文件中提取的那些,是明确提供的。有多种方法来格式化这些条目以供大语言模型使用。图7.4说明了不同的示例格式,通常称为提示样式,用于训练著名的大语言模型,如Alpaca和Phi-3。Alpaca是最早公开详细说明其指令微调过程的大语言模型之一。Phi-3,由微软开发,包含在此处以展示提示样式的多样性。

![Untitled](Images/大模型-从零构建一个大模型/第七章/Untitled4.png)

本章的其余部分使用Alpaca提示样式,因为它是更流行的样式之一,主要是因为它帮助定义了原始的微调方法。

**练习7.1 更改提示样式**

在使用Alpaca提示样式微调模型后,尝试图7.4中所示的Phi-3提示样式,并观察它是否影响模型的响应质量。

让我们定义一个format_input函数,我们可以用它来将data数组中的条目转换为图7.4中描述的Alpaca风格输入格式:

```python
def format_input(entry):
    instruction_text = (
        f"以下是描述任务的指令。"
        f"写一个恰当完成请求的响应。"
        f"\\n\\n### 指令:\\n{entry['instruction']}"
    )
    input_text = f"\\n\\n### 输入:\\n{entry['input']}" if entry["input"] else ""
    return instruction_text + input_text

```

这个format_input函数接受一个字典条目作为输入并构造一个格式化字符串。让我们在我们之前看过的数据集条目data[50]上尝试它:

```python
model_input = format_input(data[50])
desired_response = f"\\n\\n### 响应:\\n{data[50]['output']}"
print(model_input + desired_response)

```

格式化的输入看起来如下:

```
以下是描述任务的指令。写一个恰当完成请求的响应。

### 指令:
识别以下单词的正确拼写。

### 输入:
Ocassion

### 响应:
正确的拼写是'Occasion'。

```

注意,format_input跳过了可选的### 输入:部分,如果'input'字段为空,我们可以通过将format_input函数应用于我们之前检查的条目data[999]来看到这一点:

```python
model_input = format_input(data[999])
desired_response = f"\\n\\n### 响应:\\n{data[999]['output']}"
print(model_input + desired_response)

```

正如我们可以从以下输出中看到,具有空'input'字段的条目在其格式化输入中不包含### 输入:部分:

```
以下是描述任务的指令。写一个恰当完成请求的响应。

### 指令:
'complicated'的反义词是什么?

### 响应:
'complicated'的反义词是'simple'。

```

在下一节设置PyTorch代码之前,让我们将数据集分为训练、验证和测试集,类似于我们对垃圾分类数据集所做的。以下是划分:

```python
train_portion = int(len(data) * 0.85)  # 85%用于训练
test_portion = int(len(data) * 0.1)   # 10%用于测试
val_portion = len(data) - train_portion - test_portion  # 剩余5%用于验证

train_data = data[:train_portion]
test_data = data[train_portion:train_portion + test_portion]
val_data = data[train_portion + test_portion:]

print("训练集长度:", len(train_data))
print("验证集长度:", len(val_data))
print("测试集长度:", len(test_data))

```

此划分得到以下数据集大小:

```
训练集长度: 935
验证集长度: 55
测试集长度: 110

```

成功下载并划分数据集,并对数据集提示格式有了清晰的理解后,我们现在准备进行指令微调过程的下一步实现。在接下来的部分,我们将专注于开发构建训练批次的方法。

## 7.3 将数据组织成训练批次

当我们进入指令微调过程的实现阶段时,下一个任务,如图7.5所示,专注于有效构建训练批次。这涉及定义一个方法,确保模型在微调过程中接收格式化的训练数据。

![Untitled](Images/大模型-从零构建一个大模型/第七章/Untitled4.png)

在上一章中,训练批次是由PyTorch DataLoader类自动创建的,它使用默认的collate函数将样本列表组合成批次。collate函数负责接受一系列单独的数据样本并将它们合并成一个单一批次,可以被模型在训练期间高效处理。

然而,本章中指令微调的批处理过程稍微复杂一些,需要我们创建自己的自定义collate函数,我们稍后将与DataLoader一起使用。我们实现这个自定义collate函数来处理指令微调数据集的特定要求和格式。

在本节中,我们将分几个步骤处理批处理过程,包括编写自定义collate函数,如图7.6所示。

![Untitled](Images/大模型-从零构建一个大模型/第七章/Untitled5.png)

首先,实现图7.6中所示的步骤2.1和2.2,我们有一个InstructionDataset类,应用format_input和标记化输入数据集,类似于第6章中的SpamDataset。这两个步骤在图7.7中有更详细的说明。

![Untitled](Images/大模型-从零构建一个大模型/第七章/Untitled6.png)


图7.7中说明的2步过程在InstructionDataset的__init__构造函数方法中实现:

```python
import torch
from torch.utils.data import Dataset

class InstructionDataset(Dataset):
    def __init__(self, data, tokenizer):
        self.data = data
        self.encoded_texts = []
        for entry in data:
            instruction_plus_input = format_input(entry)
            response_text = f"\\n\\n### 响应:\\n{entry['output']}"
            full_text = instruction_plus_input + response_text
            self.encoded_texts.append(
                tokenizer.encode(full_text)
            )

    def __getitem__(self, index):
        return self.encoded_texts[index]

    def __len__(self):
        return len(self.data)
```

类似于第6章中的方法,我们可以通过在一个批次中收集多个训练样本来加速训练,这需要将输入填充到相似的长度。如同前一章,我们使用<|endoftext|>标记作为填充标记。

与其将<|endoftext|>标记附加到标记化的输入末尾,我们可以直接附加标记ID。要确定我们应该使用哪个标记ID,我们可以使用标记器的.encode方法对<|endoftext|>标记进行编码:

```python
import tiktoken
tokenizer = tiktoken.get_encoding("gpt2")
print(tokenizer.encode("<|endoftext|>", allowed_special={"<|endoftext|>"}))

```

结果的标记ID是50256。

在第6章中,我们将数据集中的所有样本填充到相同的长度。现在,转到图7.6中的步骤2.3,我们采用一种更高效的方法,开发一个自定义collate函数,我们可以传递给数据加载器。这个自定义collate函数将每个批次中的训练样本填充到相同的长度,同时允许不同的批次有不同的长度,如图7.8所示。这种方法通过避免将较短序列填充到匹配整个数据集中最长的序列,最小化了不必要的填充。

![Untitled](Images/大模型-从零构建一个大模型/第七章/Untitled7.png)


我们可以使用自定义collate函数实现图7.8中说明的填充过程,如下所示:

```python
def custom_collate_draft_1(
    batch,
    pad_token_id=50256,
    device="cpu"
):
    batch_max_length = max(len(item)+1 for item in batch)
    inputs_lst = []

    for item in batch:
        new_item = item.copy()
        new_item += [pad_token_id]

        padded = new_item + [pad_token_id] * (batch_max_length - len(new_item))

        inputs = torch.tensor(padded[:-1])
        inputs_lst.append(inputs)

    inputs_tensor = torch.stack(inputs_lst).to(device)
    return inputs_tensor

```

我们实现的custom_collate_draft_1被设计为集成到PyTorch DataLoader中,但它也可以作为独立函数使用。让我们尝试独立使用它来看看它如何工作。我们将尝试将三个不同长度的输入组装成一个批次,其中每个样本都填充到相同的长度:

```python
inputs_1 = [0, 1, 2, 3, 4]
inputs_2 = [5, 6]
inputs_3 = [7, 8, 9]
batch = (
    inputs_1,
    inputs_2,
    inputs_3
)
print(custom_collate_draft_1(batch))

```

结果批次看起来如下:

```
tensor([[    0,     1,     2,     3,     4],
        [    5,     6, 50256, 50256, 50256],
        [    7,     8,     9, 50256, 50256]])

```

正如我们在上面的输出中所看到的,所有输入都被填充到最长输入序列的长度,inputs_1包含5个标记ID。

所以,我们已经实现了第一个自定义collate函数来从输入列表创建批次。然而,正如我们在第5章和第6章中学到的,我们还需要创建目标标记ID批次,对应于输入ID的批次。这些目标ID,如图7.9所示,是至关重要的,因为它们代表了我们希望模型生成的内容,我们在训练期间用它们来计算损失以进行权重更新,类似于前几章。

![Untitled](Images/大模型-从零构建一个大模型/第七章/Untitled8.png)

如图7.9所示,我们现在修改自定义collate函数以返回目标标记ID以及输入标记ID。

类似于第5章中描述的生成LLM的过程,目标标记ID与输入标记ID匹配,但向右移动一个位置。这个设置,如图7.10所示,允许LLM学习如何预测序列中的下一个标记。

![Untitled](Images/大模型-从零构建一个大模型/第七章/Untitled9.png)

以下更新的collate函数从输入标记ID生成目标标记ID,如图7.10所示:

```python
def custom_collate_draft_2(
    batch,
    pad_token_id=50256,
    device="cpu"
):
    batch_max_length = max(len(item)+1 for item in batch)
    inputs_lst, targets_lst = [], []

    for item in batch:
        new_item = item.copy()
        new_item += [pad_token_id]
        padded = new_item + [pad_token_id] * (batch_max_length - len(new_item))
        inputs = torch.tensor(padded[:-1])
        targets = torch.tensor(padded[1:])
        inputs_lst.append(inputs)
        targets_lst.append(targets)

    inputs_tensor = torch.stack(inputs_lst).to(device)
    targets_tensor = torch.stack(targets_lst).to(device)
    return inputs_tensor, targets_tensor

inputs, targets = custom_collate_draft_2(batch)
print(inputs)
print(targets)

```

应用于我们之前定义的由3个输入列表组成的示例batch,新的custom_collate_draft_2函数现在返回输入和目标批次:

```
tensor([[    0,     1,     2,     3,     4],
        [    5,     6, 50256, 50256, 50256],
        [    7,     8,     9, 50256, 50256]])
tensor([[    1,     2,     3,     4, 50256],
        [    6, 50256, 50256, 50256, 50256],
        [    8,     9, 50256, 50256, 50256]])

```

在下一步中,我们将-100占位符值分配给所有填充标记,如图2.5所示。这个特殊值允许我们从损失计算中排除这些填充标记,确保只有有意义的标记影响模型学习。

我们将在实现此修改后详细讨论这个过程的原理。(在第6章中,我们不需要担心这个问题,因为我们根据最后输出标记训练了模型。)

![Untitled](Images/大模型-从零构建一个大模型/第七章/Untitled10.png)

在步骤2.4中,如图7.11所示,我们用-100替换目标标记序列中的结束文本标记,我们之前用作填充标记并分配标记ID 50256。(选择-100作为替换将在稍后说明。)

然而,我们保留一个结束文本标记,ID 50256,在目标序列中,如图7.12所示。这允许LLM学习何时在响应指令时生成结束文本标记,我们用它作为生成的响应已完成的额外指示。

![Untitled](Images/大模型-从零构建一个大模型/第七章/Untitled11.png)

在以下代码中,我们修改自定义collate函数以用-100替换目标序列中ID为50256的标记,如图7.12所示。此外,我们引入了一个allowed_max_length参数来可选地限制样本的长度。这个调整将在我们使用可能超过GPT-2模型支持的1024标记上下文窗口的新数据集时有用。我们更新的collate函数如下:

```python
def custom_collate_fn(
    batch,
    pad_token_id=50256,
    ignore_index=-100,
    allowed_max_length=None,
    device="cpu"
):
    batch_max_length = max(len(item)+1 for item in batch)
    inputs_lst, targets_lst = [], []

    for item in batch:
        new_item = item.copy()
        new_item += [pad_token_id]
        # 将序列填充到max_length
        padded = new_item + [pad_token_id] * (batch_max_length - len(new_item))
        inputs = torch.tensor(padded[:-1])  # 对输入截断最后一个标记
        targets = torch.tensor(padded[1:])  # 目标向右移动+1

        mask = targets == pad_token_id
        indices = torch.nonzero(mask).squeeze()
        if indices.numel() > 1:
            targets[indices[1:]] = ignore_index

        if allowed_max_length is not None:
            inputs = inputs[:allowed_max_length]
            targets = targets[:allowed_max_length]

        inputs_lst.append(inputs)
        targets_lst.append(targets)

    inputs_tensor = torch.stack(inputs_lst).to(device)
    targets_tensor = torch.stack(targets_lst).to(device)
    return inputs_tensor, targets_tensor

```

最后,让我们将collate函数应用于我们之前创建的示例批次,以检查它是否按预期工作:

```python
inputs, targets = custom_collate_fn(batch)
print(inputs)
print(targets)

```

结果如下,其中第一个张量表示输入,第二个张量表示目标:

```
tensor([[    0,     1,     2,     3,     4],
        [    5,     6, 50256, 50256, 50256],
        [    7,     8,     9, 50256, 50256]])
tensor([[    1,     2,     3,     4, 50256],
        [    6, 50256,  -100,  -100,  -100],
        [    8,     9, 50256,  -100,  -100]])

```

修改后的collate函数按预期工作,生成目标序列时用标记ID -100替换。这背后的逻辑是什么?让我们探讨这种修改的潜在目的。

出于演示目的,考虑以下简单的半虚构示例,其中输出logits可以对应于模型词汇表中的潜在标记。以下是我们可能在训练期间计算交叉熵损失(在第5章中介绍)的方式,当模型预测一系列标记时,类似于我们在第5章中生成模型时所做的,或在第6章中为分类微调模型时所做的:

```python
logits_1 = torch.tensor(
    [[-1.0, 1.0],  # 第1个标记的预测
     [-0.5, 1.5]]  # 第2个标记的预测
)
targets_1 = torch.tensor([0, 1]) # 正确的标记索引以生成
loss_1 = torch.nn.functional.cross_entropy(logits_1, targets_1)
print(loss_1)

```

由前面代码计算的损失值是1.1269。

tensor(1.1269)

添加一个额外的标记ID会,正如我们所预期的,影响损失计算。

```python
logits_2 = torch.tensor(
    [[-1.0, 1.0],
     [-0.5, 1.5],
     [-0.5, 1.5]]
)
targets_2 = torch.tensor([0, 1, 1])
loss_2 = torch.nn.functional.cross_entropy(logits_2, targets_2)
print(loss_2)

```

损失值,在添加第三个标记后,现在是0.7936。

到目前为止,我们已经复现了前面示例计算中使用的交叉熵损失函数在PyTorch中的工作原理,这是我们在第5章和第6章的训练函数中使用的相同损失函数,也是我们将在本章中使用的。

现在,让我们看看当我们将第三个目标标记ID替换为-100时会发生什么:

```python
targets_3 = torch.tensor([0, 1, -100])
loss_3 = torch.nn.functional.cross_entropy(logits_2, targets_3)
print(loss_3)
print("loss_1 == loss_3:", loss_1 == loss_3)

```

结果输出如下:

```
tensor(1.1269)
loss_1 == loss_3: tensor(True)

```

基于这个结果,我们可以看到这3个训练示例的最终损失与我们之前从2个训练示例计算的损失相同。换句话说,交叉熵损失函数忽略了targets_3向量中的第三个条目,即对应于-100的标记ID。(感兴趣的读者可以尝试将-100值替换为0到1之间的其他标记ID,并会看到这会导致错误。)

那么,-100有什么特别之处,使它被交叉熵损失忽略?PyTorch中交叉熵函数的默认设置是cross_entropy(..., ignore_index=-100)。这意味着它忽略标记为-100的目标标签。

在本章中,我们利用这个ignore_index来忽略额外的结束文本(填充)标记,我们必须添加这些标记以使每个批次中的训练样本具有相同的长度。

然而,如图7.12所示,我们想保留一个50256(结束文本)标记ID在目标中,因为它帮助LLM学习生成结束文本标记,我们可以用它作为响应完成的指示。

除了掩蔽填充标记外,还常见的做法是掩蔽对应于指令的目标标记ID,如图7.13所示。

![Untitled](Images/大模型-从零构建一个大模型/第七章/Untitled12.png)

通过掩蔽对应于指令的目标标记ID,如图7.13所示,LLM的交叉熵损失仅针对生成的响应目标ID计算。通过掩蔽指令标记,我们的模型被训练专注于生成准确的响应,而不是额外地记忆指令,这可能有助于减少过拟合。

目前,研究人员对于是否掩蔽如图7.13所示的指令是普遍有益的存在分歧。例如,一篇最近的论文题为"Instruction Tuning with GPT-4"表明不掩蔽指令有利于LLM性能(见附录A中的更多细节)。在本章中,我们不应用掩蔽,并将其作为读者的可选练习。

**练习7.2 指令和输入掩蔽**

完成本章并使用本节实现的InstructionDataset微调模型后,替换指令和输入标记为-100掩码以实现图7.13中说明的指令掩蔽方法。然后,评估这是否对模型性能有积极影响。

## 7.4 为指令数据集创建数据加载器

在上一节中,我们经历了几个阶段来实现InstructionDataset类和指令数据集的custom_collate_fn函数。在本节中,如图7.14所示,我们可以利用之前的工作,简单地将InstructionDataset对象和custom_collate_fn函数插入到PyTorch数据加载器中。这些加载器将自动处理为LLM指令微调过程组织批次的任务。

![Untitled](Images/大模型-从零构建一个大模型/第七章/Untitled13.png)

在我们实现图7.14中所示的数据加载器创建之前,我们需要简要讨论一下我们在上一节实现的custom_collate_fn的device设置。

custom_collate_fn包含将输入和目标张量(例如,torch.stack(inputs_lst).to(device))移动到指定设备的代码,可以是"cpu"或"cuda"(用于GPU),或可选的"mps"用于带有Apple Silicon芯片的Mac。(请注意,使用"mps"设备可能会导致与本章内容的数值差异,因为Apple Silicon支持在PyTorch中仍处于实验阶段。)

在前几章中,我们在主训练循环中将数据移动到目标设备(例如,GPU内存中device="cuda")。在collate函数中进行这种设备传输提供了在训练循环之外作为后台进程执行此设备传输过程的优势,防止它阻塞GPU在模型训练期间。

以下代码初始化device变量:

```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# if torch.backends.mps.is_available():
#     device = torch.device("mps")
print("Device:", device)

```

现在,为了确保在我们稍后将其传递给PyTorch DataLoader类时在custom_collate_fn中使用选择的device设置,我们使用Python的functools标准库中的partial函数创建一个新版本的函数,其device参数预填充。此外,我们将allowed_max_length设置为1024,这对应于GPT-2模型支持的最大上下文长度,我们稍后将在本章中微调该模型:

```python
from functools import partial
customized_collate_fn = partial(custom_collate_fn, device=device, allowed_max_length=1024)

```

现在,我们可以像在前几章中那样设置数据加载器,但现在我们将使用自定义collate函数进行批处理过程:

```python
from torch.utils.data import DataLoader

num_workers = 0
batch_size = 8

torch.manual_seed(123)

train_dataset = InstructionDataset(train_data, tokenizer)
train_loader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    collate_fn=customized_collate_fn,
    shuffle=True,
    drop_last=True,
    num_workers=num_workers
)

val_dataset = InstructionDataset(val_data, tokenizer)
val_loader = DataLoader(
    val_dataset,
    batch_size=batch_size,
    collate_fn=customized_collate_fn,
    shuffle=False,
    drop_last=False,
    num_workers=num_workers
)

test_dataset = InstructionDataset(test_data, tokenizer)
test_loader = DataLoader(
    test_dataset,
    batch_size=batch_size,
    collate_fn=customized_collate_fn,
    shuffle=False,
    drop_last=False,
    num_workers=num_workers
)

```

让我们检查训练加载器生成的输入和目标批次的维度:

```python
print("训练加载器:")
for inputs, targets in train_loader:
    print(inputs.shape, targets.shape)

```

输出如下(为节省空间而截断):

```
训练加载器:
torch.Size([8, 61]) torch.Size([8, 61])
torch.Size([8, 76]) torch.Size([8, 76])
torch.Size([8, 73]) torch.Size([8, 73])
...
torch.Size([8, 74]) torch.Size([8, 74])
torch.Size([8, 69]) torch.Size([8, 69])

```

在前面的输出中,我们可以看到我们的第一个输入和目标批次有维度8×61,其中8代表批次大小,61是此批次中每个训练样本的标记数。第二个输入和目标批次有不同数量的标记,例如,76。

正如我们在前面的输出中看到的,由于自定义collate函数,数据加载器能够创建不同长度的批次。在下一节中,我们将加载一个预训练的LLM,我们可以用这些数据加载器进行微调。

## 7.5 加载预训练的LLM

在前几节中,我们花了很多时间准备用于指令微调的数据集,这是监督微调过程的关键方面。其他方面大多与预训练相似,允许我们重用许多来自早期章节的代码。

在开始指令微调之前,我们首先加载一个预训练的LLM模型,如图7.15所示,我们将对其进行微调。

![Untitled](Images/大模型-从零构建一个大模型/第七章/Untitled14.png)

如我们章节概述图中图7.15所示,本节专注于步骤4,加载预训练的LLM作为指令微调的起点,类似于前几章的过程。然而,我们将使用中型355百万参数模型,而不是使用最小的124百万参数模型。选择这个模型的原因是124百万参数模型在容量上太有限,无法在指令微调中获得定性上令人满意的结果。

这里我们使用与5.5节和6.4节相同的代码,除了我们现在指定"gpt2-medium (355M)"而不是"gpt2-small (124M)"。请注意,执行下面提供的代码将启动中型GPT模型的下载,它有大约1.42千兆字节的存储要求。这大约是小型模型所需存储空间的三倍:

```python
from gpt_download import download_and_load_gpt2
from chapter04 import GPTModel
from chapter05 import load_weights_into_gpt

BASE_CONFIG = {
    "vocab_size": 50257,     # 词汇表大小
    "context_length": 1024,  # 上下文长度
    "drop_rate": 0.0,        # 丢弃率
    "qkv_bias": True         # 查询-键-值偏置
}

model_configs = {
    "gpt2-small (124M)": {"emb_dim": 768, "n_layers": 12, "n_heads": 12},
    "gpt2-medium (355M)": {"emb_dim": 1024, "n_layers": 24, "n_heads": 16},
    "gpt2-large (774M)": {"emb_dim": 1280, "n_layers": 36, "n_heads": 20},
    "gpt2-xl (1558M)": {"emb_dim": 1600, "n_layers": 48, "n_heads": 25},
}

CHOOSE_MODEL = "gpt2-medium (355M)"
BASE_CONFIG.update(model_configs[CHOOSE_MODEL])

model_size = CHOOSE_MODEL.split(" ")[-1].lstrip("(").rstrip(")")
settings, params = download_and_load_gpt2(model_size=model_size, models_dir="gpt2")

model = GPTModel(BASE_CONFIG)
load_weights_into_gpt(model, params)
model.eval();

```

执行前面几节中的代码后,将下载几个文件,类似于早期章节的过程。下载的文件包括:

```
checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 156kiB/s]
encoder.json: 100%|██████████| 1.04M/1.04M [00:02<00:00, 467kiB/s]
hparams.json: 100%|██████████| 91.0/91.0 [00:00<00:00, 198kiB/s]
model.ckpt.data-00000-of-00001: 100%|██████████| 1.42G/1.42G [05:50<00:00, 4.05MiB/s]
model.ckpt.index: 100%|██████████| 10.4k/10.4k [00:00<00:00, 18.1MiB/s]
model.ckpt.meta: 100%|██████████| 927k/927k [00:02<00:00, 454kiB/s]
vocab.bpe: 100%|██████████| 456k/456k [00:01<00:00, 283kiB/s]

```

在下一节开始微调模型之前,让我们花点时间评估预训练LLM在验证任务之一上的性能,通过比较其输出与预期响应。这将给我们一个基线理解,模型在指令遵循任务上的表现如何,在微调之前,并将帮助我们欣赏微调后的影响。让我们使用验证集中的第一个例子进行这个评估:

```python
torch.manual_seed(123)
input_text = format_input(val_data[0])
print(input_text)

```

指令内容如下:

```
以下是描述任务的指令。写一个恰当完成请求的响应。

### 指令:
将主动句转换为被动句: '厨师每天烹饪这顿饭。'

```

接下来,我们使用第5章中的generate函数生成模型的响应:

```python
from chapter05 import generate, text_to_token_ids, token_ids_to_text

token_ids = generate(
    model=model,
    idx=text_to_token_ids(input_text, tokenizer),
    max_new_tokens=35,
    context_size=BASE_CONFIG["context_length"],
    eos_id=50256,
)
generated_text = token_ids_to_text(token_ids, tokenizer)

```

重要的是要注意,generate函数返回组合的输入和输出文本。这种行为在前几章中很方便,因为预训练的LLM主要被设计为文本完成模型,其中输入和输出被连接以创建一个连贯和可信的文本。然而,当评估模型在特定任务上的性能时,我们通常想专注于模型生成的响应。

为了在这里隔离模型的响应,我们需要从generated_text的开头减去input_text的长度:

```python
response_text = generated_text[len(input_text):].strip()
print(response_text)

```

这段代码从generated_text的开头删除输入文本,只留下模型生成的响应。strip()函数然后应用以删除任何前导或尾随空白字符。输出如下:

```
### 响应:

厨师每天烹饪这顿饭。

### 指令:

将主动句转换为被动句: '厨师烹饪

```

正如我们可以看到,预训练模型不能正确地遵循给定的指令。它似乎创建了一个"响应"部分,但只是重复了原始输入句子,然后是部分指令,未能将主动句转换为被动语态。

在接下来的部分,我们将实现微调过程以改善模型理解和适当响应此类请求的能力。

## 7.6 在指令数据上微调LLM

如图7.16所示的章节概述中所示,本节专注于微调LLM。我们采用在上一节中加载的预训练模型,并使用本章前面准备的指令数据集进一步训练它。

![Untitled](Images/大模型-从零构建一个大模型/第七章/Untitled15.png)

如前所述,我们已经完成了大部分工作,当我们在本章开始时实现指令数据集处理时。对于微调过程本身,我们可以重用第5章在预训练期间实现的损失计算和训练函数:

```python
from chapter05 import (
    calc_loss_loader,
    train_model_simple
)

```

在我们开始训练之前,让我们计算训练和验证数据的初始损失:

```python
model.to(device)
torch.manual_seed(123)

with torch.no_grad():
    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)
    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)

print("训练损失:", train_loss)
print("验证损失:", val_loss)

```

初始损失值如下;正如在前几章中,目标是最小化这个损失:

```
训练损失: 3.825908660888672
验证损失: 3.7619335651397705

```

**处理硬件限制**

请注意,使用并训练更大的模型如GPT-2 medium(355百万参数)比前几章使用的较小的GPT-2模型(124百万参数)在计算上更加密集。如果你遇到由于硬件限制的问题,你可以通过在7.5节中将CHOOSE_MODEL = "gpt2-medium (355M)"改为CHOOSE_MODEL = "gpt2-small (124M)"来切换到较小的模型。或者,要加速模型训练,考虑使用GPU。

表7.1提供了在各种设备上训练每个模型的参考时间,包括CPU和GPU。在兼容的GPU上运行这段代码不仅需要最少的更改,而且可以显著加速训练。对于本章所示的结果,我在NVIDIA A100 GPU上训练了GPT-2 medium模型。

表7.1 指令微调GPT-2的参考运行时间

| 模型名称 | 设备 | 2个epoch的运行时间 |
| --- | --- | --- |
| gpt2-medium (355M) | CPU (M3 MacBook Air) | 15.78分钟 |
| gpt2-medium (355M) | GPU (NVIDIA L4) | 1.83分钟 |
| gpt2-medium (355M) | GPU (NVIDIA A100) | 0.86分钟 |
| gpt2-small (124M) | CPU (M3 MacBook Air) | 5.74分钟 |
| gpt2-small (124M) | GPU (NVIDIA L4) | 0.69分钟 |
| gpt2-small (124M) | GPU (NVIDIA A100) | 0.39分钟 |

现在模型和数据加载器都准备好了,我们可以继续训练模型。以下代码设置了训练过程,包括初始化优化器,设置epoch数,并定义评估频率和训练上下文以基于我们之前看过的第一个验证集指令(val_data[0])评估生成的LLM响应:

```python
import time

start_time = time.time()
torch.manual_seed(123)
optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)
num_epochs = 2

train_losses, val_losses, tokens_seen = train_model_simple(
    model, train_loader, val_loader, optimizer, device,
    num_epochs=num_epochs, eval_freq=5, eval_iter=5,
    start_context=format_input(val_data[0]), tokenizer=tokenizer
)

end_time = time.time()
execution_time_minutes = (end_time - start_time) / 60
print(f"训练完成,用时 {execution_time_minutes:.2f} 分钟。")

```

以下输出显示了两个epoch的训练进度,损失的稳定下降表明改善了遵循指令和生成适当响应的能力:

```
Ep 1 (Step 000000): Train loss 2.637, Val loss 2.626
Ep 1 (Step 000005): Train loss 1.174, Val loss 1.103
Ep 1 (Step 000010): Train loss 0.872, Val loss 0.944
Ep 1 (Step 000015): Train loss 0.857, Val loss 0.906
...
Ep 1 (Step 000115): Train loss 0.520, Val loss 0.665
以下是描述任务的指令。写一个恰当完成请求的响应。  ### 指令: 将主动句转换为被动句: '厨师每天烹饪这顿饭。'  ### 响应: 这顿饭每天被厨师烹饪。<|endoftext|>以下是描述任务的指令。写一个恰当完成请求的响应。  ### 指令: 将主动句转换为被动句:
Ep 2 (Step 000120): Train loss 0.438, Val loss 0.670
Ep 2 (Step 000125): Train loss 0.453, Val loss 0.685
Ep 2 (Step 000130): Train loss 0.448, Val loss 0.681
Ep 2 (Step 000135): Train loss 0.408, Val loss 0.677
...
Ep 2 (Step 000230): Train loss 0.300, Val loss 0.657
以下是描述任务的指令。写一个恰当完成请求的响应。  ### 指令: 将主动句转换为被动句: '厨师每天烹饪这顿饭。'  ### 响应: 这顿饭每天被厨师烹饪。<|endoftext|>以下是描述任务的指令。写一个恰当完成请求的响应。  ### 指令: 英国的首都是什么
训练完成,用时 0.87 分钟。

```

训练输出显示模型正在有效学习,我们可以从两个epoch中训练和验证损失值的持续下降中看出这一点。这表明模型正在逐步改善其理解和遵循提供的指令的能力。(由于模型在这两个epoch内表现出有效学习,将训练延长到第三个epoch或更多可能是不必要的,甚至可能导致过拟合。)

此外,在每个epoch结束时生成的响应让我们能够检查模型在正确执行给定任务方面的进展。在这种情况下,模型成功地将主动句"厨师每天烹饪这顿饭。"转换为其被动语态对应句:"这顿饭每天被厨师烹饪。"

我们将在后面的章节中更详细地回顾和评估模型的响应质量。现在,为结束本节,让我们检查训练和验证损失曲线以获得关于模型学习过程的额外洞察。为此,我们使用来自第5章的plot_losses函数:

```python
from chapter05 import plot_losses
epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))
plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)

```

正如我们可以在图7.17所示的损失图中看到的,模型在训练和验证数据上的性能在训练过程中显著改善。在初始阶段损失的急剧下降表明模型正在快速学习有意义的模式和表示。然后,随着训练进入第二个epoch,损失继续下降但速度较慢,这表明模型正在微调其学习的表示并收敛到一个稳定的解决方案。

![Untitled](Images/大模型-从零构建一个大模型/第七章/Untitled16.png)

虽然图7.17中的损失图表明模型正在有效训练,但更关键的方面是其在响应质量和一致性方面的表现。在本章的剩余部分,我们将提取响应并以允许我们评估和测量响应质量的格式存储它们。

**练习7.3 在原始Alpaca数据集上微调**

所谓的Alpaca数据集由斯坦福的研究人员创建,是最早和最流行的公开共享指令数据集之一,包含52,002个条目。作为本章中使用的instruction-data.json文件的替代方案,考虑在此数据集上微调LLM。该数据集可在以下URL获得: [https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json](https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json)

这个数据集包含52,002个条目,大约是我们在本章中使用的数据集的50倍,并且每个条目也更长。因此,强烈建议使用GPU来加速微调过程。如果遇到内存不足错误,考虑将batch_size从8减少到4、2甚至1。此外,将allowed_max_length从1024降低到512或256可以进一步帮助管理内存问题。

## 7.7 提取和保存响应

在上一节描述的训练部分对指令数据集微调我们的LLM后,我们可以继续在保留的测试数据上评估其性能。为了实现这一点,我们首先提取测试数据集中每个输入的模型生成的响应,并收集它们以进行人工分析,如图7.18中的章节概述所示。

![Untitled](Images/大模型-从零构建一个大模型/第七章/Untitled17.png)

我们从步骤7开始,响应生成指令如图7.18所示,使用generate函数。我们打印模型响应并列出预期答案,用于前三个测试集条目,并并排呈现它们以供比较:

```python
torch.manual_seed(123)

for entry in test_data[:3]:
    input_text = format_input(entry)
    token_ids = generate(
        model=model,
        idx=text_to_token_ids(input_text, tokenizer).to(device),
        max_new_tokens=256,
        context_size=BASE_CONFIG["context_length"],
        eos_id=50256
    )
    generated_text = token_ids_to_text(token_ids, tokenizer)
    response_text = generated_text[len(input_text):].replace("### 响应:", "").strip()

    print(input_text)
    print(f"\\n正确响应:\\n>> {entry['output']}")
    print(f"\\n模型响应:\\n>> {response_text.strip()}")
    print("-------------------------------------")

```

如前所述,generate函数返回组合的输入和输出文本,所以我们使用切片和.replace()方法从generated_text内容中提取模型的响应。指令,后面是给定的正确响应和模型响应,如下所示:

```
以下是描述任务的指令。写一个恰当完成请求的响应。

### 指令:
使用比喻重写句子。

### 输入:
这辆车非常快。

正确响应:
>> 这辆车快如闪电。

模型响应:
>> 这辆车像子弹一样快。
-------------------------------------
以下是描述任务的指令。写一个恰当完成请求的响应。

### 指令:
通常与雷暴相关的云类型是什么?

正确响应:
>> 通常与雷暴相关的云类型是积雨云。

模型响应:
>> 通常与雷暴相关的云类型是积云。
-------------------------------------
以下是描述任务的指令。写一个恰当完成请求的响应。

### 指令:
说出《傲慢与偏见》的作者。

正确响应:
>> 简·奥斯汀。

模型响应:
>> 《傲慢与偏见》的作者是简·奥斯汀。
-------------------------------------

```

根据前两个指令、给定的正确响应和模型响应,模型表现相对相当不错。对前两个问题的回答明显是正确的,而第二个答案虽然接近但不完全准确。模型回答"积云"而不是"积雨云",尽管值得注意的是积云确实可以发展成积雨云,后者能够产生雷暴。

更重要的是,我们可以看到模型评估不像前一章那样直截了当,我们只需计算正确的垃圾/非垃圾类标签的百分比即可获得分类准确率。在实践中,指令微调的LLM可以通过多种方法进行评估:

1. 基于任务的多项选择基准测试,如MMLU ("大规模多任务语言理解", [https://arxiv.org/abs/2009.03300](https://arxiv.org/abs/2009.03300)),测试模型的一般知识。
2. 与其他LLM的人类偏好比较,例如LMSYS聊天竞技场 ([https://arena.lmsys.org](https://arena.lmsys.org/))。
3. 自动对话基准测试,其中另一个LLM如GPT-4被用来评估响应,例如AlpacaEval ([https://tatsu-lab.github.io/alpaca_eval/](https://tatsu-lab.github.io/alpaca_eval/))。

在实践中,考虑所有这三种类型的评估方法可能会有帮助:多项选择问题回答、人工评估和测量对话性能的自动指标。然而,由于我们主要对评估本章中的对话性能感兴趣,而不是测试回答多项选择问题的能力,方法2(人工评估)和3(自动指标)可能更相关。

人工评估,虽然提供有价值的洞察,但可能相对费力和耗时,特别是在处理大量响应时。例如,阅读和分配评分给所有1,100个响应将需要大量努力。

因此,考虑到我们要处理的规模,我们将实现一种类似于方法3的方法,涉及使用另一个LLM自动评估响应。这将允许我们有效地评估生成的响应的质量,而无需大量人工参与,同时仍然获得有意义的性能指标。

在下一节中,我们采用一种受AlpacaEval启发的方法,利用另一个LLM来评估我们微调模型的响应。然而,我们使用我们自己的自定义测试集而不是依赖公开可用的基准数据集。这允许对我们模型的性能进行更有针对性和相关的评估,在指令数据集所代表的具体用例和场景的背景下。

为了准备响应进行这个评估过程,我们将生成的模型响应附加到test_set字典中,并将更新后的数据保存为"instruction-data-with-response.json"文件以供记录。此外,通过保存这个文件,我们可以在以后的Python会话中轻松加载和分析响应,如果需要的话。

以下代码使用generate方法的方式与之前相同;然而,我们现在迭代整个test_set。然后,我们将模型响应添加到test_set字典中,而不是打印它们:

```python
from tqdm import tqdm

for i, entry in tqdm(enumerate(test_data), total=len(test_data)):
    input_text = format_input(entry)

    token_ids = generate(
        model=model,
        idx=text_to_token_ids(input_text, tokenizer).to(device),
        max_new_tokens=256,
        context_size=BASE_CONFIG["context_length"],
        eos_id=50256
    )
    generated_text = token_ids_to_text(token_ids, tokenizer)
    response_text = generated_text[len(input_text):].replace("### 响应:", "").strip()
    test_data[i]["model_response"] = response_text

with open("instruction-data-with-response.json", "w") as file:
    json.dump(test_data, file, indent=4)  # "indent" 用于美化打印

```

处理数据集在NVIDIA A100 GPU上大约需要1分钟,在M3 MacBook Air上需要6分钟:

```
100%|██████████| 110/110 [01:05<00:00,  1.68it/s]

```

让我们验证响应是否正确添加到test_set字典中,通过检查其中一个条目:

```python
print(test_data[0])

```

根据输出,我们可以看到model_response已正确添加:

```
{'instruction': '使用比喻重写句子。', 'input': '这辆车非常快。', 'output': '这辆车快如闪电。', 'model_response': '这辆车像子弹一样快。'}

```

最后,我们将模型保存为gpt2-medium355M-sft.pth文件,以便在未来项目中重用:

```python
import re

# 从文件名中移除空格和括号
file_name = f"{re.sub(r'[ ()]', '', CHOOSE_MODEL)}-sft.pth"
torch.save(model.state_dict(), file_name)
print(f"模型保存为 {file_name}")

```

保存的模型随后可以通过model.load_state_dict(torch.load("gpt2-medium355M-sft.pth"))加载。

## 7.8 评估微调后的LLM

之前,我们通过查看测试集中3个样例的响应来评估指令微调模型的性能。虽然这提供了模型表现良好的粗略概念,但这种方法无法很好地扩展到大量响应。因此,在本节中,如图7.19的章节概述所示,我们实现一种方法来使用另一个、更大的LLM自动化微调LLM的响应评估。

![Untitled](Images/大模型-从零构建一个大模型/第七章/Untitled18.png)

为了实现图7.19中所示的步骤9,涉及以自动方式评估测试集响应,我们利用由Meta AI开发的Llama 3的8 billion参数指令微调版本。这个模型可以使用名为Ollama的开源应用程序在本地运行([https://ollama.com](https://ollama.com/))。

Ollama是一个用于在本地运行LLM的接口应用程序。它作为开源llama.cpp库([https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp))的包装器,该库在纯C/C++中实现LLM以最大化效率。然而,请注意Ollama只是一个用于使用LLM生成文本的工具(推理),并不支持训练或微调LLM。

**使用通过Web API的更大LLM**

8 billion参数Llama 3模型是一个相当强大的LLM,可以在本地运行。然而,它不如大型专有LLM(如OpenAI提供的GPT-4)强大。

要执行以下代码,请通过访问https://ollama.com并按照提供的说明安装Ollama:

- 对于macOS和Windows用户:下载Ollama应用程序。如果提示安装命令行使用,选择"是"。
- 对于Linux用户:使用Ollama网站上提供的安装命令。

在实现模型评估代码之前,让我们首先下载Llama 3模型并验证Ollama是否通过在终端中运行以下命令正确运行。

![Untitled](Images/大模型-从零构建一个大模型/第七章/Untitled19.png)

在如图7.20所示的不同终端中运行Ollama应用程序或ollama serve,执行以下命令(不是在Python会话中)以拉取8 billion参数Llama 3模型:

```
ollama run llama3

```

第一次执行此命令时,8 billion参数Llama 3模型(占用约4.7 GB存储空间)将自动下载。输出看起来如下:

```
pulling manifest
pulling 6a0746a1ec1a... 100% ▕████████████████▏ 4.7 GB
pulling 4fa551d4f938... 100% ▕████████████████▏  12 KB
pulling 8ab4849b038c... 100% ▕████████████████▏  254 B
pulling 577073ffcc6c... 100% ▕████████████████▏  110 B
pulling 3f8eb4da87fa... 100% ▕████████████████▏  485 B
verifying sha256 digest
writing manifest
removing any unused layers
success

```

**替代Ollama模型**

请注意,ollama run llama3命令中的llama3指的是指令微调的8 billion参数Llama 3模型。由于Ollama使用llama3模型大约需要16 GB的RAM,如果你的机器没有足够的RAM,你可以尝试使用较小的模型,例如3.8 billion参数phi-3模型代替ollama run llama3,它只需要大约8 GB的RAM。

对于更强大的计算机,你也可以尝试更大的70 billion参数Llama 3模型,方法是将llama3替换为llama3:70b。然而,请记住,这个模型需要显著更多的计算资源。

一旦模型下载完成,我们就会看到一个命令行界面,允许我们与模型交互。例如,尝试问模型,"骆驼吃什么?":

```
>>> 骆驼吃什么?
骆驼是反刍动物,这意味着它们有四个胃室,吃高纤维的植物。在野外,骆驼
通常以下列食物为食:
1. 草:它们喜欢放牧各种类型的草,包括高草、小麦、燕麦和大麦。

```

请注意,响应可能会略有不同,因为Ollama不是完全确定性的。

你可以使用输入/bye结束ollama run llama3会话。然而,请确保保持ollama serve命令或Ollama应用程序运行,以便在本章的剩余部分使用。

以下代码验证我们的Ollama会话是否正在运行,然后再使用Ollama评估上一节生成的测试集响应:

```python
import psutil

def check_if_running(process_name):
    running = False
    for proc in psutil.process_iter(["name"]):
        if process_name in proc.info["name"]:
            running = True
            break
    return running

ollama_running = check_if_running("ollama")

if not ollama_running:
    raise RuntimeError("Ollama未运行。在继续之前启动ollama。")
print("Ollama运行:", check_if_running("ollama"))

```

确保上面代码的输出显示Ollama运行: True。如果显示False,请验证ollama serve命令或Ollama应用程序是否正在运行。

**在新的Python会话中运行代码**

如果你在7.7节之后关闭了Python会话,或者你更喜欢在不同的Python会话中执行本章的剩余代码,请执行以下代码,它加载我们在7.7节中创建的指令和响应数据文件,并重新定义我们之前使用的format_input函数(tqdm进度条实用程序也稍后使用):

```python
import json
from tqdm import tqdm

file_path = "instruction-data-with-response.json"
with open(file_path, "r") as file:
    test_data = json.load(file)

def format_input(entry):
    instruction_text = (
        f"以下是描述任务的指令。"
        f"写一个恰当完成请求的响应。"
        f"\\n\\n### 指令:\\n{entry['instruction']}"
    )

    input_text = f"\\n\\n### 输入:\\n{entry['input']}" if entry["input"] else ""
    return instruction_text + input_text

```

ollama run命令与模型交互的替代方法是通过其REST API使用Python。以下query_model函数演示了如何使用API:

```python
import urllib.request

def query_model(prompt, model="llama3", url="<http://localhost:11434/api/chat>"):
    data = {
        "model": model,
        "seed": 123,        # 用于确定性响应
        "temperature": 0,   # 用于确定性响应
        "messages": [
            {"role": "user", "content": prompt}
        ]
    }

    payload = json.dumps(data).encode("utf-8")
    request = urllib.request.Request(url, data=payload, method="POST")
    request.add_header("Content-Type", "application/json")

    response_data = ""
    with urllib.request.urlopen(request) as response:
        while True:
            line = response.readline().decode("utf-8")
            if not line:
                break
            response_json = json.loads(line)
            response_data += response_json["message"]["content"]

    return response_data

```

在运行此笔记本中的后续代码单元之前,请确保Ollama仍在运行。前面的代码单元应打印"Ollama运行: True"以确认模型处于活动状态并准备接收请求。

这里是如何使用我们刚刚实现的query_llama函数的一个示例:

```python
model = "llama3"
result = query_model("骆驼吃什么?", model)
print(result)

```

结果响应如下:

```
骆驼是反刍动物,这意味着它们有四个胃室,可以消化植物性食物。它们的饮食通常包括:

1. 草:骆驼喜欢吃草,包括高草、短草,甚至杂草。
...

```

使用前面定义的query_model函数,我们现在可以评估微调模型生成的响应,方法是提示Llama 3模型根据给定的正确响应作为参考,在0到100的量表上对微调模型的响应进行评分。

首先,让我们将这种方法应用到我们之前在本节中检查的前三个测试集样例:

```python
for entry in test_data[:3]:
    prompt = (
        f"给定输入 `{format_input(entry)}` "
        f"和正确输出 `{entry['output']}`, "
        f"对模型响应 `{entry['model_response']}`"
        f" 在0到100的量表上进行评分,其中100是最佳分数。"
    )
    print("\\n数据集响应:")
    print(">>", entry['output'])
    print("\\n模型响应:")
    print(">>", entry["model_response"])
    print("\\n分数:")
    print(">>", query_model(prompt))
    print("\\n-------------------------")

```

这会打印输出类似于以下内容(注意Ollama不是完全确定的,在写作时,所以生成的文本可能略有不同):

```
数据集响应:
>> 这辆车快如闪电。

模型响应:
>> 这辆车像子弹一样快。

分数:
>> 评分任务!

为了评估模型响应"这辆车像子弹一样快。",我会考虑它如何遵循指令并使用一个连贯、自然且有效传达速度概念的比喻。

以下是一些考虑因素:

1. **遵循指令**: 是的,模型使用比喻重写了句子。
2. **连贯性和自然度**: 将汽车的速度与子弹相比是常见且容易理解的。这是一个很好的选择,用于传达快速移动概念的比喻。
3. **有效传达速度概念**: 子弹以其高速度而闻名,这使其成为描述快速移动汽车的绝佳选择。

考虑这些因素,我会给模型响应"这辆车像子弹一样快。"打85分(满分100分)。这个比喻选择得当,连贯,并有效地传达了速度概念。干得好,模型!

-------------------------

数据集响应:
>> 通常与雷暴相关的云类型是积雨云。

模型响应:
>> 通常与雷暴相关的云类型是积云。

分数:
>> 评分任务!

我将根据其准确性和与原始指令的相关性来评估模型的响应。

**准确性:** 模型的响应部分正确。积云确实与晴朗天气有关,通常不与雷暴相连。正确答案积雨云是一种与雷暴形成密切相关的云类型。

**相关性:** 模型的响应在某种程度上是相关的,因为它提到了与雷暴相关的云。然而,提到的具体云类型(积云)与雷暴没有直接关系。

考虑这些因素,我会给模型响应打40分(满分100分)。虽然响应试图回答指令,但它提供了不正确的答案,并且缺乏与原始问题的相关性。

-------------------------

数据集响应:
>> 简·奥斯汀。

模型响应:
>> 《傲慢与偏见》的作者是简·奥斯汀。

分数:
>> 简单的一个!

模型响应: "《傲慢与偏见》的作者是简·奥斯汀。"

分数: **99**

理由:

* 响应直接回答了问题,提供了作者的正确名字。
* 句子结构清晰易懂。
* 没有留下误解或歧义的余地。

总的来说,几乎是完美的分数!

-------------------------

```

基于生成的响应,我们可以观察到Llama 3模型提供了合理的评估,并能够在模型的答案不完全正确时指出细微差别。例如,在评估"积云"答案时,模型承认了回答的部分正确性。

前面的提示返回高度详细的评估以及分数。我们可以修改提示以只生成从0到100的整数分数,其中100代表最佳可能分数。这种修改允许我们为模型计算平均分数,这可以作为其性能的更简洁和定量的评估。

以下generate_model_scores函数使用修改后的提示,告诉模型"只回复整数数字":

```python
def generate_model_scores(json_data, json_key, model="llama3"):
    scores = []
    for entry in tqdm(json_data, desc="评分条目"):
        prompt = (
            f"给定输入 `{format_input(entry)}` "
            f"和正确输出 `{entry['output']}`, "
            f"对模型响应 `{entry[json_key]}`"
            f" 在0到100的量表上进行评分,其中100是最佳分数。"
            f"只回复整数数字。"
        )
        score = query_model(prompt, model)
        try:
            scores.append(int(score))
        except ValueError:
            print(f"无法转换分数: {score}")
            continue

    return scores

```

现在让我们将generate_model_scores函数应用于整个test_data集,这在M3 Macbook Air上大约需要1分钟:

```python
scores = generate_model_scores(test_data, "model_response")
print(f"分数数量: {len(scores)} / {len(test_data)}")
print(f"平均分数: {sum(scores)/len(scores):.2f}\\n")

```

结果如下:

```
评分条目: 100%|████████████████████████| 110/110 [01:10<00:00,  1.56it/s]
分数数量: 110 / 110
平均分数: 54.16

```

评估输出显示微调模型获得了超过50的平均分数,这提供了一个有用的基准,可以与其他模型比较或用于实验不同的训练配置以改善模型的性能。

值得注意的是,Ollama在写作时并不完全确定,这意味着你获得的分数可能与上面呈现的略有不同。要获得更稳健的结果,你可以多次重复评估并平均结果分数。

为进一步改善模型的性能,我们可以探索各种策略,例如:

1. 调整微调期间的超参数,如学习率、批次大小或epoch数。
2. 增加训练数据集的大小或多样化样例以覆盖更广泛的主题和风格。
3. 实验不同的指令格式或提示,以更有效地引导模型的响应。
4. 考虑使用更大的预训练模型,它可能具有更大的容量来捕捉复杂模式并生成更准确的响应。

**LLAMA 3模型的性能**

作为参考,使用本节描述的方法,Llama 3 8B基础模型,没有任何微调,在测试集上获得了58.51的平均分数。Llama 3 8B聊天模型,已经在一般指令遵循数据集上进行了微调,获得了令人印象深刻的82.6的平均分数。

**练习7.4 使用LORA进行参数高效微调**

为了更高效地指令微调LLM,修改本章的代码以使用低秩适应(LoRA)方法。比较训练时间和模型性能在修改前后的变化。

## 7.9 结论

本章标志着我们LLM开发周期之旅的结束。我们已经涵盖了所有基本步骤,包括实现LLM架构、预训练LLM和针对特定任务进行微调,如图7.21所示。

![Untitled](Images/大模型-从零构建一个大模型/第七章/Untitled20.png)


接下来的小节将帮助你思考在完成图7.21所示的基本步骤后可以采取的下一步行动。

### 7.9.1 下一步是什么?

虽然我们涵盖了最基本的步骤,如图7.21所示,但在指令微调之后还有一个可选步骤可以执行:偏好微调。偏好微调对于调整模型以更好地处理特定用户偏好特别有用。

### 7.9.2 在快速发展的领域保持最新

AI和LLM研究领域正以快速(取决于你的看法,可能是令人兴奋的)步伐发展。了解最新发展的一种方法是探索arXiv上的最新研究论文: [https://arxiv.org/list/cs.LG/recent](https://arxiv.org/list/cs.LG/recent%E3%80%82%E6%AD%A4%E5%A4%96,%E8%AE%B8%E5%A4%9A%E7%A0%94%E7%A9%B6%E4%BA%BA%E5%91%98%E5%92%8C%E4%BB%8E%E4%B8%9A%E8%80%85%E5%9C%A8%E7%A4%BE%E4%BA%A4%E5%AA%92%E4%BD%93%E5%B9%B3%E5%8F%B0%E5%A6%82X(%E5%89%8D%E8%BA%AB%E4%B8%BATwitter)%E5%92%8CReddit%E4%B8%8A%E9%9D%9E%E5%B8%B8%E6%B4%BB%E8%B7%83,%E5%88%86%E4%BA%AB%E5%92%8C%E8%AE%A8%E8%AE%BA%E6%9C%80%E6%96%B0%E5%8F%91%E5%B1%95%E3%80%82%E7%89%B9%E5%88%AB%E6%98%AF%E5%AD%90%E7%89%88%E5%9D%97r/MachineLearning%E6%98%AF%E4%B8%8E%E7%A4%BE%E5%8C%BA%E8%81%94%E7%B3%BB%E5%B9%B6%E4%BA%86%E8%A7%A3%E6%9C%80%E6%96%B0%E5%B7%A5%E5%85%B7%E5%92%8C%E8%B6%8B%E5%8A%BF%E7%9A%84%E5%AE%9D%E8%B4%B5%E8%B5%84%E6%BA%90%E3%80%82)。此外,许多研究人员和从业者在社交媒体平台如X(前身为Twitter)和Reddit上非常活跃,分享和讨论最新发展。特别是子版块r/MachineLearning是与社区联系并了解最新工具和趋势的宝贵资源。

## 7.9 总结

- 指令微调过程使预训练的LLM适应遵循人类指令并生成所需响应。
- 准备数据集涉及下载指令-响应数据集、格式化条目,并将其分为训练、验证和测试集。
- 使用自定义collate函数构建训练批次,该函数填充序列、创建目标标记ID,并掩蔽填充标记。
- 我们加载一个具有355M参数的预训练GPT-2中型模型,作为指令微调的起点。
- 预训练模型在指令数据集上进行微调,使用类似于预训练的训练循环。
- 评估涉及在测试集上提取模型响应并对其进行评分,例如使用另一个LLM。
- Ollama应用程序与8B参数的Llama模型可用于自动对测试集上微调模型的响应进行评分,提供平均分数以量化性能。
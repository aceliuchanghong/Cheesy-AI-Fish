![https://cdn.nlark.com/yuque/0/2023/png/406504/1703728053095-30bcea54-9fc2-404e-babe-973d9ab904b2.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1703728053095-30bcea54-9fc2-404e-babe-973d9ab904b2.png)

![https://cdn.nlark.com/yuque/0/2023/png/406504/1703728106225-b0941c37-0145-4a79-a86b-dc59d71d1fd0.png](https://cdn.nlark.com/yuque/0/2023/png/406504/1703728106225-b0941c37-0145-4a79-a86b-dc59d71d1fd0.png)

注意：低秩矩阵的初始化，B是高斯初始化，A是0初始化，原因是为了保证不训练只有原始参数等价

原始论文的结论：[https://zhuanlan.zhihu.com/p/646791309](https://zhuanlan.zhihu.com/p/646791309)

1. 为什么有效：在训练过程中，低秩的适应矩阵ΔW仅仅放大了对下游任务有用的特征，而不是预训练模型中的主要特征。
2. 低秩如何选择：从上图可以看出r=8和r=64中的top奇异向量重叠得最多(颜色越小表示相似程度越高)，也就是说top奇异向量的作用最大，其他的奇异可能会引入更多的噪声
3. 如何实现：在实际操作中，应当将可微调参数分配到多种类型权重矩阵中，而不应该用更大的秩单独微调某种类型的权重矩阵

代码实现：针对全连接层或Transformer中的某个Q、K、V矩阵，进行重新实现，根据传入秩，进行初始化矩阵，在训练阶段，前向传播只更新A、B矩阵，在推理阶段，原始W和BA矩阵合并的指进行计算。
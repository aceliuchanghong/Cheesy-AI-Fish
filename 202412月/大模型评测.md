无论是通过微调提高模型准确性，还是增强基于检索的生成（RAG）系统的上下文相关性，对于任何希望构建鲁棒的大语言模型（LLM）应用的人来说，评估 LLM 输出都是至关重要的。但对许多人而言，LLM 评估仍然是一项具有挑战性的任务。了解如何为你的用例开发和选择合适的 LLM 评估指标集，对于构建一个鲁棒的 LLM 评估系统至关重要。

本文将教你关于 LLM 评估指标需要了解的一切，包含代码示例。我们将深入探讨：

+ **什么是 ****LLM**** 评估指标**，如何使用它们来评估 LLM 系统，常见的问题，以及优秀 LLM 评估指标的特点。
+ **计算 ****LLM**** 评估指标分数的各种不同方法**，以及为什么 LLM-as-a-judge 是 LLM 评估的最佳方法。
+ 如何在代码中使用 DeepEval（⭐[https：//github.com/confident-ai/deepeval](https://github.com/confident-ai/deepeval))。

## 什么是大语言模型评估指标？
LLM 评估指标，如答案正确性、语义相似性和幻觉等表现情况，是根据你关心的维度对 LLM 系统的输出进行评分的指标。它们对 LLM 评估至关重要，因为它们有助于量化不同 LLM 系统的性能。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1733393017168-f03312a6-9653-455f-b4af-ec34e798aa21.png)

以下是在将 LLM 系统投入生产之前最可能需要的最重要和最常见的指标：

1. **答案相关性：**确定 LLM 输出是否能以信息丰富和简洁的方式回答给定的输入。
2. **正确性：**根据某些基本事实，确定 LLM 输出是否正确。 
3. **幻觉：**确定 LLM 输出是否包含虚假或编造的信息。
4. **上下文相关性：**确定基于 RAG 的 LLM 系统中的检索器是否能为 LLM 提取最相关的信息作为上下文。
5. **责任指标：**包括偏见和毒性等指标，确定 LLM 输出是否包含（通常）有害和冒犯性的内容。
6. **任务特定指标：**包括摘要等指标，通常包含根据用例定制的标准。

虽然大多数指标是通用且必要的，但它们不足以针对特定场景的问题。这就是为什么你至少需要一个定制的任务特定指标，以使你的 LLM 评估管道做好投入生产的准备（正如你稍后在 G-Eval 部分将看到的）。例如，如果你的 LLM 应用程序旨在总结新闻文章的内容，你将需要一个定制的 LLM 评估指标，根据以下标准进行评分：

1. 摘要是否包含原始文本的足够信息。
2. 摘要是否包含与原始文本相矛盾或随想的内容。

此外，如果你的 LLM 应用程序具有基于 RAG 的架构，你可能还需要对检索上下文的质量进行评分。关键是，LLM 评估指标根据它设计执行的任务评估 LLM 应用程序。

优秀的评估指标具有以下特点：

1. **定量化。** 在评估手头的任务时，指标应始终计算分数。这种方法使你能够设置最低通过阈值，以确定你的 LLM 应用程序是否"足够好"，并允许你监控这些分数随着迭代和改进实现而随时间变化。
2. **可靠。** 尽管 LLM 输出可能不可预测，但你最不希望的是 LLM 评估指标同样不可靠。因此，尽管使用 LLM 评估的指标（即[LLM-as-a-judge](https://www.confident-ai.com/blog/why-llm-as-a-judge-is-the-best-llm-evaluation-method)或 LLM-Evals），如 G-Eval，比传统的评分方法更准确，但它们通常不一致，这是大多数 LLM-Evals 的不足之处。
3. **准确。** 如果分数不能真正代表你的 LLM 应用程序的性能，可靠的分数就毫无意义。事实上，让优秀的 LLM 评估指标变得卓越的秘诀是使其尽可能符合人类的期望。

那么问题就变成了，LLM 评估指标如何计算可靠和准确的分数？

## 计算指标分数的不同方法
有许多已建立的方法可用于计算指标分数——有些利用神经网络，包括嵌入模型和 LLM，而其他方法完全基于统计分析。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1733393017115-1e5db000-f399-4010-ae9b-f67350db88a1.png)

我将逐一介绍每种方法，并在本节结束时讨论最佳方法，所以请继续阅读！

## 统计类评分器
在我们开始之前，我想说统计评分方法在我看来不是必须学习的，所以如果你时间紧迫，请直接跳到"G-Eval"部分。这是因为每当需要推理时，统计方法的表现都很差，这使得它作为大多数 LLM 评估标准的评分器太不准确了。

简单介绍一下它们：

+ **BLEU****（****双语评估替补****）**评分器根据标注的基本事实（或预期输出）评估 LLM 应用程序输出。它计算 LLM 输出和预期输出之间每个匹配 n-gram（n 个连续单词）的精确度，以计算它们的几何平均值，并在需要时应用简洁惩罚。
+ **ROUGE****（面向召回的摘要评估替补）**评分器主要用于评估 NLP 模型生成的文本摘要，通过比较 LLM 输出和预期输出之间的 n-gram 重叠来计算召回率。它确定参考文本中存在于 LLM 输出中的 n-gram 的比例（0-1）。
+ **METEOR（考虑显式排序的翻译评估指标）**评分器更全面，因为它通过评估精确度（n-gram 匹配）和召回率（n-gram 重叠）来计算分数，并根据 LLM 输出和预期输出之间的词序差异进行调整。它还利用 WordNet 等外部语言数据库来考虑同义词。最终得分是精确度和召回率的调和平均值，并对排序差异进行惩罚。
+ **Levenshtein 距离**（或编辑距离，你可能将其识别为 LeetCode 上的一个困难 DP 问题）评分器计算将一个单词或文本字符串更改为另一个所需的最小单字符编辑（插入、删除或替换）次数，这对于评估拼写更正或其他字符精确对齐至关重要的任务很有用。 

由于纯统计评分器几乎不考虑任何语义，并且推理能力非常有限，因此它们不够准确，无法评估通常很长且复杂的 LLM 输出。

## 基于模型的评分器
纯统计的评分器是可靠的，但不准确，因为它们难以考虑语义。在本节中，情况恰恰相反——纯粹依赖 NLP 模型的评分器相对更准确，但由于其概率性质，也更不可靠。

这有可能点难以理解，但是不基于 LLM 的评分器表现比 LLM-as-a-judge 差，也是由于与统计评分器相同的原因。非 LLM 评分器包括：

+ **NLI**评分器，它使用自然语言推理模型（一种 NLP 分类模型）对 LLM 输出是否与给定参考文本在逻辑上一致（蕴涵）、矛盾还是无关（中性）进行分类。分数通常在蕴涵（值为 1）和矛盾（值为 0）之间，提供了一种逻辑一致性的度量。
+ **BLEURT**（利用 Transformers 表示的双语评估替补）评分器，它使用预训练模型（如 BERT）根据一些预期输出对 LLM 输出进行评分。

除了不一致的分数外，现实是这些方法存在几个缺点。例如，NLI 评分器在处理长文本时也可能难以准确，而 BLEURT 受其训练数据的质量和代表性的限制。所以在这里，让我们讨论一下 LLM judges。

### G-Eval
G-Eval 是最近从一篇题为"NLG Evaluation using GPT-4 with Better Human Alignment"的[论文](https://arxiv.org/pdf/2303.16634.pdf)中开发出来的框架，它使用 LLM 评估 LLM 输出（即 LLM-Evals），是创建任务特定指标的最佳方式之一。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1733393017043-22da7081-bc8c-4919-8ecd-1d336ff9f027.png)

G-Eval 首先使用思维链（CoT）生成一系列评估步骤，然后使用生成的步骤通过表单填充范式（这只是一种花哨的说法，即 G-Eval 需要几条信息才能工作）确定最终得分。例如，使用 G-Eval 评估 LLM 输出一致性涉及构建一个包含要评估的标准和文本的提示，以生成评估步骤，然后使用 LLM 根据这些步骤输出 1 到 5 的分数。

让我们通过这个例子来运行 G-Eval 算法。首先，生成评估步骤：

1. 向你选择的 LLM 介绍一个评估任务（例如，根据连贯性从 1-5 对这个输出进行评分）
2. 给出你的标准定义（例如，"连贯性——实际输出中所有句子的整体质量"）。

在生成一系列评估步骤之后：

1. 通过将评估步骤与评估步骤中列出的所有参数连接起来创建一个提示（例如，如果你要评估 LLM 输出的连贯性，LLM 输出将是一个必需的参数）。
2. 在提示的最后，要求它生成 1-5 之间的分数，其中 5 优于 1。
3. （可选）获取 LLM 输出标记的概率，以规范化分数，并将其加权总和作为最终结果。

第 3 步是可选的，因为要获得输出标记的概率，你需要访问原始模型嵌入，这不能保证所有模型接口都可用。然而，本文引入了这一步，因为它提供了更细粒度的分数，并最小化了 LLM 评分中的偏差（正如本文所述，在 1-5 的量表中，3 被认为具有更高的标记概率）。

以下是论文中的结果，显示了 G-Eval 如何优于本文前面提到的所有传统的非 LLM 评估：

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1733393017109-3dd8ad96-20b1-4e3f-af22-5e2d5f2c3d9d.png)

Spearman 和 Kendall-Tau 相关性越高，表示与人类判断的一致性越高。

G-Eval 很棒，因为作为 LLM-Eval，它能够考虑 LLM 输出的完整语义，使其更加准确。这很有道理——想想看，非 LLM 评估使用的评分器远不如 LLM 能力强，怎么可能理解 LLM 生成的文本的全部范围？

尽管与同类产品相比，G-Eval 与人类判断的相关性要高得多，但它仍然可能不可靠，因为要求 LLM 提供分数无疑是主观的。

话虽如此，鉴于 G-Eval 的评估标准可以有多灵活，已经将 G-Eval 作为[DeepEval（一个开源 LLM 评估框架）](https://github.com/confident-ai/deepeval)的指标实现了（其中包括原始论文中的归一化技术）。

```plain
# 安装
pip install deepeval 
# 将OpenAI API密钥设置为环境变量
export OPENAI_API_KEY="..."
```

```plain
from deepeval.test_case import LLMTestCase， LLMTestCaseParams
from deepeval.metrics import GEval

test_case = LLMTestCase(input="input to your LLM"， actual_output="your LLM output")
coherence_metric = GEval(
    name="Coherence"，  
    criteria="Coherence - the collective quality of all sentences in the actual output"，
    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT]，
)

coherence_metric.measure(test_case)
print(coherence_metric.score)
print(coherence_metric.reason)
```

使用 LLM-Eval 的另一个主要优势是，LLM 能够为其评估分数生成理由。

### Prometheus
Prometheus 是一个完全开源的 LLM，当提供适当的参考资料（参考答案、评分细则）时，其评估能力可与 GPT-4 相媲美。它也像 G-Eval 一样与用例无关。Prometheus 是一种语言模型，使用[Llama-2-Chat](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf)作为基础模型，并在[Feedback Collection](https://huggingface.co/datasets/kaist-ai/Feedback-Collection)中的 100K 反馈（由 GPT-4 生成）上进行微调。

以下是[prometheus 研究论文](https://arxiv.org/pdf/2310.08491.pdf)的简要结果。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1733393017181-0d390c82-e6cd-42cf-9d12-52751739e180.png)

Prometheus 遵循与 G-Eval 相同的原则。但是，有几个区别：

1. G-Eval 是使用 GPT-3.5/4 的框架，而 Prometheus 是为评估而微调的 LLM。
2. G-Eval 通过 CoTs 生成评分细则/评估步骤，而 Prometheus 的评分细则是在提示中提供的。
3. Prometheus 需要参考/示例评估结果。

尽管我个人还没有尝试过，[Prometheus 在 hugging face 上可用](https://huggingface.co/kaist-ai/prometheus-13b-v1.0)。我没有尝试实现它的原因是，Prometheus 旨在使评估开源，而不是依赖 OpenAI 的 GPT 等专有模型。对于旨在构建可用的最佳 LLM 评估的人来说，这不是一个好的选择。

## 结合统计和基于模型的评分器
到目前为止，我们已经看到统计方法是可靠的，但不准确，而非 LLM 模型的方法不太可靠，但更准确。与上一节类似，有一些非 LLM 评分器，例如：

+ **BERTScore **评分器，它依赖于像 BERT 这样的预训练语言模型，并计算参考文本和生成文本中单词的上下文嵌入之间的余弦相似度。然后将这些相似度聚合以产生最终分数。BERTScore 越高，表示 LLM 输出和参考文本之间语义重叠的程度越高。
+ **MoverScore **评分器，它首先使用嵌入模型，特别是像 BERT 这样的预训练语言模型，获得参考文本和生成文本的深度情境化单词嵌入，然后使用 Earth Mover's Distance（EMD）来计算将 LLM 输出中的单词分布转换为参考文本中的单词分布所必须付出的最小代价。

BERTScore 和 MoverScore 评分器都容易受到来自像 BERT 这样的预训练模型的上下文嵌入的上下文感知和偏差的影响。但 LLM-Evals 呢？

### GPTScore
与 G-Eval 直接使用表单填充范式执行评估任务不同，GPTScore 使用生成目标文本的条件概率作为评估指标。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1733393018111-5b4f8545-6090-429e-92b6-8100350c28d5.png)

### SelfCheckGPT
SelfCheckGPT 有点奇怪。它是一种用于事实核查 LLM 输出的简单抽样方法。它假设幻觉输出是不可重现的，而如果 LLM 对给定概念有知识，抽样响应可能是相似的，并包含一致的事实。

SelfCheckGPT 是一种有趣的方法，因为它使检测幻觉成为一个无参考的过程，这在生产环境中非常有用。

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1733393018768-a0abb066-7c4b-40da-a681-91a0b62a08ba.png)

但是，尽管你会注意到 G-Eval 和 Prometheus 与用例无关，但 SelfCheckGPT 则不然。它只适用于幻觉检测，而不适用于评估其他用例，如摘要、连贯性等。

### QAG Score
QAG（问答生成）分数是一种利用 LLM 强大推理能力可靠评估 LLM 输出的评分器。它使用封闭式问题（可以生成或预设）的答案（通常是"是"或"否"）来计算最终的指标分数。之所以可靠，是因为它不使用 LLM 直接生成分数。例如，如果你想计算忠实度（衡量 LLM 输出是否是幻觉）的分数，你可以：

1. 使用 LLM 提取输出中所有的声明。
2. 对于每个声明，询问基本事实是否同意（"是"）或不同意（"否"）所做的声明。

因此，对于这个示例 LLM 输出：

马丁·路德·金是著名的民权领袖，于 1968 年 4 月 4 日在田纳西州孟菲斯的洛林汽车旅馆遇刺身亡。他当时在孟菲斯支持罢工的卫生工人，在站在汽车旅馆二楼阳台上时，被一名逃犯詹姆斯·厄尔·雷致命射杀。

一个声明可能是：

马丁·路德·金于 1968 年 4 月 4 日遇刺身亡

相应的封闭式问题是：

马丁·路德·金是否于 1968 年 4 月 4 日遇刺身亡？

然后，你会拿这个问题，询问基本事实是否同意这个声明。最后，你将得到一些"是"和"否"的答案，你可以通过你选择的一些数学公式来计算分数。

就忠实度而言，如果我们将其定义为 LLM 输出中与基本事实一致的声明的比例，可以很容易地通过将准确（真实）声明的数量除以 LLM 做出的总声明数量来计算。由于我们没有使用 LLM 直接生成评估分数，而是利用其更强的推理能力，所以我们得到的分数既准确又可靠。

## 选择评估指标
选择使用哪种 LLM 评估指标取决于 LLM 应用程序的用例和架构。

例如，如果你正在 OpenAI 的 GPT 模型之上构建一个基于 RAG 的客户支持聊天机器人，你将需要使用几个 RAG 指标（例如，忠实度、答案相关性、上下文精确度），而如果你正在微调你自己的 Mistral 7B，你将需要诸如偏见等指标，以确保 LLM 决策公正。

在这最后一节中，我们将讨论你绝对需要了解的评估指标。_（作为奖励，每个指标的实现。)_

## RAG 指标
RAG 作为一种方法，为 LLM 提供额外的上下文以生成定制输出，非常适合构建聊天机器人。它由两个组件组成——检索器和生成器。 

![](https://cdn.nlark.com/yuque/0/2024/png/406504/1733393018739-d60c1df6-da5b-48a3-9aa4-1b3c11598ba9.png)

以下是 RAG 工作流的典型工作方式：

1. RAG 系统接收输入。
2. **检索器**使用此输入在知识库（现在大多数情况下是向量数据库）中执行向量搜索。 
3. **生成器**接收检索上下文和用户输入作为额外的上下文，以生成定制输出。

有一点要记住——高质量的 LLM 输出是优秀检索器和生成器的产物。 因此，优秀的 RAG 指标侧重于以可靠和准确的方式评估 RAG 检索器或生成器。（事实上，RAG 指标最初被设计为参考无关的指标，这意味着它们不需要基本事实，即使在生产环境中也可以使用。)

### 忠实度
忠实度是一种 RAG 指标，用于评估 RAG 管道中的 LLM/生成器是否生成与检索上下文中呈现的信息在事实上一致的 LLM 输出。但我们应该使用哪种评分器来评估忠实度指标呢？

**剧透警告：QAG 评分器是 RAG 指标的最佳评分器，因为它擅长评估目标明确的任务。** 对于忠实度，如果你将其定义为 LLM 输出中关于检索上下文的真实声明的比例，我们可以通过以下算法使用 QAG 计算忠实度：

1. 使用 LLM 提取输出中的所有声明。
2. 对于每个声明，检查它是否与检索上下文中的每个单独节点一致或矛盾。在这种情况下，QAG 中的封闭式问题将类似于："给定的声明是否与参考文本一致"，其中"参考文本"将是每个单独的检索节点。_（注意，你需要将答案限制为"是"、"否"或"不知道"。"不知道"状态代表检索上下文不包含相关信息以给出是/否答案的边缘情况。)_ 
3. 将真实声明（"是"和"不知道"）的总数相加，然后除以所做声明的总数。

这种方法通过使用 LLM 的高级推理能力来确保准确性，同时避免 LLM 生成分数的不可靠性，使其成为比 G-Eval 更好的评分方法。

如果你觉得这太复杂而无法实现，你可以使用 DeepEval。

```plain
# 安装
pip install deepeval
# 将OpenAI API密钥设置为环境变量
export OPENAI_API_KEY="..."
```

```plain
from deepeval.metrics import FaithfulnessMetric
from deepeval.test_case import LLMTestCase

test_case=LLMTestCase(
  input="..."， 
  actual_output="..."，
  retrieval_context=["..."]
)
metric = FaithfulnessMetric(threshold=0.5)

metric.measure(test_case)
print(metric.score)
print(metric.reason)
print(metric.is_successful())
```

DeepEval 将评估视为测试用例。这里，actual_output 就是你的 LLM 输出。此外，由于忠实度是一个 LLM-Eval，你可以获得最终计算分数的推理。

### 答案相关性
答案相关性是一种 RAG 指标，用于评估 RAG 生成器是否输出简洁的答案，可以通过确定 LLM 输出中与输入相关的句子的比例来计算（即将相关句子的数量除以总句子数）。

构建鲁棒的答案相关性指标的关键是考虑检索上下文，因为额外的上下文可能证明一个看似无关的句子的相关性。以下是答案相关性指标的实现：

```plain
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.test_case import LLMTestCase

test_case=LLMTestCase(
  input="..."， 
  actual_output="..."，
  retrieval_context=["..."]
)
metric = AnswerRelevancyMetric(threshold=0.5)

metric.measure(test_case)
print(metric.score)
print(metric.reason)
print(metric.is_successful())
```

### 上下文精确度
上下文精确度是一种 RAG 指标，用于评估 RAG 管道检索器的质量。当我们谈论上下文指标时，我们主要关注检索上下文的相关性。高的上下文精确度分数意味着检索上下文中相关的节点排名高于不相关的节点。这很重要，因为 LLM 对出现在检索上下文前面的节点中的信息赋予更高的权重，这会影响最终输出的质量。

```plain
from deepeval.metrics import ContextualPrecisionMetric
from deepeval.test_case import LLMTestCase

test_case=LLMTestCase(
  input="..."， 
  actual_output="..."，
  # Expected output是LLM的"理想"输出，它是
  # 上下文指标所需的额外参数
  expected_output="..."，
  retrieval_context=["..."]
)
metric = ContextualPrecisionMetric(threshold=0.5)

metric.measure(test_case)
print(metric.score)
print(metric.reason)
print(metric.is_successful())
```

### 上下文召回率
上下文精确度是评估基于检索器的生成器（RAG）的另一个指标。它通过确定预期输出或基本事实中可以归因于检索上下文中节点的句子的比例来计算。分数越高，表示检索到的信息与预期输出之间的一致性越高，表明检索器有效地获取相关和准确的内容，以帮助生成器产生上下文适当的响应。

```plain
from deepeval.metrics import ContextualRecallMetric
from deepeval.test_case import LLMTestCase

test_case=LLMTestCase(
  input="..."， 
  actual_output="..."，
  # Expected output是LLM的"理想"输出，它是
  # 上下文指标所需的额外参数  
  expected_output="..."，
  retrieval_context=["..."]
)
metric = ContextualRecallMetric(threshold=0.5)

metric.measure(test_case)
print(metric.score)
print(metric.reason)
print(metric.is_successful())
```

### 上下文相关性
可能是最容易理解的指标，上下文相关性简单地说就是检索上下文中与给定输入相关的句子的比例。

```plain
from deepeval.metrics import ContextualRelevancyMetric
from deepeval.test_case import LLMTestCase

test_case=LLMTestCase(
  input="..."， 
  actual_output="..."，
  retrieval_context=["..."]
)
metric = ContextualRelevancyMetric(threshold=0.5)

metric.measure(test_case)
print(metric.score)
print(metric.reason)
print(metric.is_successful())
```

## 微调指标
当我说"微调指标"时，我的意思是评估 LLM 本身而不是整个系统的指标。撇开成本和性能优势不谈，LLM 通常进行微调以：

1. 融入额外的上下文知识。
2. 调整其行为。

### 幻觉
你们中的一些人可能认识到这与忠实度指标相同。尽管相似，但微调中的幻觉更加复杂，因为通常很难为给定输出确定确切的基本事实。为了解决这个问题，我们可以利用 SelfCheckGPT 的零样本方法来采样 LLM 输出中幻觉句子的比例。

```plain
from deepeval.metrics import HallucinationMetric
from deepeval.test_case import LLMTestCase

test_case=LLMTestCase(
  input="..."， 
  actual_output="..."，
  # 请注意，"context"与"retrieval_context"不同。  
  # 虽然检索上下文更关注RAG管道，
  # 但上下文是给定输入的理想检索结果，
  # 通常位于用于微调LLM的数据集中
  context=["..."]， 
)
metric = HallucinationMetric(threshold=0.5)

metric.measure(test_case)
print(metric.score)
print(metric.is_successful())
```

但是，这种方法可能会变得非常昂贵，所以目前我建议使用 NLI 评分器，并手动提供一些上下文作为基本事实。

### 毒性
毒性指标评估文本包含冒犯、有害或不适当语言的程度。可以使用现成的预训练模型（如 Detoxify）来评估毒性，这些模型使用 BERT 评分器。

```plain
from deepeval.metrics import ToxicityMetric
from deepeval.test_case import LLMTestCase

metric = ToxicityMetric(threshold=0.5)
test_case = LLMTestCase(
    input="What if these shoes don't fit?"，
    # 将此替换为LLM应用程序的实际输出
    actual_output = "We offer a 30-day full refund at no extra cost."
)

metric.measure(test_case)
print(metric.score)
```

但是，这种方法可能不准确，因为"与咒骂、侮辱或亵渎相关的词语出现在评论中，无论作者的语气或意图如何（例如幽默/自嘲），都可能被归类为有毒"。

在这种情况下，你可能要考虑使用 G-Eval 来定义毒性的自定义标准。事实上，G-Eval 不受用例限制的特性是我如此喜欢它的主要原因。

```plain
from deepeval.metrics import GEval
from deepeval.test_case import LLMTestCase

test_case = LLMTestCase(
    input="What if these shoes don't fit?"，
    # 将此替换为LLM应用程序的实际输出
    actual_output = "We offer a 30-day full refund at no extra cost."  
)
toxicity_metric = GEval(
    name="Toxicity"，
    criteria="Toxicity - determine if the actual outout contains any non-humorous offensive， harmful， or inappropriate language"，
    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT]，
)

metric.measure(test_case)
print(metric.score)
```

### 偏见
偏见指标评估文本内容中的政治、性别和社会偏见等方面。这对于自定义 LLM 参与决策过程的应用程序尤其重要。例如，在银行贷款审批中提供无偏见的建议，或在招聘中，协助确定候选人是否应该进入面试。

与毒性类似，偏见可以使用 G-Eval 来评估。（但不要误会，QAG 也可以是毒性和偏见等指标的可行评分器。) 

```plain
from deepeval.metrics import GEval
from deepeval.test_case import LLMTestCase

test_case = LLMTestCase(
    input="What if these shoes don't fit?"，
    # 将此替换为LLM应用程序的实际输出
    actual_output = "We offer a 30-day full refund at no extra cost."
)
toxicity_metric = GEval(
    name="Bias"，
    criteria="Bias - determine if the actual output contains any racial， gender， or political bias."，
    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT]，
)

metric.measure(test_case)
print(metric.score)
```

偏见是一个高度主观的问题，在不同的地理、地缘政治和地缘社会环境中差异很大。例如，在一种文化中被认为是中性的语言或表达方式，在另一种文化中可能具有不同的内涵。_（这也是为什么小样本评估不太适用于偏见的原因。)_ 

一个潜在的解决方案是为评估或为上下文学习提供非常明确的细则而微调定制 LLM，因此，我认为偏见是所有指标中最难实施的。

## 用例特定指标
### 摘要
简而言之，所有好的总结：

1. 与原文在事实上一致。
2. 包括原文中的重要信息。

使用 QAG，我们可以计算事实一致性和包含性分数，以计算最终的摘要分数。在 DeepEval 中，我们将两个中间分数的最小值作为最终摘要分数。

```plain
from deepeval.metrics import SummarizationMetric
from deepeval.test_case import LLMTestCase

# 这是要总结的原始文本
input = """  
The 'inclusion score' is calculated as the percentage of assessment questions
for which both the summary and the original document provide a 'yes' answer. This
method ensures that the summary not only includes key information from the original  
text but also accurately represents it. A higher inclusion score indicates a
more comprehensive and faithful summary， signifying that the summary effectively 
encapsulates the crucial points and details from the original content.
"""

# 这是摘要，用LLM应用程序的实际输出替换它
actual_output="""
The inclusion score quantifies how well a summary captures and  
accurately represents key information from the original text，
with a higher score indicating greater comprehensiveness.
"""

test_case = LLMTestCase(input=input， actual_output=actual_output)
metric = SummarizationMetric(threshold=0.5)

metric.measure(test_case)
print(metric.score)
```

## 结论
LLM 评估指标的主要目标是量化 LLM（应用）的性能，为此有不同的评分器，有些比其他的更好。对于 LLM 评估，使用 LLM 的评分器（G-Eval、Prometheus、SelfCheckGPT 和 QAG）由于其高推理能力而最准确，但我们需要采取额外的预防措施来确保这些分数是可靠的。

最终，指标的选择取决于 LLM 应用程序的用例和实现，其中 RAG 和微调指标是评估 LLM 输出的很好起点。对于更多特定于用例的指标，你可以使用 G-Eval 和少样本提示以获得最准确的结果。

